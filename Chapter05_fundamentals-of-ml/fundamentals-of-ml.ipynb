{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentals of machine learning\n",
    "After the three practical examples in chapter 4, you should be starting to feel familiar with how to approach classification and regression problems using neural networks, and you’ve witnessed the central problem of machine learning: overfitting.<br>\n",
    "This chapter will formalize some of your new intuition about machine learning into a solid conceptual framework, highlighting the importance of accurate model evaluation and the balance between training and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalization: The goal of machine learning\n",
    "In the three examples presented in chapter 4—predicting movie reviews, topic classification, and house-price regression—we split the data into a training set, a validation set, and a test set. The reason not to evaluate the models on the same data they were trained on quickly became evident: after just a few epochs, performance on never-before-seen data started diverging from performance on the training data, which always improves as training progresses. <br>\n",
    "The models started to **overfit**. Overfitting happens in every machine learning problem. <br>\n",
    "The fundamental issue in machine learning is the tension between optimization and generalization. **Optimization** refers to the process of adjusting a model to get the best performance possible on the training data (the learning in machine learning), whereas **generalization** refers to how well the trained model performs on data it has never seen before. The goal of the game is to get good generalization, of course, but you don’t control generalization; you can only fit the model to its training data. <br> \n",
    "If you do that too well, overfitting kicks in and generalization suffers. <br>\n",
    "But what causes overfitting? How can we achieve good generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Underfitting and overfitting\n",
    "For the models you saw in the previous chapter, performance on the held-out validation data started improving as training went on and then inevitably peaked after a while. This pattern (illustrated in figure 5.1) is universal. You’ll see it with any model type and any dataset.\n",
    "\n",
    "![overfitting](./images/5.1.png)\n",
    "\n",
    "At the beginning of training, optimization and generalization are correlated: the lower the loss on training data, the lower the loss on test data. While this is happening, your model is said to be **underfit**: there is still progress to be made; the network hasn’t yet modeled all relevant patterns in the training data. But after a certain number of iterations on the training data, generalization stops improving, validation metrics stall and then begin to degrade: the model is starting to **overfit**. That is, it’s beginning to learn patterns that are specific to the training data but that are misleading or irrelevant when it comes to new data. <br>\n",
    "Overfitting is particularly likely to occur when your data is noisy, if it involves uncertainty, or if it includes rare features. Let’s look at concrete examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOISY TRAINING DATA\n",
    "In real-world datasets, it’s fairly common for some inputs to be invalid. Perhaps a MNIST digit could be an all-black image, for instance, or something like figure 5.2.\n",
    "\n",
    "![noisy-data](./images/5.2.png)\n",
    "\n",
    "What are these? I don’t know either. But they’re all part of the MNIST training set.\n",
    "What’s even worse, however, is having perfectly valid inputs that end up mislabeled, like those in figure 5.3.\n",
    "\n",
    "![mislabeled-data](./images/5.3.png)\n",
    "\n",
    "If a model goes out of its way to incorporate such outliers, its generalization performance will degrade, as shown in figure 5.4. For instance, a 4 that looks very close to the mislabeled 4 in figure 5.3 may end up getting classified as a 9.\n",
    "\n",
    "![](./images/5.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AMBIGUOUS FEATURES\n",
    "Not all data noise comes from inaccuracies—even perfectly clean and neatly labeled data can be noisy when the problem involves uncertainty and ambiguity. In classification tasks, it is often the case that some regions of the input feature space are associated with multiple classes at the same time. Let’s say you’re developing a model that takes an image of a banana and predicts whether the banana is unripe, ripe, or rotten. <br>\n",
    "These categories have no objective boundaries, so the same picture might be classified as either unripe or ripe by different human labelers. Similarly, many problems involve randomness. You could use atmospheric pressure data to predict whether it will rain tomorrow, but the exact same measurements may be followed sometimes by rain and sometimes by a clear sky, with some probability. <br>\n",
    "A model could overfit to such probabilistic data by being too confident about ambiguous regions of the feature space, like in figure 5.5. A more robust fit would ignore individual data points and look at the bigger picture.\n",
    "\n",
    "![](./images/5.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RARE FEATURES AND SPURIOUS CORRELATIONS\n",
    "If you’ve only ever seen two orange tabby cats in your life, and they both happened to be terribly antisocial, you might infer that orange tabby cats are generally likely to be antisocial. That’s overfitting: if you had been exposed to a wider variety of cats, including more orange ones, you’d have learned that cat color is not well correlated with character. <br>\n",
    "Likewise, machine learning models trained on datasets that include rare feature values are highly susceptible to overfitting. In a sentiment classification task, if the word “cherimoya” (a fruit native to the Andes) only appears in one text in the training data, and this text happens to be negative in sentiment, a poorly regularized model might put a very high weight on this word and always classify new texts that mention cherimoyas as negative, whereas, objectively, there’s nothing negative about the cherimoya. <br>\n",
    "Importantly, a feature value doesn’t need to occur only a couple of times to lead to spurious correlations. Consider a word that occurs in 100 samples in your training data and that’s associated with a positive sentiment 54% of the time and with a negative sentiment 46% of the time. That difference may well be a complete statistical fluke, yet your model is likely to learn to leverage that feature for its classification task. <br>\n",
    "This is one of the most common sources of **overfitting**. <br>\n",
    "Here’s a striking example. Take MNIST. Create a new training set by concatenating 784 white noise dimensions to the existing 784 dimensions of the data, so half of the data is now noise. For comparison, also create an equivalent dataset by concatenating 784 all-zeros dimensions. Our concatenation of meaningless features does not at all affect the information content of the data: we’re only adding something. **Human classification accuracy wouldn’t be affected by these transformations at all.**\n",
    "\n",
    "##### Adding white noise channels or all-zeros channels to MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "train_images_with_noise_channels = np.concatenate(\n",
    "    [train_images, np.random.random((len(train_images), 784))], axis=1)\n",
    "\n",
    "train_images_with_zeros_channels = np.concatenate(\n",
    "    [train_images, np.zeros((len(train_images), 784))], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s train the model from chapter 2 on both of these training sets.\n",
    "##### Training the same model on MNIST data with noise channels or all-zero channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 5s 11ms/step - loss: 0.6323 - accuracy: 0.8067 - val_loss: 0.2572 - val_accuracy: 0.9219\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.2492 - accuracy: 0.9223 - val_loss: 0.2133 - val_accuracy: 0.9363\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.1592 - accuracy: 0.9508 - val_loss: 0.1411 - val_accuracy: 0.9576\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.1107 - accuracy: 0.9652 - val_loss: 0.1804 - val_accuracy: 0.9448\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0844 - accuracy: 0.9730 - val_loss: 0.1359 - val_accuracy: 0.9618\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0609 - accuracy: 0.9800 - val_loss: 0.1359 - val_accuracy: 0.9631\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0437 - accuracy: 0.9857 - val_loss: 0.1402 - val_accuracy: 0.9639\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0331 - accuracy: 0.9893 - val_loss: 0.1285 - val_accuracy: 0.9679\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0253 - accuracy: 0.9915 - val_loss: 0.1207 - val_accuracy: 0.9699\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0194 - accuracy: 0.9935 - val_loss: 0.1575 - val_accuracy: 0.9616\n",
      "Epoch 1/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.2903 - accuracy: 0.9164 - val_loss: 0.1427 - val_accuracy: 0.9588\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.1186 - accuracy: 0.9654 - val_loss: 0.0979 - val_accuracy: 0.9714\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0773 - accuracy: 0.9767 - val_loss: 0.0971 - val_accuracy: 0.9730\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.0555 - accuracy: 0.9838 - val_loss: 0.0851 - val_accuracy: 0.9746\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0416 - accuracy: 0.9881 - val_loss: 0.0803 - val_accuracy: 0.9758\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0313 - accuracy: 0.9906 - val_loss: 0.0787 - val_accuracy: 0.9783\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0236 - accuracy: 0.9933 - val_loss: 0.0765 - val_accuracy: 0.9803\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0179 - accuracy: 0.9953 - val_loss: 0.0852 - val_accuracy: 0.9780\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0137 - accuracy: 0.9963 - val_loss: 0.0818 - val_accuracy: 0.9791\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0101 - accuracy: 0.9973 - val_loss: 0.0846 - val_accuracy: 0.9793\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "history_noise = model.fit(\n",
    "    train_images_with_noise_channels, train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)\n",
    "\n",
    "model = get_model()\n",
    "history_zeros = model.fit(\n",
    "    train_images_with_zeros_channels, train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s compare how the validation accuracy of each model evolves over time.\n",
    "##### Plotting a validation accuracy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cb19160d00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_acc_noise = history_noise.history[\"val_accuracy\"]\n",
    "val_acc_zeros = history_zeros.history[\"val_accuracy\"]\n",
    "epochs = range(1, 11)\n",
    "plt.plot(epochs, val_acc_noise, \"b-\",\n",
    "         label=\"Validation accuracy with noise channels\")\n",
    "plt.plot(epochs, val_acc_zeros, \"b--\",\n",
    "         label=\"Validation accuracy with zeros channels\")\n",
    "plt.title(\"Effect of noise channels on validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the data holding the same information in both cases, the validation accuracy of the model trained with noise channels ends up about one percentage point lower—purely through the influence of spurious correlations. The more noise channels you add, the further accuracy will degrade. <br>\n",
    "Noisy features inevitably lead to overfitting. As such, in cases where you aren’t sure whether the features you have are informative or distracting, it’s common to do **feature selection** before training. Restricting the IMDB data to the top 10,000 most common words was a crude form of feature selection, for instance. The typical way to do feature selection is to compute some usefulness score for each feature available—a measure of how informative the feature is with respect to the task, such as the mutual information between the feature and the labels—and only keep features that are above some threshold. Doing this would filter out the white noise channels in the preceding example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The nature of generalization in deep learning\n",
    "A remarkable fact about deep learning models is that they can be trained to fit anything, as long as they have enough representational power. <br>\n",
    "Don’t believe me? Try shuffling the MNIST labels and train a model on that. Even though there is no relationship whatsoever between the inputs and the shuffled labels, the training loss goes down just fine, even with a relatively small model. Naturally, the validation loss does not improve at all over time, since there is no possibility of generalization in this setting.\n",
    "\n",
    "##### Fitting an MNIST model with randomly shuffled labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 2.3164 - accuracy: 0.1049 - val_loss: 2.3062 - val_accuracy: 0.1080\n",
      "Epoch 2/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 2.2996 - accuracy: 0.1181 - val_loss: 2.3117 - val_accuracy: 0.1042\n",
      "Epoch 3/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 2.2890 - accuracy: 0.1296 - val_loss: 2.3209 - val_accuracy: 0.1051\n",
      "Epoch 4/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 2.2752 - accuracy: 0.1422 - val_loss: 2.3320 - val_accuracy: 0.1018\n",
      "Epoch 5/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 2.2581 - accuracy: 0.1550 - val_loss: 2.3376 - val_accuracy: 0.1017\n",
      "Epoch 6/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 2.2368 - accuracy: 0.1677 - val_loss: 2.3510 - val_accuracy: 0.1003\n",
      "Epoch 7/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 2.2145 - accuracy: 0.1831 - val_loss: 2.3694 - val_accuracy: 0.1022\n",
      "Epoch 8/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 2.1874 - accuracy: 0.1974 - val_loss: 2.3894 - val_accuracy: 0.1025\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 2.1585 - accuracy: 0.2112 - val_loss: 2.4002 - val_accuracy: 0.0985\n",
      "Epoch 10/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 2.1280 - accuracy: 0.2257 - val_loss: 2.4188 - val_accuracy: 0.0991\n",
      "Epoch 11/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 2.0954 - accuracy: 0.2459 - val_loss: 2.4584 - val_accuracy: 0.1014\n",
      "Epoch 12/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 2.0631 - accuracy: 0.2592 - val_loss: 2.4673 - val_accuracy: 0.1048\n",
      "Epoch 13/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 2.0299 - accuracy: 0.2743 - val_loss: 2.5006 - val_accuracy: 0.0990\n",
      "Epoch 14/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.9942 - accuracy: 0.2909 - val_loss: 2.5348 - val_accuracy: 0.0996\n",
      "Epoch 15/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 1.9618 - accuracy: 0.3053 - val_loss: 2.5584 - val_accuracy: 0.1042\n",
      "Epoch 16/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.9262 - accuracy: 0.3182 - val_loss: 2.5691 - val_accuracy: 0.1049\n",
      "Epoch 17/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.8935 - accuracy: 0.3313 - val_loss: 2.6253 - val_accuracy: 0.0962\n",
      "Epoch 18/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.8610 - accuracy: 0.3434 - val_loss: 2.6528 - val_accuracy: 0.0968\n",
      "Epoch 19/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.8292 - accuracy: 0.3586 - val_loss: 2.6740 - val_accuracy: 0.1044\n",
      "Epoch 20/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.7960 - accuracy: 0.3714 - val_loss: 2.7343 - val_accuracy: 0.1045\n",
      "Epoch 21/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.7628 - accuracy: 0.3823 - val_loss: 2.7610 - val_accuracy: 0.1014\n",
      "Epoch 22/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.7331 - accuracy: 0.3946 - val_loss: 2.8050 - val_accuracy: 0.0972\n",
      "Epoch 23/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.7026 - accuracy: 0.4053 - val_loss: 2.8199 - val_accuracy: 0.1003\n",
      "Epoch 24/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.6736 - accuracy: 0.4177 - val_loss: 2.8707 - val_accuracy: 0.1022\n",
      "Epoch 25/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.6459 - accuracy: 0.4287 - val_loss: 2.9278 - val_accuracy: 0.0995\n",
      "Epoch 26/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.6174 - accuracy: 0.4415 - val_loss: 2.9432 - val_accuracy: 0.0992\n",
      "Epoch 27/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.5897 - accuracy: 0.4508 - val_loss: 3.0157 - val_accuracy: 0.1001\n",
      "Epoch 28/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.5609 - accuracy: 0.4602 - val_loss: 3.0260 - val_accuracy: 0.1025\n",
      "Epoch 29/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.5350 - accuracy: 0.4705 - val_loss: 3.1003 - val_accuracy: 0.1013\n",
      "Epoch 30/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.5089 - accuracy: 0.4821 - val_loss: 3.1219 - val_accuracy: 0.1013\n",
      "Epoch 31/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.4860 - accuracy: 0.4885 - val_loss: 3.1556 - val_accuracy: 0.1020\n",
      "Epoch 32/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 1.4618 - accuracy: 0.4974 - val_loss: 3.1976 - val_accuracy: 0.0981\n",
      "Epoch 33/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.4362 - accuracy: 0.5073 - val_loss: 3.2453 - val_accuracy: 0.1026\n",
      "Epoch 34/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.4145 - accuracy: 0.5140 - val_loss: 3.2817 - val_accuracy: 0.1023\n",
      "Epoch 35/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.3902 - accuracy: 0.5258 - val_loss: 3.3399 - val_accuracy: 0.1037\n",
      "Epoch 36/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.3674 - accuracy: 0.5330 - val_loss: 3.3745 - val_accuracy: 0.1002\n",
      "Epoch 37/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.3463 - accuracy: 0.5382 - val_loss: 3.4440 - val_accuracy: 0.0995\n",
      "Epoch 38/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.3250 - accuracy: 0.5484 - val_loss: 3.4757 - val_accuracy: 0.1022\n",
      "Epoch 39/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.3043 - accuracy: 0.5562 - val_loss: 3.5354 - val_accuracy: 0.1018\n",
      "Epoch 40/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.2838 - accuracy: 0.5627 - val_loss: 3.5323 - val_accuracy: 0.1026\n",
      "Epoch 41/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.2634 - accuracy: 0.5712 - val_loss: 3.6074 - val_accuracy: 0.0994\n",
      "Epoch 42/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.2420 - accuracy: 0.5759 - val_loss: 3.6791 - val_accuracy: 0.1023\n",
      "Epoch 43/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.2241 - accuracy: 0.5852 - val_loss: 3.6929 - val_accuracy: 0.1019\n",
      "Epoch 44/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.2040 - accuracy: 0.5917 - val_loss: 3.7824 - val_accuracy: 0.1019\n",
      "Epoch 45/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.1860 - accuracy: 0.5980 - val_loss: 3.8472 - val_accuracy: 0.0999\n",
      "Epoch 46/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.1678 - accuracy: 0.6060 - val_loss: 3.8597 - val_accuracy: 0.0989\n",
      "Epoch 47/100\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 1.1496 - accuracy: 0.6117 - val_loss: 3.9607 - val_accuracy: 0.0993\n",
      "Epoch 48/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 1.1321 - accuracy: 0.6182 - val_loss: 3.9625 - val_accuracy: 0.0990\n",
      "Epoch 49/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.1166 - accuracy: 0.6233 - val_loss: 4.0094 - val_accuracy: 0.0995\n",
      "Epoch 50/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.0982 - accuracy: 0.6298 - val_loss: 4.0778 - val_accuracy: 0.1004\n",
      "Epoch 51/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.0822 - accuracy: 0.6356 - val_loss: 4.1665 - val_accuracy: 0.1020\n",
      "Epoch 52/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 1.0668 - accuracy: 0.6401 - val_loss: 4.2300 - val_accuracy: 0.0976\n",
      "Epoch 53/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 1.0514 - accuracy: 0.6486 - val_loss: 4.2056 - val_accuracy: 0.1024\n",
      "Epoch 54/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.0340 - accuracy: 0.6530 - val_loss: 4.3191 - val_accuracy: 0.1008\n",
      "Epoch 55/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 1.0197 - accuracy: 0.6594 - val_loss: 4.3377 - val_accuracy: 0.1002\n",
      "Epoch 56/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 1.0035 - accuracy: 0.6626 - val_loss: 4.4261 - val_accuracy: 0.0994\n",
      "Epoch 57/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.9901 - accuracy: 0.6684 - val_loss: 4.4455 - val_accuracy: 0.0983\n",
      "Epoch 58/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.9754 - accuracy: 0.6763 - val_loss: 4.5141 - val_accuracy: 0.0985\n",
      "Epoch 59/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.9595 - accuracy: 0.6810 - val_loss: 4.6469 - val_accuracy: 0.0999\n",
      "Epoch 60/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.9474 - accuracy: 0.6853 - val_loss: 4.6922 - val_accuracy: 0.1013\n",
      "Epoch 61/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.9335 - accuracy: 0.6889 - val_loss: 4.6850 - val_accuracy: 0.1036\n",
      "Epoch 62/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.9199 - accuracy: 0.6921 - val_loss: 4.7130 - val_accuracy: 0.0979\n",
      "Epoch 63/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.9064 - accuracy: 0.6996 - val_loss: 4.8252 - val_accuracy: 0.0996\n",
      "Epoch 64/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.8943 - accuracy: 0.7026 - val_loss: 4.8958 - val_accuracy: 0.0988\n",
      "Epoch 65/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.8786 - accuracy: 0.7078 - val_loss: 4.9149 - val_accuracy: 0.1001\n",
      "Epoch 66/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.8677 - accuracy: 0.7112 - val_loss: 5.0197 - val_accuracy: 0.0993\n",
      "Epoch 67/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.8546 - accuracy: 0.7182 - val_loss: 5.0136 - val_accuracy: 0.0997\n",
      "Epoch 68/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.8412 - accuracy: 0.7205 - val_loss: 5.1261 - val_accuracy: 0.1006\n",
      "Epoch 69/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.8312 - accuracy: 0.7244 - val_loss: 5.1550 - val_accuracy: 0.0983\n",
      "Epoch 70/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.8171 - accuracy: 0.7286 - val_loss: 5.2194 - val_accuracy: 0.1011\n",
      "Epoch 71/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.8054 - accuracy: 0.7338 - val_loss: 5.3143 - val_accuracy: 0.1016\n",
      "Epoch 72/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.7953 - accuracy: 0.7363 - val_loss: 5.3589 - val_accuracy: 0.0987\n",
      "Epoch 73/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.7842 - accuracy: 0.7404 - val_loss: 5.3528 - val_accuracy: 0.1000\n",
      "Epoch 74/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.7736 - accuracy: 0.7455 - val_loss: 5.5019 - val_accuracy: 0.0993\n",
      "Epoch 75/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.7631 - accuracy: 0.7480 - val_loss: 5.5578 - val_accuracy: 0.1039\n",
      "Epoch 76/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.7526 - accuracy: 0.7531 - val_loss: 5.5744 - val_accuracy: 0.0993\n",
      "Epoch 77/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.7401 - accuracy: 0.7570 - val_loss: 5.6633 - val_accuracy: 0.0997\n",
      "Epoch 78/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.7305 - accuracy: 0.7589 - val_loss: 5.7158 - val_accuracy: 0.1009\n",
      "Epoch 79/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.7210 - accuracy: 0.7632 - val_loss: 5.7627 - val_accuracy: 0.1003\n",
      "Epoch 80/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.7115 - accuracy: 0.7667 - val_loss: 5.8542 - val_accuracy: 0.0980\n",
      "Epoch 81/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.7031 - accuracy: 0.7681 - val_loss: 5.8610 - val_accuracy: 0.0978\n",
      "Epoch 82/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.6906 - accuracy: 0.7746 - val_loss: 5.8824 - val_accuracy: 0.1023\n",
      "Epoch 83/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.6802 - accuracy: 0.7773 - val_loss: 6.0531 - val_accuracy: 0.1018\n",
      "Epoch 84/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.6714 - accuracy: 0.7791 - val_loss: 6.0784 - val_accuracy: 0.1018\n",
      "Epoch 85/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.6624 - accuracy: 0.7833 - val_loss: 6.1397 - val_accuracy: 0.1016\n",
      "Epoch 86/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.6541 - accuracy: 0.7855 - val_loss: 6.1815 - val_accuracy: 0.0991\n",
      "Epoch 87/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.6433 - accuracy: 0.7907 - val_loss: 6.2889 - val_accuracy: 0.0976\n",
      "Epoch 88/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.6383 - accuracy: 0.7912 - val_loss: 6.3282 - val_accuracy: 0.1021\n",
      "Epoch 89/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.6271 - accuracy: 0.7935 - val_loss: 6.4251 - val_accuracy: 0.1005\n",
      "Epoch 90/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.6177 - accuracy: 0.7989 - val_loss: 6.4759 - val_accuracy: 0.1053\n",
      "Epoch 91/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.6084 - accuracy: 0.8015 - val_loss: 6.5115 - val_accuracy: 0.1024\n",
      "Epoch 92/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.6013 - accuracy: 0.8059 - val_loss: 6.5945 - val_accuracy: 0.1024\n",
      "Epoch 93/100\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.5920 - accuracy: 0.8088 - val_loss: 6.6932 - val_accuracy: 0.1018\n",
      "Epoch 94/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.5855 - accuracy: 0.8094 - val_loss: 6.6989 - val_accuracy: 0.0998\n",
      "Epoch 95/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.5758 - accuracy: 0.8140 - val_loss: 6.7936 - val_accuracy: 0.1002\n",
      "Epoch 96/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.5700 - accuracy: 0.8170 - val_loss: 6.8416 - val_accuracy: 0.1007\n",
      "Epoch 97/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.5623 - accuracy: 0.8171 - val_loss: 6.9050 - val_accuracy: 0.1002\n",
      "Epoch 98/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.5537 - accuracy: 0.8208 - val_loss: 6.9949 - val_accuracy: 0.1005\n",
      "Epoch 99/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.5467 - accuracy: 0.8223 - val_loss: 7.0277 - val_accuracy: 0.1002\n",
      "Epoch 100/100\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.5380 - accuracy: 0.8250 - val_loss: 7.1637 - val_accuracy: 0.1017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cb195a4730>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "random_train_labels = train_labels[:]\n",
    "np.random.shuffle(random_train_labels)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, random_train_labels,\n",
    "          epochs=100,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, you don’t even need to do this with MNIST data—you could just generate white noise inputs and random labels. You could fit a model on that, too, as long as it has enough parameters. It would just end up memorizing specific inputs, much like a Python dictionary. <br>\n",
    "If this is the case, then how come deep learning models generalize at all? Shouldn’t they just learn an ad hoc mapping between training inputs and targets, like a fancy dict? What expectation can we have that this mapping will work for new inputs? As it turns out, the nature of generalization in deep learning has rather little to do with deep learning models themselves, and much to do with the structure of information in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating machine learning models\n",
    "You can only control what you can observe. Since your goal is to develop models that can successfully generalize to new data, it’s essential to be able to reliably measure the generalization power of your model. In this section, I’ll formally introduce the different ways you can evaluate machine learning models. You’ve already seen most of them in action in the previous chapter.\n",
    "\n",
    "##### Training, validation, and test sets\n",
    "- Evaluating a model always boils down to splitting the available data into three sets: **training, validation, and test**. \n",
    "- You train on the training data and evaluate your model on the validation data. \n",
    "- Once your model is ready for prime time, you test it one final time on the test data, which is meant to be as similar as possible to production data.\n",
    "- Then you can deploy the model in production. <br>\n",
    "You may ask, why not have two sets: a training set and a test set? You’d train on the training data and evaluate on the test data. Much simpler! <br>\n",
    "\n",
    "The reason is that developing a model always involves tuning its configuration: for example, choosing the number of layers or the size of the layers (called the **hyperparameters** of the model, to distinguish them from the **parameters**, which are the network’s weights). You do this tuning by using as a feedback signal the performance of the model on the validation data. In essence, this tuning is a form of learning: a search for a good configuration in some parameter space. As a result, tuning the configuration of the model based on its performance on the validation set can quickly result in overfitting to the validation set, even though your model is never directly trained on it. <br>\n",
    "\n",
    "Central to this phenomenon is the notion of **information leaks**. Every time you tune a hyperparameter of your model based on the model’s performance on the validation set, some information about the validation data leaks into the model. If you do this only once, for one parameter, then very few bits of information will leak, and your validation set will remain reliable for evaluating the model. But if you repeat this many times—running one experiment, evaluating on the validation set, and modifying your model as a result—then you’ll leak an increasingly significant amount of information about the validation set into the model.\n",
    "\n",
    "At the end of the day, you’ll end up with a model that performs artificially well on the validation data, because that’s what you optimized it for. You care about performance on completely new data, not on the validation data, so you need to use a completely different, **never-before-seen dataset** to evaluate the model: **the test dataset**. <br>\n",
    "Your model shouldn’t have had access to any information about the test set, even indirectly. If anything about the model has been tuned based on test set performance, then your measure of generalization will be flawed.\n",
    "\n",
    "Splitting your data into training, validation, and test sets may seem straightforward, but there are a few advanced ways to do it that can come in handy when little data is available. Let’s review three classic evaluation recipes: simple holdout validation, K-fold validation, and iterated K-fold validation with shuffling. We’ll also talk about the use of common-sense baselines to check that your training is going somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SIMPLE HOLDOUT VALIDATION\n",
    "Set apart some fraction of your data as your test set. Train on the remaining data, and evaluate on the test set. As you saw in the previous sections, in order to prevent information leaks, you shouldn’t tune your model based on the test set, and therefore you should also reserve a validation set. <br>\n",
    "Schematically, holdout validation looks like figure 5.12.\n",
    "\n",
    "![](./images/5.12.png)\n",
    "\n",
    "##### Holdout validation (note that labels are omitted for simplicity)\n",
    "\n",
    "```python\n",
    "num_validation_samples = 10000\n",
    "np.random.shuffle(data) # Shuffling the data is usually appropriate.\n",
    "validation_data = data[:num_validation_samples] # Defines the validation set\n",
    "training_data = data[num_validation_samples:] # Defines the training set\n",
    "model = get_model()\n",
    "model.fit(training_data, ...)\n",
    "validation_score = model.evaluate(validation_data, ...)\n",
    "# Trains a model on the training data, and evaluates it on the validation data\n",
    "... # At this point you can tune your model, retrain it, evaluate it, tune it again.\n",
    "model = get_model()\n",
    "model.fit(np.concatenate([training_data,\n",
    "validation_data]), ...)\n",
    "test_score = model.evaluate(test_data, ...)\n",
    "# Once you’ve tuned your hyperparameters, it’s common to train your final model from scratch on all non-test data available.\n",
    "```\n",
    "\n",
    "This is the simplest evaluation protocol, and it suffers from one flaw: if little data is available, then your validation and test sets may contain too few samples to be statistically representative of the data at hand. This is easy to recognize: if different random shuffling rounds of the data before splitting end up yielding very different measures of model performance, then you’re having this issue. **K-fold validation** and **iterated K-fold validation** are two ways to address this, as discussed next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-FOLD VALIDATION\n",
    "With this approach, you split your data into K partitions of equal size. For each partition i, train a model on the remaining K - 1 partitions, and evaluate it on partition i. <br>\n",
    "Your final score is then the averages of the K scores obtained. This method is helpful when the performance of your model shows significant variance based on your train-test split. Like holdout validation, this method doesn’t exempt you from using a distinct validation set for model calibration. <br>\n",
    "Schematically, K-fold cross-validation looks like figure 5.13.\n",
    "\n",
    "![](./images/5.13.png)\n",
    "\n",
    "##### K-fold cross-validation (note that labels are omitted for simplicity)\n",
    "\n",
    "```python\n",
    "k = 3\n",
    "num_validation_samples = len(data) // k\n",
    "np.random.shuffle(data)\n",
    "validation_scores = []\n",
    "for fold in range(k):\n",
    "    validation_data = data[num_validation_samples * fold:  # Selects the validation data partition\n",
    "        num_validation_samples * (fold + 1)]\n",
    "    training_data = np.concatenate(\n",
    "        data[:num_validation_samples * fold],\n",
    "        data[num_validation_samples * (fold + 1):])\n",
    "    # Uses the remainder of the data as training data. Note that the + operator represents list concatenation, not summation.\n",
    "    model = get_model() # Creates a brand-new instance of the model (untrained)\n",
    "    model.fit(training_data, ...)\n",
    "    validation_score = model.evaluate(validation_data, ...)\n",
    "    validation_scores.append(validation_score)\n",
    "validation_score = np.average(validation_scores) # Validation score: average of the validation scores of the k folds\n",
    "model = get_model()\n",
    "model.fit(data, ...) # Trains the final model on all nontest data available\n",
    "test_score = model.evaluate(test_data, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ITERATED K-FOLD VALIDATION WITH SHUFFLING\n",
    "This one is for situations in which you have relatively little data available and you need to evaluate your model as precisely as possible. I’ve found it to be extremely helpful in Kaggle competitions. It consists of applying K-fold validation multiple times, shuffling the data every time before splitting it K ways. The final score is the average of the scores obtained at each run of K-fold validation. Note that you end up training and evaluating P * K models (where P is the number of iterations you use), which can be very expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Things to keep in mind about model evaluation\n",
    "Keep an eye out for the following when you’re choosing an evaluation protocol:\n",
    "- **Data representativeness**—You want both your training set and test set to be representative of the data at hand. For instance, if you’re trying to classify images of digits, and you’re starting from an array of samples where the samples are ordered by their class, taking the first 80% of the array as your training set and the remaining 20% as your test set will result in your training set containing only classes 0–7, whereas your test set will contain only classes 8–9. This seems like a ridiculous mistake, but it’s surprisingly common. For this reason, you usually should **randomly shuffle your data before splitting it into training and test sets.**\n",
    "- **The arrow of time**—If you’re trying to **predict the future given the past** (for example, tomorrow’s weather, stock movements, and so on), **you should not randomly shuffle your data before splitting it**, because doing so will create a temporal leak: your model will effectively be trained on data from the future. In such situations, you should **always make sure all data in your test set is posterior to the data in the training set.**\n",
    "- **Redundancy in your data**—If some data points in your data **appear twice** (fairly common with real-world data), then shuffling the data and splitting it into a training set and a validation set will result in redundancy between the training and validation sets. In effect, you’ll be testing on part of your training data, which is the worst thing you can do! **Make sure your training set and validation set are disjoint.**\n",
    "\n",
    "Having a reliable way to evaluate the performance of your model is how you’ll be able to monitor the tension at the heart of machine learning—between optimization and generalization, underfitting and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving model fit\n",
    "To achieve the perfect fit, you must first overfit. Since you don’t know in advance where the boundary lies, you must cross it to find it. Thus, your initial goal as you start working on a problem is to achieve a model that shows some generalization power and that is able to overfit. Once you have such a model, you’ll focus on refining generalization by fighting overfitting. <br>\n",
    "There are three common problems you’ll encounter at this stage:\n",
    "- Training doesn’t get started: your training loss doesn’t go down over time.\n",
    "- Training gets started just fine, but your model doesn’t meaningfully generalize: you can’t beat the common-sense baseline you set.\n",
    "- Training and validation loss both go down over time, and you can beat your baseline, but you don’t seem to be able to overfit, which indicates you’re still underfitting.\n",
    "\n",
    "Let’s see how you can address these issues to achieve the first big milestone of a machine learning project: getting a model that has some generalization power (it can beat a trivial baseline) and that is able to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning key gradient descent parameters\n",
    "Sometimes training doesn’t get started, or it stalls too early. Your loss is stuck. This is always something you can overcome: remember that you can fit a model to random data. Even if nothing about your problem makes sense, you should still be able to train something—if only by memorizing the training data. <br>\n",
    "When this happens, it’s always a problem with the configuration of the gradient descent process: your choice of optimizer, the distribution of initial values in the weights of your model, your learning rate, or your batch size. All these parameters are interdependent, and as such it is usually sufficient to tune the learning rate and the batch size while keeping the rest of the parameters constant. <br>\n",
    "Let’s look at a concrete example: let’s train the MNIST model from chapter 2 with an inappropriately large learning rate of value 1.\n",
    "\n",
    "##### Training an MNIST model with an incorrectly high learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 850.0276 - accuracy: 0.3504 - val_loss: 2.2311 - val_accuracy: 0.2449\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 4.6690 - accuracy: 0.2246 - val_loss: 2.4064 - val_accuracy: 0.2244\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 2.8338 - accuracy: 0.2365 - val_loss: 3.8123 - val_accuracy: 0.2853\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 2.9162 - accuracy: 0.2397 - val_loss: 2.3583 - val_accuracy: 0.2748\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 3.4884 - accuracy: 0.2605 - val_loss: 2.1565 - val_accuracy: 0.2674\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 2.6663 - accuracy: 0.2474 - val_loss: 2.4233 - val_accuracy: 0.2835\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 2.6693 - accuracy: 0.2406 - val_loss: 2.2915 - val_accuracy: 0.2049\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 4.6118 - accuracy: 0.2454 - val_loss: 2.7611 - val_accuracy: 0.2625\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 2.4600 - accuracy: 0.2525 - val_loss: 2.1521 - val_accuracy: 0.2457\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 2.6444 - accuracy: 0.2287 - val_loss: 2.0511 - val_accuracy: 0.2472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cb19685fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1.),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model quickly reaches a training and validation accuracy in the 30%–40% range, but cannot get past that. Let’s try to lower the learning rate to a more reasonable value of 1e-2.\n",
    "##### The same model with a more appropriate learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.3776 - accuracy: 0.9130 - val_loss: 0.1412 - val_accuracy: 0.9627\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1413 - accuracy: 0.9640 - val_loss: 0.1630 - val_accuracy: 0.9630\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1162 - accuracy: 0.9730 - val_loss: 0.1516 - val_accuracy: 0.9727\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.1003 - accuracy: 0.9779 - val_loss: 0.2197 - val_accuracy: 0.9687\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.0869 - accuracy: 0.9820 - val_loss: 0.2183 - val_accuracy: 0.9703\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.0757 - accuracy: 0.9850 - val_loss: 0.3012 - val_accuracy: 0.9607\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0738 - accuracy: 0.9864 - val_loss: 0.2440 - val_accuracy: 0.9732\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0672 - accuracy: 0.9876 - val_loss: 0.2502 - val_accuracy: 0.9738\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.0608 - accuracy: 0.9893 - val_loss: 0.2622 - val_accuracy: 0.9759\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.0550 - accuracy: 0.9906 - val_loss: 0.3420 - val_accuracy: 0.9725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cb198428e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-2),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now able to train.\n",
    "If you find yourself in a similar situation, try\n",
    "- **Lowering or increasing the learning rate.** A learning rate that is too high may lead to updates that vastly overshoot a proper fit, like in the preceding example, and a learning rate that is too low may make training so slow that it appears to stall.\n",
    "- **Increasing the batch size.** A batch with more samples will lead to gradients that are more informative and less noisy (lower variance).\n",
    "\n",
    "You will, eventually, find a configuration that gets training started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leveraging better architecture priors\n",
    "You have a model that fits, but for some reason your validation metrics aren’t improving at all. They remain no better than what a random classifier would achieve: your model trains but doesn’t generalize. What’s going on? <br>\n",
    "This is perhaps the worst machine learning situation you can find yourself in. It indicates that something is fundamentally wrong with your approach, and it may not be easy to tell what. Here are some tips.\n",
    "\n",
    "First, it may be that the input data you’re using simply doesn’t contain sufficient information to predict your targets: the problem as formulated is not solvable. This is what happened earlier when we tried to fit an MNIST model where the labels were shuffled: the model would train just fine, but validation accuracy would stay stuck at 10%, because it was plainly impossible to generalize with such a dataset. <br>\n",
    "It may also be that the kind of model you’re using is not suited for the problem at hand. For instance, in chapter 10, you’ll see an example of a timeseries prediction problem where a densely connected architecture isn’t able to beat a trivial baseline, whereas a more appropriate recurrent architecture does manage to generalize well. <br>\n",
    "Using a model that makes the right assumptions about the problem is essential to achieve generalization: you should leverage the right architecture priors.\n",
    "\n",
    "In the following chapters, you’ll learn about the best architectures to use for a variety of data modalities—images, text, timeseries, and so on. In general, you should always make sure to read up on architecture best practices for the kind of task you’re attacking—chances are you’re not the first person to attempt it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Increasing model capacity\n",
    "If you manage to get to a model that fits, where validation metrics are going down, and that seems to achieve at least some level of generalization power, congratulations: you’re almost there. Next, you need to get your model to start overfitting. <br>\n",
    "Consider the following small model—a simple logistic regression—trained on MNIST pixels.\n",
    "\n",
    "##### A simple logistic regression on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6601 - accuracy: 0.8376 - val_loss: 0.3603 - val_accuracy: 0.9059\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.3509 - accuracy: 0.9036 - val_loss: 0.3078 - val_accuracy: 0.9149\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3154 - accuracy: 0.9125 - val_loss: 0.2924 - val_accuracy: 0.9170\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2993 - accuracy: 0.9168 - val_loss: 0.2812 - val_accuracy: 0.9219\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2896 - accuracy: 0.9188 - val_loss: 0.2769 - val_accuracy: 0.9237\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2828 - accuracy: 0.9208 - val_loss: 0.2720 - val_accuracy: 0.9238\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2779 - accuracy: 0.9221 - val_loss: 0.2710 - val_accuracy: 0.9252\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2743 - accuracy: 0.9238 - val_loss: 0.2685 - val_accuracy: 0.9263\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2710 - accuracy: 0.9245 - val_loss: 0.2672 - val_accuracy: 0.9270\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2686 - accuracy: 0.9260 - val_loss: 0.2645 - val_accuracy: 0.9289\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2665 - accuracy: 0.9262 - val_loss: 0.2666 - val_accuracy: 0.9280\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2647 - accuracy: 0.9270 - val_loss: 0.2632 - val_accuracy: 0.9299\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2628 - accuracy: 0.9275 - val_loss: 0.2625 - val_accuracy: 0.9301\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2618 - accuracy: 0.9279 - val_loss: 0.2629 - val_accuracy: 0.9281\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2599 - accuracy: 0.9290 - val_loss: 0.2634 - val_accuracy: 0.9280\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2594 - accuracy: 0.9292 - val_loss: 0.2633 - val_accuracy: 0.9293\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2580 - accuracy: 0.9291 - val_loss: 0.2617 - val_accuracy: 0.9299\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2572 - accuracy: 0.9298 - val_loss: 0.2628 - val_accuracy: 0.9301\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2563 - accuracy: 0.9304 - val_loss: 0.2623 - val_accuracy: 0.9307\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2557 - accuracy: 0.9300 - val_loss: 0.2619 - val_accuracy: 0.9305\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([layers.Dense(10, activation=\"softmax\")])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_small_model = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cb19908430>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_loss = history_small_model.history[\"val_loss\"]\n",
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, val_loss, \"b--\",\n",
    "label=\"Validation loss\")\n",
    "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation metrics seem to stall, or to improve very slowly, instead of peaking and reversing course. The validation loss goes to 0.26 and just stays there. You can fit, but you can’t clearly overfit, even after many iterations over the training data. You’re likely to encounter similar curves often in your career. <br>\n",
    "Remember that it should always be possible to overfit. Much like the problem where the training loss doesn’t go down, this is an issue that can always be solved. **If you can’t seem to be able to overfit**, it’s likely a problem with the representational power of your model: **you’re going to need a bigger model**, one with more capacity, that is to say, one able to store more information. You can increase representational power by adding more layers, using bigger layers (layers with more parameters), or using kinds of layers that are more appropriate for the problem at hand (better architecture priors).\n",
    "\n",
    "Let’s try training a bigger model, one with two intermediate layers with 96 units each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.3526 - accuracy: 0.9007 - val_loss: 0.1913 - val_accuracy: 0.9439\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1589 - accuracy: 0.9528 - val_loss: 0.1341 - val_accuracy: 0.9607\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1127 - accuracy: 0.9666 - val_loss: 0.1159 - val_accuracy: 0.9659\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0865 - accuracy: 0.9736 - val_loss: 0.1022 - val_accuracy: 0.9697\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0694 - accuracy: 0.9786 - val_loss: 0.1012 - val_accuracy: 0.9697\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0963 - val_accuracy: 0.9730\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0479 - accuracy: 0.9859 - val_loss: 0.0998 - val_accuracy: 0.9718\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0402 - accuracy: 0.9878 - val_loss: 0.1008 - val_accuracy: 0.9728\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0337 - accuracy: 0.9896 - val_loss: 0.0949 - val_accuracy: 0.9758\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0283 - accuracy: 0.9918 - val_loss: 0.1050 - val_accuracy: 0.9744\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 0.1098 - val_accuracy: 0.9729\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0209 - accuracy: 0.9936 - val_loss: 0.1037 - val_accuracy: 0.9756\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.1137 - val_accuracy: 0.9733\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0151 - accuracy: 0.9955 - val_loss: 0.1078 - val_accuracy: 0.9755\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9965 - val_loss: 0.1247 - val_accuracy: 0.9759\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.1240 - val_accuracy: 0.9729\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.1345 - val_accuracy: 0.9743\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.1334 - val_accuracy: 0.9727\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.1352 - val_accuracy: 0.9757\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.1535 - val_accuracy: 0.9743\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(96, activation=\"relu\"),\n",
    "    layers.Dense(96, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_large_model = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation curve now looks exactly like it should: the model fits fast and starts overfitting after 8 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cb230c9790>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_loss = history_large_model.history[\"val_loss\"]\n",
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, val_loss, \"b--\",\n",
    "         label=\"Validation loss\")\n",
    "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving generalization\n",
    "Once your model has shown itself to have some generalization power and to be able to overfit, it’s time to switch your focus to maximizing generalization.\n",
    "\n",
    "##### Dataset curation\n",
    "You’ve already learned that generalization in deep learning originates from the latent structure of your data. If your data makes it possible to smoothly interpolate between samples, you will be able to train a deep learning model that generalizes. If your problem is overly noisy or fundamentally discrete, like, say, list sorting, deep learning will not help you. Deep learning is curve fitting, not magic. <br>\n",
    "As such, it is essential that you make sure that you’re working with an appropriate dataset. Spending more effort and money on data collection almost always yields a much greater return on investment than spending the same on developing a better model.\n",
    "- Make sure you have enough data. Remember that you need a **dense sampling** of the input-cross-output space. **More data will yield a better model.** Sometimes, problems that seem impossible at first become solvable with a larger dataset.\n",
    "- Minimize labeling errors—visualize your inputs to check for anomalies, and proofread your labels.\n",
    "- Clean your data and deal with missing values (we’ll cover this in the next chapter).\n",
    "- If you have many features and you aren’t sure which ones are actually useful, do feature selection.\n",
    "\n",
    "A particularly important way to improve the generalization potential of your data is **feature engineering**. For most machine learning problems, feature engineering is a key ingredient for success. Let’s take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering\n",
    "**Feature engineering** is the process of using your own knowledge about the data and about the machine learning algorithm at hand (in this case, a neural network) to make the algorithm work better by applying hardcoded (non-learned) transformations to the data before it goes into the model. In many cases, it isn’t reasonable to expect a machine learning model to be able to learn from completely arbitrary data. The data needs to be presented to the model in a way that will make the model’s job easier.\n",
    "\n",
    "Let’s look at an intuitive example. Suppose you’re trying to develop a model that can take as input an image of a clock and can output the time of day (see figure 5.16).\n",
    "\n",
    "![](./images/5.16.png)\n",
    "\n",
    "If you choose to use the raw pixels of the image as input data, you have a difficult machine learning problem on your hands. You’ll need a convolutional neural network to solve it, and you’ll have to expend quite a bit of computational resources to train the network. <br>\n",
    "But if you already understand the problem at a high level (you understand how humans read time on a clock face), you can come up with much better input features for a machine learning algorithm: for instance, it’s easy to write a five-line Python script to follow the black pixels of the clock hands and output the (x, y) coordinates of the tip of each hand. Then a simple machine learning algorithm can learn to associate these coordinates with the appropriate time of day. <br>\n",
    "You can go even further: do a coordinate change, and express the (x, y) coordinates as polar coordinates with regard to the center of the image. Your input will become the angle **theta** of each clock hand. At this point, your features are making the problem so easy that no machine learning is required; a simple rounding operation and dictionary lookup are enough to recover the approximate time of day.\n",
    "\n",
    "That’s the essence of feature engineering: making a problem easier by expressing it in a simpler way. Make the latent manifold smoother, simpler, better organized. Doing so usually requires understanding the problem in depth.\n",
    "\n",
    "Before deep learning, feature engineering used to be the most important part of the machine learning workflow, because classical shallow algorithms didn’t have hypothesis spaces rich enough to learn useful features by themselves. The way you presented the data to the algorithm was absolutely critical to its success. For instance, before convolutional neural networks became successful on the MNIST digit-classification problem, solutions were typically based on hardcoded features such as the number of loops in a digit image, the height of each digit in an image, a histogram of pixel values, and so on. <br>\n",
    "Fortunately, modern deep learning removes the need for most feature engineering, because neural networks are capable of automatically extracting useful features from raw data. Does this mean you don’t have to worry about feature engineering as long as you’re using deep neural networks? No, for two reasons:\n",
    "- Good features still allow you to solve problems more elegantly while using fewer resources. For instance, it would be ridiculous to solve the problem of reading a clock face using a convolutional neural network.\n",
    "- Good features let you solve a problem with far less data. The ability of deep learning models to learn features on their own relies on having lots of training data available; if you have only a few samples, the information value in their features becomes critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using early stopping\n",
    "In deep learning, we always use models that are vastly overparameterized: they have way more degrees of freedom than the minimum necessary to fit to the latent manifold of the data. This overparameterization is not an issue, because you never fully fit a deep learning model. Such a fit wouldn’t generalize at all. You will always interrupt training long before you’ve reached the minimum possible training loss. <br>\n",
    "Finding the exact point during training where you’ve reached the most generalizable fit—the exact boundary between an underfit curve and an overfit curve—is one of the most effective things you can do to improve generalization. <br>\n",
    "In the examples in the previous chapter, we would start by training our models for longer than needed to figure out the number of epochs that yielded the best validation metrics, and then we would retrain a new model for exactly that number of epochs. This is pretty standard, but it requires you to do redundant work, which can sometimes be expensive. Naturally, you could just save your model at the end of each epoch, and once you’ve found the best epoch, reuse the closest saved model you have.\n",
    "\n",
    "In Keras, it’s typical to do this with an **EarlyStopping** callback, which will interrupt training as soon as validation metrics have stopped improving, while remembering the best known model state. You’ll learn to use callbacks in chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regularizing your model\n",
    "Regularization techniques are a set of best practices that actively impede the model’s ability to fit perfectly to the training data, with the goal of making the model perform better during validation. This is called “regularizing” the model, because it tends to make the model simpler, more “regular,” its curve smoother, more “generic”; thus it is less specific to the training set and better able to generalize by more closely approximating the latent manifold of the data. <br>\n",
    "Keep in mind that regularizing a model is a process that should always be guided by an accurate evaluation procedure. You will only achieve generalization if you can\n",
    "measure it. <br>\n",
    "Let’s review some of the most common regularization techniques and apply them in practice to improve the movie-classification model from chapter 4.\n",
    "\n",
    "##### REDUCING THE NETWORK’S SIZE\n",
    "You’ve already learned that a model that is too small will not overfit. The simplest way to mitigate overfitting is to **reduce the size of the model (the number of learnable parameters in the model, determined by the number of layers and the number of units per layer)**. If the model has limited memorization resources, it won’t be able to simply memorize its training data; thus, in order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets—precisely the type of representations we’re interested in. **At the same time, keep in mind that you should use models that have enough parameters that they don’t underfit: your model shouldn’t be starved for memorization resources.** There is a compromise to be found between too much capacity and not enough capacity. <br>\n",
    "Unfortunately, there is no magical formula to determine the right number of layers or the right size for each layer. You must evaluate an array of different architectures (on your validation set, not on your test set, of course) in order to find the correct model size for your data. **The general workflow for finding an appropriate model size is to start with relatively few layers and parameters, and increase the size of the layers or add new layers until you see diminishing returns with regard to validation loss.**\n",
    "\n",
    "Let’s try this on the movie-review classification model. The following listing shows our original model.\n",
    "\n",
    "##### Original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 2s 33ms/step - loss: 0.5184 - accuracy: 0.7773 - val_loss: 0.3863 - val_accuracy: 0.8678\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3069 - accuracy: 0.9019 - val_loss: 0.3443 - val_accuracy: 0.8594\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2232 - accuracy: 0.9280 - val_loss: 0.2865 - val_accuracy: 0.8872\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1760 - accuracy: 0.9431 - val_loss: 0.2768 - val_accuracy: 0.8906\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1429 - accuracy: 0.9538 - val_loss: 0.2855 - val_accuracy: 0.8876\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1180 - accuracy: 0.9631 - val_loss: 0.3009 - val_accuracy: 0.8847\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0987 - accuracy: 0.9703 - val_loss: 0.3201 - val_accuracy: 0.8838\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0814 - accuracy: 0.9770 - val_loss: 0.3344 - val_accuracy: 0.8831\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0670 - accuracy: 0.9819 - val_loss: 0.3900 - val_accuracy: 0.8733\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0560 - accuracy: 0.9858 - val_loss: 0.3887 - val_accuracy: 0.8770\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0455 - accuracy: 0.9890 - val_loss: 0.4145 - val_accuracy: 0.8767\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0374 - accuracy: 0.9909 - val_loss: 0.4928 - val_accuracy: 0.8662\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.0285 - accuracy: 0.9947 - val_loss: 0.4789 - val_accuracy: 0.8738\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0250 - accuracy: 0.9947 - val_loss: 0.5086 - val_accuracy: 0.8716\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0194 - accuracy: 0.9965 - val_loss: 0.5416 - val_accuracy: 0.8712\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0155 - accuracy: 0.9978 - val_loss: 0.5762 - val_accuracy: 0.8673\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0142 - accuracy: 0.9967 - val_loss: 0.6137 - val_accuracy: 0.8670\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0072 - accuracy: 0.9993 - val_loss: 0.6686 - val_accuracy: 0.8656\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0086 - accuracy: 0.9984 - val_loss: 0.6836 - val_accuracy: 0.8659\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0042 - accuracy: 0.9996 - val_loss: 0.9376 - val_accuracy: 0.8435\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data, train_labels), _ = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "train_data = vectorize_sequences(train_data)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_original = model.fit(train_data, train_labels,\n",
    "                             epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s try to replace it with this smaller model.\n",
    "##### Version of the model with lower capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.6504 - accuracy: 0.6224 - val_loss: 0.6103 - val_accuracy: 0.7478\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.5789 - accuracy: 0.7457 - val_loss: 0.5608 - val_accuracy: 0.7669\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.5299 - accuracy: 0.8067 - val_loss: 0.5265 - val_accuracy: 0.8365\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.4924 - accuracy: 0.8478 - val_loss: 0.4996 - val_accuracy: 0.8282\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4614 - accuracy: 0.8754 - val_loss: 0.4807 - val_accuracy: 0.8373\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4347 - accuracy: 0.8990 - val_loss: 0.4715 - val_accuracy: 0.8286\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.4119 - accuracy: 0.9136 - val_loss: 0.4601 - val_accuracy: 0.8402\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3914 - accuracy: 0.9282 - val_loss: 0.4431 - val_accuracy: 0.8685\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3726 - accuracy: 0.9402 - val_loss: 0.4390 - val_accuracy: 0.8695\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3557 - accuracy: 0.9494 - val_loss: 0.4277 - val_accuracy: 0.8803\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3378 - accuracy: 0.9582 - val_loss: 0.4386 - val_accuracy: 0.8648\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.3052 - accuracy: 0.9633 - val_loss: 0.3892 - val_accuracy: 0.8749\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2404 - accuracy: 0.9681 - val_loss: 0.3305 - val_accuracy: 0.8895\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1870 - accuracy: 0.9719 - val_loss: 0.3077 - val_accuracy: 0.8899\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.1505 - accuracy: 0.9734 - val_loss: 0.3043 - val_accuracy: 0.8869\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1266 - accuracy: 0.9775 - val_loss: 0.3107 - val_accuracy: 0.8837\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1105 - accuracy: 0.9786 - val_loss: 0.3131 - val_accuracy: 0.8850\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.0976 - accuracy: 0.9813 - val_loss: 0.3161 - val_accuracy: 0.8855\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0874 - accuracy: 0.9825 - val_loss: 0.3420 - val_accuracy: 0.8835\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.0770 - accuracy: 0.9847 - val_loss: 0.3456 - val_accuracy: 0.8834\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(4, activation=\"relu\"),\n",
    "    layers.Dense(4, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_smaller_model = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smaller model starts overfitting later than the reference model, and its performance degrades more slowly once it starts overfitting.\n",
    "\n",
    "Now, let’s add to our benchmark a model that has much more capacity—far more than the problem warrants. While it is standard to work with models that are significantly overparameterized for what they’re trying to learn, there can definitely be such a thing as too much memorization capacity. You’ll know your model is too large if it starts overfitting right away and if its validation loss curve looks choppy with highvariance (although choppy validation metrics could also be a symptom of using an unreliable validation process, such as a validation split that’s too small).\n",
    "\n",
    "##### Version of the model with higher capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 4s 105ms/step - loss: 0.5551 - accuracy: 0.7467 - val_loss: 0.3028 - val_accuracy: 0.8856\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 0.2565 - accuracy: 0.8992 - val_loss: 0.4158 - val_accuracy: 0.8269\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 3s 98ms/step - loss: 0.1596 - accuracy: 0.9399 - val_loss: 0.2986 - val_accuracy: 0.8921\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 3s 97ms/step - loss: 0.0736 - accuracy: 0.9741 - val_loss: 0.5105 - val_accuracy: 0.8515\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 3s 94ms/step - loss: 0.1118 - accuracy: 0.9761 - val_loss: 0.3396 - val_accuracy: 0.8854\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 3s 92ms/step - loss: 0.0061 - accuracy: 0.9994 - val_loss: 0.4922 - val_accuracy: 0.8854\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 3s 93ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.6020 - val_accuracy: 0.8852\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 3s 89ms/step - loss: 0.1767 - accuracy: 0.9767 - val_loss: 0.5126 - val_accuracy: 0.8687\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 3s 92ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5394 - val_accuracy: 0.8839\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 3s 93ms/step - loss: 2.0618e-04 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 0.8835\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 3s 88ms/step - loss: 6.2563e-05 - accuracy: 1.0000 - val_loss: 0.7066 - val_accuracy: 0.8834\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 1.7792e-05 - accuracy: 1.0000 - val_loss: 0.8021 - val_accuracy: 0.8845\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 0.2234 - accuracy: 0.9771 - val_loss: 0.7606 - val_accuracy: 0.8654\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 3s 87ms/step - loss: 4.2578e-04 - accuracy: 1.0000 - val_loss: 0.6551 - val_accuracy: 0.8796\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 5.3767e-05 - accuracy: 1.0000 - val_loss: 0.6682 - val_accuracy: 0.8835\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 2.1860e-05 - accuracy: 1.0000 - val_loss: 0.7148 - val_accuracy: 0.8834\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 3s 85ms/step - loss: 9.6153e-06 - accuracy: 1.0000 - val_loss: 0.7833 - val_accuracy: 0.8824\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 3s 90ms/step - loss: 3.5955e-06 - accuracy: 1.0000 - val_loss: 0.8710 - val_accuracy: 0.8825\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 3s 91ms/step - loss: 1.2312e-06 - accuracy: 1.0000 - val_loss: 0.9652 - val_accuracy: 0.8821\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 3s 88ms/step - loss: 4.3009e-07 - accuracy: 1.0000 - val_loss: 1.0438 - val_accuracy: 0.8854\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_larger_model = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigger model starts overfitting almost immediately, after just one epoch, and it overfits much more severely. Its validation loss is also noisier. It gets training loss near zero very quickly. The more capacity the model has, the more quickly it can model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large difference between the training and validation loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADDING WEIGHT REGULARIZATION\n",
    "You may be familiar with the principle of Occam’s razor: given two explanations for something, the explanation most likely to be correct is the simplest one—the one that makes fewer assumptions. This idea also applies to the models learned by neural networks: given some training data and a network architecture, multiple sets of weight values (multiple models) could explain the data. Simpler models are less likely to overfit than complex ones. <br>\n",
    "A simple model in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters, as you saw in the previous section). <br>\n",
    "Thus, a common way to mitigate overfitting is to put constraints on the complexity of a model by forcing its weights to take only small values, which makes the distribution of weight values more regular. This is called **weight regularization**, and it’s done by adding to the loss function of the model a cost associated with having large weights. This cost comes in two flavors:\n",
    "- **L1 regularization**—The cost added is proportional to the absolute value of the weight coefficients (the L1 norm of the weights).\n",
    "- **L2 regularization**—The cost added is proportional to the square of the value of the weight coefficients (the L2 norm of the weights). L2 regularization is also called **weight decay** in the context of neural networks. Don’t let the different name confuse you: weight decay is mathematically the same as L2 regularization.\n",
    "\n",
    "In Keras, weight regularization is added by passing **weight regularizer** instances to layers as keyword arguments. Let’s add L2 weight regularization to our initial movie-review classification model.\n",
    "\n",
    "##### Adding L2 weight regularization to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 2s 29ms/step - loss: 0.5795 - accuracy: 0.7941 - val_loss: 0.4836 - val_accuracy: 0.8350\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.3936 - accuracy: 0.8937 - val_loss: 0.3894 - val_accuracy: 0.8840\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3331 - accuracy: 0.9115 - val_loss: 0.3656 - val_accuracy: 0.8872\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3016 - accuracy: 0.9243 - val_loss: 0.3602 - val_accuracy: 0.8867\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2820 - accuracy: 0.9320 - val_loss: 0.3619 - val_accuracy: 0.8874\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2716 - accuracy: 0.9350 - val_loss: 0.3631 - val_accuracy: 0.8842\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2607 - accuracy: 0.9381 - val_loss: 0.3632 - val_accuracy: 0.8845\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2532 - accuracy: 0.9397 - val_loss: 0.3828 - val_accuracy: 0.8788\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2499 - accuracy: 0.9402 - val_loss: 0.3729 - val_accuracy: 0.8828\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2395 - accuracy: 0.9471 - val_loss: 0.3928 - val_accuracy: 0.8775\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2342 - accuracy: 0.9479 - val_loss: 0.4076 - val_accuracy: 0.8723\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2308 - accuracy: 0.9488 - val_loss: 0.3981 - val_accuracy: 0.8740\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2311 - accuracy: 0.9483 - val_loss: 0.3868 - val_accuracy: 0.8787\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2218 - accuracy: 0.9523 - val_loss: 0.4044 - val_accuracy: 0.8769\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2301 - accuracy: 0.9495 - val_loss: 0.4037 - val_accuracy: 0.8748\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.2173 - accuracy: 0.9543 - val_loss: 0.4700 - val_accuracy: 0.8539\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2140 - accuracy: 0.9547 - val_loss: 0.4008 - val_accuracy: 0.8760\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2178 - accuracy: 0.9519 - val_loss: 0.4043 - val_accuracy: 0.8773\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2087 - accuracy: 0.9571 - val_loss: 0.4240 - val_accuracy: 0.8698\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.2139 - accuracy: 0.9539 - val_loss: 0.4119 - val_accuracy: 0.8760\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002),\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002),\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_l2_reg = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding listing, l2(0.002) means every coefficient in the weight matrix of\n",
    "the layer will add 0.002 * weight_coefficient_value ** 2 to the total loss of the\n",
    "model. Note that because this penalty is only added at training time, the loss for this\n",
    "model will be much higher at training than at test time.\n",
    "The plot below shows the impact of the L2 regularization penalty. As you can see, the model with L2 regularization has become much more resistant to overfitting than the reference model, even though both models have the same number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the validation loss of original model and l2 regularized model\n",
    "import matplotlib.pyplot as plt\n",
    "val_loss = history_original.history[\"val_loss\"]\n",
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, val_loss, \"b--\", label=\"Original model\")\n",
    "val_loss_l2 = history_l2_reg.history[\"val_loss\"]\n",
    "plt.plot(epochs, val_loss_l2, \"r--\", label=\"L2 regularized model\")\n",
    "plt.title(\"Validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to L2 regularization, you can use one of the following Keras weight regularizers.\n",
    "##### Different weight regularizers available in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.regularizers.L1L2 at 0x1cb27e62dc0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "regularizers.l1(0.001) # L1 regularization\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001) # Simultaneous L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **weight regularization is more typically used for smaller deep learning models**. Large deep learning models tend to be so overparameterized that imposing constraints on weight values hasn’t much impact on model capacity and generalization. <br>\n",
    "In these cases, a different regularization technique is preferred: **dropout**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADDING DROPOUT\n",
    "**Dropout** is one of the most effective and most commonly used regularization techniques for neural networks; it was developed by Geoff Hinton and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training. Let’s say a given layer would normally return a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training. After applying dropout, this vector will have a few zero entries distributed at random: for example, [0, 0.5, 1.3, 0, 1.1]. The dropout rate is the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5. <br>\n",
    "At test time, no units are dropped out; instead, **the layer’s output values are scaled down by a factor equal to the dropout rate**, to balance for the fact that more units are active than at training time.\n",
    "\n",
    "Consider a NumPy matrix containing the output of a layer, layer_output, of shape (batch_size, features). At training time, we zero out at random a fraction of the values in the matrix:\n",
    "\n",
    "```python\n",
    "layer_output *= np.random.randint(0, high=2, size=layer_output.shape) \n",
    "# At training time, drops out 50% of the units in the output\n",
    "```\n",
    "\n",
    "At test time, we scale down the output by the dropout rate. Here, we scale by 0.5 (because we previously dropped half the units):\n",
    "\n",
    "```python\n",
    "layer_output *= 0.5 # At test time\n",
    "```\n",
    "\n",
    "Note that this process can be implemented by doing both operations at training time and leaving the output unchanged at test time, which is often the way it’s implemented in practice (see figure 5.20):\n",
    "\n",
    "```python\n",
    "layer_output *= np.random.randint(0, high=2, size=layer_output.shape) # At training time\n",
    "layer_output /= 0.5 # Note that we’re scaling up rather than scaling down in this case.\n",
    "```\n",
    "\n",
    "![](./images/5.20.png)\n",
    "\n",
    "This technique may seem strange and arbitrary. Why would this help reduce overfitting? Hinton says he was inspired by, among other things, a fraud-prevention mechanism used by banks. In his own words, “I went to my bank. The tellers kept changing and I asked one of them why. He said he didn’t know but they got moved around a lot. <br>\n",
    "I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting.” The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that aren’t significant (what Hinton refers to as conspiracies), which the model will start memorizing if no noise is present.\n",
    "\n",
    "In Keras, you can introduce dropout in a model via the **Dropout layer**, which is applied to the output of the layer right before it. Let’s add two Dropout layers in the IMDB model to see how well they do at reducing overfitting.\n",
    "\n",
    "##### Adding dropout to the IMDB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 2s 28ms/step - loss: 0.6350 - accuracy: 0.6377 - val_loss: 0.5485 - val_accuracy: 0.8296\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.5208 - accuracy: 0.7698 - val_loss: 0.4306 - val_accuracy: 0.8655\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4383 - accuracy: 0.8303 - val_loss: 0.3747 - val_accuracy: 0.8700\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3789 - accuracy: 0.8629 - val_loss: 0.3236 - val_accuracy: 0.8809\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.3282 - accuracy: 0.8933 - val_loss: 0.2861 - val_accuracy: 0.8927\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2821 - accuracy: 0.9116 - val_loss: 0.2757 - val_accuracy: 0.8923\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2534 - accuracy: 0.9238 - val_loss: 0.2914 - val_accuracy: 0.8881\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2269 - accuracy: 0.9315 - val_loss: 0.2827 - val_accuracy: 0.8916\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.2002 - accuracy: 0.9388 - val_loss: 0.3060 - val_accuracy: 0.8868\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1780 - accuracy: 0.9466 - val_loss: 0.3067 - val_accuracy: 0.8914\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1633 - accuracy: 0.9501 - val_loss: 0.3117 - val_accuracy: 0.8915\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1475 - accuracy: 0.9558 - val_loss: 0.3385 - val_accuracy: 0.8884\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1382 - accuracy: 0.9593 - val_loss: 0.3559 - val_accuracy: 0.8886\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1242 - accuracy: 0.9646 - val_loss: 0.3896 - val_accuracy: 0.8885\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.1185 - accuracy: 0.9658 - val_loss: 0.4041 - val_accuracy: 0.8867\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1084 - accuracy: 0.9695 - val_loss: 0.4187 - val_accuracy: 0.8859\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.1014 - accuracy: 0.9704 - val_loss: 0.4621 - val_accuracy: 0.8873\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.0948 - accuracy: 0.9737 - val_loss: 0.5147 - val_accuracy: 0.8834\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.0911 - accuracy: 0.9721 - val_loss: 0.5155 - val_accuracy: 0.8861\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.0857 - accuracy: 0.9772 - val_loss: 0.5754 - val_accuracy: 0.8807\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_dropout = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows the results. This is a clear improvement over the reference model—it also seems to be working much better than L2 regularization, since the lowest validation loss reached has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the validation loss of original model and l2 regularized model\n",
    "import matplotlib.pyplot as plt\n",
    "val_loss = history_original.history[\"val_loss\"]\n",
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, val_loss, \"b--\", label=\"Validation loss of original model\")\n",
    "val_loss_l2 = history_dropout.history[\"val_loss\"]\n",
    "plt.plot(epochs, val_loss_l2, \"r--\", label=\"Validation loss of dropout regularized model\")\n",
    "plt.title(\"Validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, these are the most common ways to maximize generalization and prevent overfitting in neural networks:\n",
    "- Get more training data, or better training data.\n",
    "- Develop better features.\n",
    "- Reduce the capacity of the model.\n",
    "- Add weight regularization (for smaller models).\n",
    "- Add dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "- The purpose of a machine learning model is to generalize: to perform accurately on never-before-seen inputs. It’s harder than it seems.\n",
    "- A deep neural network achieves generalization by learning a parametric model that can successfully interpolate between training samples—such a model can be said to have learned the “latent manifold” of the training data. This is why deep learning models can only make sense of inputs that are very close to what they’ve seen during training.\n",
    "- The fundamental problem in machine learning is the tension between optimization and generalization: to attain generalization, you must first achieve a good fit to the training data, but improving your model’s fit to the training data will inevitably start hurting generalization after a while. Every single deep learning best practice deals with managing this tension.\n",
    "- The ability of deep learning models to generalize comes from the fact that they manage to learn to approximate the latent manifold of their data, and can thus make sense of new inputs via interpolation.\n",
    "- It’s essential to be able to accurately evaluate the generalization power of your model while you’re developing it. You have at your disposal an array of evaluation methods, from simple holdout validation to K-fold cross-validation and iterated K-fold cross-validation with shuffling. Remember to always keep a completely separate test set for final model evaluation, since information leaks from your validation data to your model may have occurred.\n",
    "- When you start working on a model, your goal is first to achieve a model that has some generalization power and that can overfit. Best practices for doing this include tuning your learning rate and batch size, leveraging better architecture priors, increasing model capacity, or simply training longer.\n",
    "- As your model starts overfitting, your goal switches to improving generalization through model regularization. You can reduce your model’s capacity, add dropout or weight regularization, and use early stopping. And naturally, a larger or better dataset is always the number one way to help a model generalize."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483abfc7fdcd927bfa336910f494643cce94b3dfa36bcded073270b8df64edb3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
