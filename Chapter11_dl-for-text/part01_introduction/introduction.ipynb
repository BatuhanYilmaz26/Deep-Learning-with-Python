{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning for text\n",
    "#### Preparing text data\n",
    "Deep learning models, being differentiable functions, can only process numeric tensors: they can’t take raw text as input. **Vectorizing** text is the process of transforming text into numeric tensors. **Text vectorization** processes come in many shapes and forms, but they all follow the same template (see figure 11.1):\n",
    "- First, you **standardize** the text to make it easier to process, such as by **converting it to lowercase** or **removing punctuation**.\n",
    "- You split the text into units (called **tokens**), such as characters, words, or groups of words. \n",
    "  - This is called **tokenization**.\n",
    "- You convert each such token into a numerical vector. \n",
    "  - This will usually involve first **indexing** all tokens present in the data.\n",
    "\n",
    "Let’s review each of these steps.\n",
    "\n",
    "![](./images/11.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text standardization\n",
    "Consider these two sentences:\n",
    "- “sunset came. i was staring at the Mexico sky. Isnt nature splendid??”\n",
    "- “Sunset came; I stared at the México sky. Isn’t nature splendid?”\n",
    "\n",
    "They’re very similar—in fact, they’re almost identical. Yet, if you were to convert them to byte strings, they would end up with very different representations, because “i” and “I” are two different characters, “Mexico” and “México” are two different words, “isnt” isn’t “isn’t,” and so on. A machine learning model doesn’t know a priori that “i” and “I” are the same letter, that “é” is an “e” with an accent, or that “staring” and “stared” are two forms of the same verb. <br>\n",
    "Text standardization is a basic form of feature engineering that aims to erase encoding differences that you don’t want your model to have to deal with. It’s not exclusive to machine learning, either—you’d have to do the same thing if you were building a search engine. <br>\n",
    "One of the simplest and most widespread standardization schemes is “**convert to lowercase and remove punctuation characters**.” Our two sentences would become\n",
    "- “sunset came i was staring at the mexico sky isnt nature splendid”\n",
    "- “sunset came i stared at the méxico sky isnt nature splendid”\n",
    "\n",
    "Much closer already. Another common transformation is to **convert special characters to a standard form**, such as replacing “é” with “e,” “æ” with “ae,” and so on. Our token “méxico” would then become “mexico”. <br>\n",
    "Lastly, a much more advanced standardization pattern that is more rarely used in a machine learning context is **stemming**: converting variations of a term (such as different conjugated forms of a verb) into a single shared representation, like turning “caught” and “been catching” into “[catch]” or “cats” into “[cat]”. With stemming, “was staring” and “stared” would become something like “[stare]”, and our two similar sentences would finally end up with an identical encoding:\n",
    "- “sunset came i [stare] at the mexico sky isnt nature splendid”\n",
    "\n",
    "With these standardization techniques, your model will require less training data and will generalize better—it won’t need abundant examples of both “Sunset” and “sunset” to learn that they mean the same thing, and it will be able to make sense of “México” even if it has only seen “mexico” in its training set. Of course, standardization may also erase some amount of information, so always keep the context in mind: for instance, if you’re writing a model that extracts questions from interview articles, it should definitely treat “?” as a separate token instead of dropping it, because it’s a useful signal for this specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text splitting (tokenization)\n",
    "Once your text is standardized, you need to break it up into units to be vectorized (tokens), a step called **tokenization**. You could do this in three different ways:\n",
    "- **Word-level tokenization**—Where tokens are space-separated (or punctuation-separated) substrings. \n",
    "  - A variant of this is to further split words into subwords when applicable—for instance, treating “staring” as “star+ing” or “called” as “call+ed.”\n",
    "- **N-gram tokenization**—Where tokens are groups of N consecutive words. \n",
    "  - For instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).\n",
    "- **Character-level tokenization**—Where each character is its own token. \n",
    "  - In practice, this scheme is rarely used, and you only really see it in specialized contexts, like text generation or speech recognition.\n",
    "\n",
    "In general, you’ll always use either **word-level** or **N-gram tokenization**. There are two kinds of text-processing models: those that care about word order, called **sequence models**, and those that treat input words as a set, discarding their original order, called **bag-of-words** models. If you’re building a **sequence model**, you’ll use **word-level tokenization**, and if you’re building a **bag-of-words model**, you’ll use **N-gram tokenization**. <br>\n",
    "N-grams are a way to artificially inject a small amount of local word order information into the model. Throughout this chapter, you’ll learn more about each type of model and when to use them.\n",
    "\n",
    "##### Understanding N-grams and bag-of-words\n",
    "**Word N-grams** are groups of N (or fewer) consecutive words that you can extract from a sentence. The same concept may also be applied to characters instead of words.<br>\n",
    "Here’s a simple example. Consider the sentence “the cat sat on the mat.” It may be decomposed into the following set of 2-grams: \n",
    "```python\n",
    "{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n",
    "\"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n",
    "```\n",
    "It may also be decomposed into the following set of 3-grams:\n",
    "```python\n",
    "{\"the\", \"the cat\", \"cat\", \"cat sat\", \"the cat sat\",\n",
    "\"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\",\n",
    "\"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n",
    "```\n",
    "Such a set is called a bag-of-2-grams or bag-of-3-grams, respectively. The term “bag” here refers to the fact that you’re dealing with a set of tokens rather than a list or sequence: the tokens have no specific order. This family of tokenization methods is called **bag-of-words** (or **bag-of-N-grams**).\n",
    "\n",
    "Because **bag-of-words isn’t an order-preserving tokenization method** (the tokens generated are understood as a set, not a sequence, and the general structure of the sentences is lost), **it tends to be used in shallow language-processing models** rather than in deep learning models. Extracting N-grams is a form of feature engineering, and deep learning sequence models do away with this manual approach, replacing it with hierarchical feature learning. One-dimensional convnets, recurrent neural networks, and Transformers are capable of learning representations for groups of words and characters without being explicitly told about the existence of such groups, by looking at continuous word or character sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vocabulary indexing\n",
    "Once your text is split into tokens, you need to **encode each token into a numerical representation**. You could potentially do this in a stateless way, such as by hashing each token into a fixed binary vector, but in practice, the way you’d go about it is to build an index of all terms found in the training data (the “**vocabulary**”), and assign a unique integer to each entry in the vocabulary. <br>\n",
    "Something like this:\n",
    "```python\n",
    "vocabulary = {}\n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)\n",
    "```\n",
    "You can then convert that integer into a vector encoding that can be processed by a neural network, like a one-hot vector:\n",
    "```python\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1\n",
    "    return vector\n",
    "```\n",
    "Note that at this step it’s common to restrict the vocabulary to only the top 20,000 or 30,000 most common words found in the training data. Any text dataset tends to feature an extremely large number of unique terms, most of which only show up once or twice—indexing those rare terms would result in an excessively large feature space, where most features would have almost no information content. <br>\n",
    "Remember when you were training your first deep learning models on the IMDB dataset in chapters 4 and 5? The data you were using from **keras.datasets.imdb** was already preprocessed into sequences of integers, where each integer stood for a given word. Back then, we used the setting **num_words=10000**, in order to restrict our vocabulary to the **top 10,000 most common words found in the training data**. <br>\n",
    "Now, there’s an important detail here that we shouldn’t overlook: when we look up a new token in our vocabulary index, it may not necessarily exist. Your training data may not have contained any instance of the word “cherimoya” (or maybe you excluded it from your index because it was too rare), so doing **token_index = vocabulary[\"cherimoya\"]** may result in a KeyError. To handle this, you should use an **“out of vocabulary” index** (abbreviated as OOV index)—a catch-all for any token that wasn’t in the index. **It’s usually index 1**: you’re actually doing **token_index = vocabulary.get(token, 1)**. **When decoding a sequence of integers back into words, you’ll replace 1 with something like “[UNK]” (which you’d call an “OOV token”)**. <br>\n",
    "“Why use 1 and not 0?” you may ask. That’s because 0 is already taken. There are two special tokens that you will commonly use: the **OOV token (index 1)**, and the **mask token (index 0)**. While \n",
    "- **the OOV token means “here was a word we did not recognize,”** \n",
    "- **the mask token tells us “ignore me, I’m not a word.”** \n",
    "\n",
    "You’d use it in particular to pad sequence data: because data batches need to be contiguous, **all sequences in a batch of sequence data must have the same length, so shorter sequences should be padded to the length of the longest sequence**. If you want to make a batch of data with the sequences [5, 7, 124, 4, 89] and [8, 34, 21], it would have to look like this:\n",
    "```python\n",
    "[[5, 7, 124, 4, 89]\n",
    "[8, 34, 21, 0, 0]]\n",
    "```\n",
    "The batches of integer sequences for the IMDB dataset that you worked with in chapters 4 and 5 were padded with zeros in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the TextVectorization layer\n",
    "Every step I’ve introduced so far would be very easy to implement in pure Python. <br>\n",
    "Maybe you could write something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "    \n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "    \n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using something like this wouldn’t be very performant. In practice, you’ll work with the Keras **TextVectorization** layer, which is fast and efficient and can be dropped directly into a **tf.data** pipeline or a Keras model. <br>\n",
    "This is what the TextVectorization layer looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\", # Configures the layer to return sequences of words encoded as integer indices. \n",
    "    #There are several other output modes available, which you will see in action in a bit.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the **TextVectorization** layer will use the setting **“convert to lowercase and remove punctuation” for text standardization**, and **“split on whitespace” for tokenization**. But importantly, you can provide custom functions for standardization and tokenization, which means the layer is flexible enough to handle any use case. Note that such custom functions should operate on **tf.string** tensors, not regular Python strings! <br>\n",
    "For instance, the default layer behavior is equivalent to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor) # Convert strings to lowercase.\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\") # Replace punctuation characters with the empty string.\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor) # Split strings on whitespace.\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To index the vocabulary of a text corpus, just call the **adapt()** method of the layer with a **Dataset** object that yields strings, or just with a list of Python strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can retrieve the computed vocabulary via **get_vocabulary()**—this can be useful if you need to convert text encoded as integer sequences back into words.<br>\n",
    "The first two entries in the vocabulary are the mask token (index 0) and the OOV token (index 1). Entries in the vocabulary list are sorted by frequency, so with a real world dataset, very common words like “the” or “a” would come first.\n",
    "\n",
    "##### Displaying the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a demonstration, let’s try to encode and then decode an example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve now learned everything you need to know about text preprocessing—let’s move on to the modeling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two approaches for representing groups of words: Sets and sequences\n",
    "How a machine learning model should represent individual words is a relatively uncontroversial question: they’re categorical features (values from a predefined set), and we know how to handle those. They should be encoded as dimensions in a feature space, or as category vectors (word vectors in this case). A much more problematic question, however, is how to encode the way words are woven into sentences: word order. <br>\n",
    "The problem of order in natural language is an interesting one: unlike the steps of a timeseries, words in a sentence don’t have a natural, canonical order. Different languages order similar words in very different ways. For instance, the sentence structure of English is quite different from that of Japanese. Even within a given language, you can typically say the same thing in different ways by reshuffling the words a bit. Even further, if you fully randomize the words in a short sentence, you can still largely figure out what it was saying—though in many cases significant ambiguity seems to arise. Order is clearly important, but its relationship to meaning isn’t straightforward. <br>\n",
    "How to represent word order is the pivotal question from which different kinds of NLP architectures spring. \n",
    "- The simplest thing you could do is just discard order and treat text as an unordered set of words—this gives you **bag-of-words models**. \n",
    "- You could also decide that words should be processed strictly in the order in which they appear, one at a time, like steps in a timeseries—you could then leverage the **recurrent models** from the last chapter. \n",
    "- Finally, a hybrid approach is also possible: the **Transformer** architecture is technically order-agnostic, yet it injects word-position information into the representations it processes, which enables it to simultaneously look at different parts of a sentence (unlike RNNs) while still being order-aware. Because they take into account word order, both **RNNs** and **Transformers** are called **sequence models**.\n",
    "\n",
    "Historically, most early applications of machine learning to NLP just involved **bag-of-words models**. Interest in **sequence models** only started rising in 2015, with the rebirth of **recurrent neural networks**. Today, both approaches remain relevant. Let’s see how they work, and when to leverage which.\n",
    "\n",
    "We’ll demonstrate each approach on a well-known text classification benchmark: the IMDB movie review sentiment-classification dataset. In chapters 4 and 5, you worked with a prevectorized version of the IMDB dataset; now, let’s process the raw IMDB text data, just like you would do when approaching a new text-classification problem in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing the IMDB movie reviews data\n",
    "Let’s start by downloading the dataset from the Stanford page of Andrew Maas and uncompressing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0 32768    0     0  20945      0  1:06:56  0:00:01  1:06:55 20951\n",
      "  0 80.2M    0  384k    0     0   157k      0  0:08:40  0:00:02  0:08:38  157k\n",
      "  1 80.2M    1 1296k    0     0   377k      0  0:03:37  0:00:03  0:03:34  377k\n",
      "  2 80.2M    2 2432k    0     0   547k      0  0:02:29  0:00:04  0:02:25  547k\n",
      "  4 80.2M    4 3744k    0     0   689k      0  0:01:59  0:00:05  0:01:54  756k\n",
      "  6 80.2M    6 5072k    0     0   788k      0  0:01:44  0:00:06  0:01:38 1034k\n",
      "  7 80.2M    7 6480k    0     0   871k      0  0:01:34  0:00:07  0:01:27 1219k\n",
      "  8 80.2M    8 7248k    0     0   859k      0  0:01:35  0:00:08  0:01:27 1190k\n",
      "  9 80.2M    9 7680k    0     0   813k      0  0:01:40  0:00:09  0:01:31 1049k\n",
      "  9 80.2M    9 8064k    0     0   772k      0  0:01:46  0:00:10  0:01:36  863k\n",
      " 10 80.2M   10 8464k    0     0   740k      0  0:01:50  0:00:11  0:01:39  678k\n",
      " 10 80.2M   10 8848k    0     0   711k      0  0:01:55  0:00:12  0:01:43  473k\n",
      " 11 80.2M   11 9136k    0     0   677k      0  0:02:01  0:00:13  0:01:48  374k\n",
      " 11 80.2M   11 9504k    0     0   658k      0  0:02:04  0:00:14  0:01:50  364k\n",
      " 12 80.2M   12 9888k    0     0   640k      0  0:02:08  0:00:15  0:01:53  365k\n",
      " 12 80.2M   12 10.0M    0     0   624k      0  0:02:11  0:00:16  0:01:55  360k\n",
      " 12 80.2M   12 10.4M    0     0   612k      0  0:02:14  0:00:17  0:01:57  364k\n",
      " 13 80.2M   13 10.7M    0     0   599k      0  0:02:16  0:00:18  0:01:58  387k\n",
      " 13 80.2M   13 11.2M    0     0   589k      0  0:02:19  0:00:19  0:02:00  393k\n",
      " 14 80.2M   14 11.6M    0     0   584k      0  0:02:20  0:00:20  0:02:00  409k\n",
      " 15 80.2M   15 12.1M    0     0   581k      0  0:02:21  0:00:21  0:02:00  439k\n",
      " 16 80.2M   16 12.8M    0     0   586k      0  0:02:19  0:00:22  0:01:57  498k\n",
      " 16 80.2M   16 13.6M    0     0   589k      0  0:02:19  0:00:23  0:01:56  554k\n",
      " 17 80.2M   17 14.1M    0     0   589k      0  0:02:19  0:00:24  0:01:55  590k\n",
      " 18 80.2M   18 14.5M    0     0   583k      0  0:02:20  0:00:25  0:01:55  582k\n",
      " 18 80.2M   18 14.7M    0     0   571k      0  0:02:23  0:00:26  0:01:57  527k\n",
      " 18 80.2M   18 15.0M    0     0   561k      0  0:02:26  0:00:27  0:01:59  448k\n",
      " 19 80.2M   19 15.3M    0     0   552k      0  0:02:28  0:00:28  0:02:00  370k\n",
      " 19 80.2M   19 15.5M    0     0   541k      0  0:02:31  0:00:29  0:02:02  296k\n",
      " 19 80.2M   19 15.7M    0     0   529k      0  0:02:35  0:00:30  0:02:05  252k\n",
      " 19 80.2M   19 15.9M    0     0   518k      0  0:02:38  0:00:31  0:02:07  238k\n",
      " 20 80.2M   20 16.0M    0     0   507k      0  0:02:41  0:00:32  0:02:09  213k\n",
      " 20 80.2M   20 16.2M    0     0   498k      0  0:02:44  0:00:33  0:02:11  189k\n",
      " 20 80.2M   20 16.4M    0     0   490k      0  0:02:47  0:00:34  0:02:13  188k\n",
      " 20 80.2M   20 16.7M    0     0   484k      0  0:02:49  0:00:35  0:02:14  210k\n",
      " 21 80.2M   21 17.1M    0     0   482k      0  0:02:50  0:00:36  0:02:14  258k\n",
      " 22 80.2M   22 17.7M    0     0   486k      0  0:02:48  0:00:37  0:02:11  349k\n",
      " 23 80.2M   23 18.7M    0     0   498k      0  0:02:44  0:00:38  0:02:06  496k\n",
      " 24 80.2M   24 19.8M    0     0   516k      0  0:02:39  0:00:39  0:02:00  698k\n",
      " 26 80.2M   26 21.3M    0     0   539k      0  0:02:32  0:00:40  0:01:52  922k\n",
      " 28 80.2M   28 22.9M    0     0   568k      0  0:02:24  0:00:41  0:01:43 1193k\n",
      " 30 80.2M   30 24.6M    0     0   595k      0  0:02:18  0:00:42  0:01:36 1404k\n",
      " 32 80.2M   32 25.8M    0     0   609k      0  0:02:14  0:00:43  0:01:31 1464k\n",
      " 33 80.2M   33 26.5M    0     0   611k      0  0:02:14  0:00:44  0:01:30 1362k\n",
      " 33 80.2M   33 27.2M    0     0   614k      0  0:02:13  0:00:45  0:01:28 1236k\n",
      " 34 80.2M   34 27.7M    0     0   612k      0  0:02:14  0:00:46  0:01:28  978k\n",
      " 35 80.2M   35 28.2M    0     0   608k      0  0:02:14  0:00:47  0:01:27  724k\n",
      " 35 80.2M   35 28.6M    0     0   606k      0  0:02:15  0:00:48  0:01:27  576k\n",
      " 36 80.2M   36 29.1M    0     0   603k      0  0:02:16  0:00:49  0:01:27  531k\n",
      " 36 80.2M   36 29.5M    0     0   600k      0  0:02:16  0:00:50  0:01:26  473k\n",
      " 37 80.2M   37 30.0M    0     0   597k      0  0:02:17  0:00:51  0:01:26  463k\n",
      " 38 80.2M   38 30.5M    0     0   596k      0  0:02:17  0:00:52  0:01:25  484k\n",
      " 38 80.2M   38 31.1M    0     0   597k      0  0:02:17  0:00:53  0:01:24  508k\n",
      " 39 80.2M   39 31.8M    0     0   599k      0  0:02:17  0:00:54  0:01:23  560k\n",
      " 40 80.2M   40 32.8M    0     0   606k      0  0:02:15  0:00:55  0:01:20  668k\n",
      " 41 80.2M   41 33.4M    0     0   605k      0  0:02:15  0:00:56  0:01:19  678k\n",
      " 42 80.2M   42 34.1M    0     0   609k      0  0:02:14  0:00:57  0:01:17  738k\n",
      " 43 80.2M   43 34.6M    0     0   607k      0  0:02:15  0:00:58  0:01:17  719k\n",
      " 43 80.2M   43 35.1M    0     0   606k      0  0:02:15  0:00:59  0:01:16  676k\n",
      " 44 80.2M   44 35.7M    0     0   605k      0  0:02:15  0:01:00  0:01:15  595k\n",
      " 45 80.2M   45 36.1M    0     0   602k      0  0:02:16  0:01:01  0:01:15  574k\n",
      " 45 80.2M   45 36.6M    0     0   600k      0  0:02:16  0:01:02  0:01:14  499k\n",
      " 46 80.2M   46 37.1M    0     0   599k      0  0:02:17  0:01:03  0:01:14  506k\n",
      " 46 80.2M   46 37.6M    0     0   598k      0  0:02:17  0:01:04  0:01:13  506k\n",
      " 47 80.2M   47 38.2M    0     0   597k      0  0:02:17  0:01:05  0:01:12  505k\n",
      " 48 80.2M   48 38.6M    0     0   595k      0  0:02:17  0:01:06  0:01:11  512k\n",
      " 48 80.2M   48 39.1M    0     0   593k      0  0:02:18  0:01:07  0:01:11  512k\n",
      " 49 80.2M   49 39.6M    0     0   592k      0  0:02:18  0:01:08  0:01:10  508k\n",
      " 50 80.2M   50 40.1M    0     0   592k      0  0:02:18  0:01:09  0:01:09  512k\n",
      " 50 80.2M   50 40.7M    0     0   591k      0  0:02:18  0:01:10  0:01:08  512k\n",
      " 51 80.2M   51 41.2M    0     0   591k      0  0:02:18  0:01:11  0:01:07  531k\n",
      " 52 80.2M   52 41.7M    0     0   590k      0  0:02:19  0:01:12  0:01:07  544k\n",
      " 52 80.2M   52 42.3M    0     0   590k      0  0:02:19  0:01:13  0:01:06  556k\n",
      " 53 80.2M   53 43.0M    0     0   592k      0  0:02:18  0:01:14  0:01:04  595k\n",
      " 54 80.2M   54 43.9M    0     0   596k      0  0:02:17  0:01:15  0:01:02  662k\n",
      " 56 80.2M   56 45.0M    0     0   603k      0  0:02:16  0:01:16  0:01:00  777k\n",
      " 57 80.2M   57 46.0M    0     0   606k      0  0:02:15  0:01:17  0:00:58  838k\n",
      " 58 80.2M   58 46.6M    0     0   607k      0  0:02:15  0:01:18  0:00:57  849k\n",
      " 58 80.2M   58 47.0M    0     0   606k      0  0:02:15  0:01:19  0:00:56  821k\n",
      " 59 80.2M   59 47.4M    0     0   604k      0  0:02:15  0:01:20  0:00:55  720k\n",
      " 59 80.2M   59 47.7M    0     0   600k      0  0:02:16  0:01:21  0:00:55  556k\n",
      " 60 80.2M   60 48.1M    0     0   598k      0  0:02:17  0:01:22  0:00:55  456k\n",
      " 60 80.2M   60 48.5M    0     0   595k      0  0:02:17  0:01:23  0:00:54  401k\n",
      " 60 80.2M   60 48.9M    0     0   593k      0  0:02:18  0:01:24  0:00:54  378k\n",
      " 61 80.2M   61 49.3M    0     0   591k      0  0:02:18  0:01:25  0:00:53  383k\n",
      " 61 80.2M   61 49.6M    0     0   587k      0  0:02:19  0:01:26  0:00:53  379k\n",
      " 62 80.2M   62 49.9M    0     0   584k      0  0:02:20  0:01:27  0:00:53  357k\n",
      " 62 80.2M   62 50.1M    0     0   580k      0  0:02:21  0:01:28  0:00:53  328k\n",
      " 62 80.2M   62 50.4M    0     0   577k      0  0:02:22  0:01:29  0:00:53  305k\n",
      " 63 80.2M   63 50.6M    0     0   573k      0  0:02:23  0:01:30  0:00:53  276k\n",
      " 63 80.2M   63 50.9M    0     0   570k      0  0:02:23  0:01:31  0:00:52  272k\n",
      " 63 80.2M   63 51.3M    0     0   568k      0  0:02:24  0:01:32  0:00:52  287k\n",
      " 64 80.2M   64 51.5M    0     0   562k      0  0:02:26  0:01:33  0:00:53  258k\n",
      " 64 80.2M   64 51.7M    0     0   561k      0  0:02:26  0:01:34  0:00:52  278k\n",
      " 64 80.2M   64 52.0M    0     0   558k      0  0:02:27  0:01:35  0:00:52  272k\n",
      " 65 80.2M   65 52.2M    0     0   554k      0  0:02:28  0:01:36  0:00:52  260k\n",
      " 65 80.2M   65 52.4M    0     0   551k      0  0:02:29  0:01:37  0:00:52  234k\n",
      " 65 80.2M   65 52.6M    0     0   548k      0  0:02:29  0:01:38  0:00:51  261k\n",
      " 66 80.2M   66 52.9M    0     0   545k      0  0:02:30  0:01:39  0:00:51  242k\n",
      " 66 80.2M   66 53.2M    0     0   543k      0  0:02:31  0:01:40  0:00:51  256k\n",
      " 66 80.2M   66 53.6M    0     0   540k      0  0:02:31  0:01:41  0:00:50  278k\n",
      " 67 80.2M   67 53.8M    0     0   538k      0  0:02:32  0:01:42  0:00:50  291k\n",
      " 67 80.2M   67 54.1M    0     0   536k      0  0:02:33  0:01:43  0:00:50  300k\n",
      " 67 80.2M   67 54.4M    0     0   533k      0  0:02:34  0:01:44  0:00:50  293k\n",
      " 68 80.2M   68 54.6M    0     0   530k      0  0:02:34  0:01:45  0:00:49  288k\n",
      " 68 80.2M   68 55.0M    0     0   528k      0  0:02:35  0:01:46  0:00:49  284k\n",
      " 68 80.2M   68 55.3M    0     0   527k      0  0:02:35  0:01:47  0:00:48  297k\n",
      " 69 80.2M   69 55.7M    0     0   526k      0  0:02:36  0:01:48  0:00:48  317k\n",
      " 70 80.2M   70 56.2M    0     0   526k      0  0:02:36  0:01:49  0:00:47  384k\n",
      " 71 80.2M   71 57.0M    0     0   528k      0  0:02:35  0:01:50  0:00:45  488k\n",
      " 72 80.2M   72 58.1M    0     0   534k      0  0:02:33  0:01:51  0:00:42  653k\n",
      " 73 80.2M   73 59.2M    0     0   539k      0  0:02:32  0:01:52  0:00:40  810k\n",
      " 75 80.2M   75 60.4M    0     0   545k      0  0:02:30  0:01:53  0:00:37  966k\n",
      " 76 80.2M   76 61.6M    0     0   552k      0  0:02:28  0:01:54  0:00:34 1110k\n",
      " 78 80.2M   78 63.0M    0     0   558k      0  0:02:26  0:01:55  0:00:31 1221k\n",
      " 79 80.2M   79 64.0M    0     0   563k      0  0:02:25  0:01:56  0:00:29 1212k\n",
      " 81 80.2M   81 65.2M    0     0   568k      0  0:02:24  0:01:57  0:00:27 1217k\n",
      " 82 80.2M   82 66.2M    0     0   572k      0  0:02:23  0:01:58  0:00:25 1187k\n",
      " 83 80.2M   83 67.2M    0     0   576k      0  0:02:22  0:01:59  0:00:23 1129k\n",
      " 85 80.2M   85 68.2M    0     0   580k      0  0:02:21  0:02:00  0:00:21 1072k\n",
      " 85 80.2M   85 68.9M    0     0   581k      0  0:02:21  0:02:01  0:00:20 1001k\n",
      " 86 80.2M   86 69.2M    0     0   579k      0  0:02:21  0:02:02  0:00:19  839k\n",
      " 86 80.2M   86 69.5M    0     0   576k      0  0:02:22  0:02:03  0:00:19  671k\n",
      " 86 80.2M   86 69.6M    0     0   573k      0  0:02:23  0:02:04  0:00:19  505k\n",
      " 87 80.2M   87 69.8M    0     0   570k      0  0:02:24  0:02:05  0:00:19  332k\n",
      " 87 80.2M   87 70.1M    0     0   567k      0  0:02:24  0:02:06  0:00:18  233k\n",
      " 87 80.2M   87 70.3M    0     0   565k      0  0:02:25  0:02:07  0:00:18  213k\n",
      " 87 80.2M   87 70.5M    0     0   562k      0  0:02:26  0:02:08  0:00:18  218k\n",
      " 88 80.2M   88 70.8M    0     0   560k      0  0:02:26  0:02:09  0:00:17  239k\n",
      " 88 80.2M   88 71.1M    0     0   558k      0  0:02:27  0:02:10  0:00:17  258k\n",
      " 89 80.2M   89 71.5M    0     0   557k      0  0:02:27  0:02:11  0:00:16  290k\n",
      " 89 80.2M   89 72.0M    0     0   557k      0  0:02:27  0:02:12  0:00:15  349k\n",
      " 90 80.2M   90 72.7M    0     0   558k      0  0:02:27  0:02:13  0:00:14  455k\n",
      " 92 80.2M   92 73.8M    0     0   562k      0  0:02:26  0:02:14  0:00:12  616k\n",
      " 93 80.2M   93 75.2M    0     0   569k      0  0:02:24  0:02:15  0:00:09  850k\n",
      " 95 80.2M   95 76.5M    0     0   574k      0  0:02:22  0:02:16  0:00:06 1039k\n",
      " 97 80.2M   97 78.0M    0     0   581k      0  0:02:21  0:02:17  0:00:04 1230k\n",
      " 98 80.2M   98 79.3M    0     0   586k      0  0:02:19  0:02:18  0:00:01 1341k\n",
      "100 80.2M  100 80.2M    0     0   590k      0  0:02:19  0:02:19 --:--:-- 1359k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re left with a directory named aclImdb, with the following structure:\n",
    "```python\n",
    "aclImdb/\n",
    "...train/\n",
    "......pos/\n",
    "......neg/\n",
    "...test/\n",
    "......pos/\n",
    "......neg/\n",
    "```\n",
    "\n",
    "For instance, the train/pos/ directory contains a set of 12,500 text files, each of which contains the text body of a positive-sentiment movie review to be used as training data. <br>\n",
    "The negative-sentiment reviews live in the “neg” directories. In total, there are 25,000 text files for training and another 25,000 for testing. <br>\n",
    "There’s also a train/unsup subdirectory in there, which we don’t need. Let’s delete it:\n",
    "```python\n",
    "!rm -r aclImdb/train/unsup\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s prepare a validation set by setting apart 20% of the training text files in a\n",
    "new directory, aclImdb/val:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files) # Shuffle the list of training files using a seed, to ensure we get the same validation set every time we run the code.\n",
    "    # Take 20% of the training files to use for validation.\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    # Move the files to aclImdb/val/neg and aclImdb/val/pos.\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how, in chapter 8, we used the **image_dataset_from_directory** utility to create a batched Dataset of images and their labels for a directory structure? You can do the exact same thing for text files using the **text_dataset_from_directory** utility. <br>\n",
    "Let’s create three **Dataset** objects for training, validation, and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets yield inputs that are TensorFlow **tf.string** tensors and targets that are int32 tensors encoding the value “0” or “1.”\n",
    "##### Displaying the shapes and dtypes of the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'No, this wasn\\'t one of the ten worst films of the 1980\\'s, but it certainly skirts the bottom 100 somewhere. This movie looks like it was put on the shelf for two or three years and then released in 1981. How else would you explain special effects pre-dating \"An American Werewolf in London,\" disco still being considered cool, and Ronald Reagan not being the 40th President of the United States? While we\\'re at it, let\\'s not overlook those 1970\\'s hairstyles in the 1950\\'s and \\'60\\'s. I\\'ve seen more of that here than in \"Happy Days\" & \"Laverne & Shirley\" combined.<br /><br />The one woman who elevates this movie to the \"so bad, it\\'s good\" category was the late, great Elizabeth Hartman, but just barely. Biff plays as Miss Montgomery, the mousey high school teacher who becomes a sexpot, a stereotype that\\'s been done to death and is still being churned out by Hollywood today, but even as a \"hot chick\" she retains her mousey qualities. Her call for help is evidence of this. She also looks much better as Miss Wimp. \"Seven bucks at the beauty parlor, shot to hell.\" She wasn\\'t kidding.<br /><br />This isn\\'t to say that there aren\\'t any good parts elsewhere, they\\'re just few and far between, and I\\'m not just saying that because I like Hartman. Incidentally, \"Teen Wolf\" was better than this. \"Teen Wolf Too\" was better than this, and that wasn\\'t even so good.<br /><br />', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set. Now let’s try learning something from this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Processing words as a set: The bag-of-words approach\n",
    "The simplest way to encode a piece of text for processing by a machine learning model is to discard order and treat it as a set (a “bag”) of tokens. You could either look at individual words (unigrams), or try to recover some local order information by looking at groups of consecutive token (N-grams).\n",
    "\n",
    "##### SINGLE WORDS (UNIGRAMS) WITH BINARY ENCODING\n",
    "If you use a bag of single words, the sentence “the cat sat on the mat” becomes\n",
    "```python\n",
    "{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}\n",
    "```\n",
    "The main advantage of this encoding is that you can represent an entire text as a single vector, where each entry is a presence indicator for a given word. For instance, using binary encoding (multi-hot), you’d encode a text as a vector with as many dimensions as there are words in your vocabulary—with 0s almost everywhere and some 1s for dimensions that encode words present in the text. This is what we did when we worked with text data in chapters 4 and 5. Let’s try this on our task.\n",
    "\n",
    "First, let’s process our raw text datasets with a **TextVectorization** layer so that they yield multi-hot encoded binary word vectors. Our layer will only look at single words (that is to say, unigrams).\n",
    "\n",
    "##### Preprocessing our datasets with a TextVectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the vocabulary to the 20,000 most frequent words. \n",
    "# Otherwise we’d be indexing every word in the training data potentially tens of thousands of terms that only occur once or twice and thus aren’t informative. \n",
    "# In general, 20,000 is the right vocabulary size for text classification.\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\", # Encode the output tokens as multi-hot binary vectors.\n",
    ")\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x) # Prepare a dataset that only yields raw text inputs (no labels).\n",
    "text_vectorization.adapt(text_only_train_ds) # Use that dataset to index the dataset vocabulary via the adapt() method.\n",
    "\n",
    "# Prepare processed versions of our training, validation, and test dataset. \n",
    "# Make sure to specify num_parallel_calls to leverage multiple CPU cores.\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to inspect the output of one of these datasets.\n",
    "##### Inspecting the output of our binary unigram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s write a reusable model-building function that we’ll use in all of our experiments in this section.\n",
    "\n",
    "##### Our model-building utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s train and test our model.\n",
    "##### Training and testing the binary unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 14s 21ms/step - loss: 0.4160 - accuracy: 0.8186 - val_loss: 0.2981 - val_accuracy: 0.8822\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2786 - accuracy: 0.8979 - val_loss: 0.2984 - val_accuracy: 0.8866\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2455 - accuracy: 0.9144 - val_loss: 0.3123 - val_accuracy: 0.8892\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2281 - accuracy: 0.9211 - val_loss: 0.3272 - val_accuracy: 0.8918\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2237 - accuracy: 0.9275 - val_loss: 0.3479 - val_accuracy: 0.8894\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2122 - accuracy: 0.9317 - val_loss: 0.3625 - val_accuracy: 0.8906\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2132 - accuracy: 0.9312 - val_loss: 0.3750 - val_accuracy: 0.8814\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1995 - accuracy: 0.9351 - val_loss: 0.3883 - val_accuracy: 0.8832\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2050 - accuracy: 0.9359 - val_loss: 0.3901 - val_accuracy: 0.8806\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1996 - accuracy: 0.9374 - val_loss: 0.3976 - val_accuracy: 0.8834\n",
      "782/782 [==============================] - 30s 37ms/step - loss: 0.2928 - accuracy: 0.8853\n",
      "Test acc: 0.885\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "# We call cache() on the datasets to cache them in memory: this way, we will only do the preprocessing once, during the first epoch, and we’ll reuse the preprocessed texts for the following epochs. \n",
    "# This can only be done if the data is small enough to fit in memory.\n",
    "\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us to a test accuracy of 88.5%: not bad! Note that in this case, since the dataset is a balanced two-class classification dataset (there are as many positive samples as negative samples), the “naive baseline” we could reach without training an actual model would only be 50%. Meanwhile, the best score that can be achieved on this dataset without leveraging external data is around 95% test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BIGRAMS WITH BINARY ENCODING\n",
    "Of course, discarding word order is very reductive, because even atomic concepts can be expressed via multiple words: the term “United States” conveys a concept that is quite distinct from the meaning of the words “states” and “united” taken separately. For this reason, you will usually end up re-injecting local order information into your bag-of-words representation by looking at N-grams rather than single words (most commonly, bigrams). <br>\n",
    "With bigrams, our sentence becomes\n",
    "```python\n",
    "{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n",
    "\"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n",
    "```\n",
    "\n",
    "The **TextVectorization** layer can be configured to return arbitrary N-grams: bigrams, trigrams, etc. Just pass an **ngrams=N** argument as in the following listing.\n",
    "\n",
    "##### Configuring the TextVectorization layer to return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s test how our model performs when trained on such binary-encoded bags of bigrams.\n",
    "\n",
    "##### Training and testing the binary bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 8s 12ms/step - loss: 0.3868 - accuracy: 0.8367 - val_loss: 0.2808 - val_accuracy: 0.8908\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2490 - accuracy: 0.9125 - val_loss: 0.2869 - val_accuracy: 0.8924\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2186 - accuracy: 0.9286 - val_loss: 0.3080 - val_accuracy: 0.8946\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2002 - accuracy: 0.9362 - val_loss: 0.3230 - val_accuracy: 0.8966\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1821 - accuracy: 0.9438 - val_loss: 0.3424 - val_accuracy: 0.8966\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1812 - accuracy: 0.9468 - val_loss: 0.3546 - val_accuracy: 0.8984\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.1820 - accuracy: 0.9491 - val_loss: 0.3619 - val_accuracy: 0.8958\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1790 - accuracy: 0.9513 - val_loss: 0.3722 - val_accuracy: 0.8956\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1831 - accuracy: 0.9503 - val_loss: 0.3858 - val_accuracy: 0.8892\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1837 - accuracy: 0.9499 - val_loss: 0.3869 - val_accuracy: 0.8902\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.2719 - accuracy: 0.8962\n",
      "Test acc: 0.896\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re now getting 89.6% test accuracy, a marked improvement! Turns out local order\n",
    "is pretty important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BIGRAMS WITH TF-IDF ENCODING\n",
    "You can also add a bit more information to this representation by counting how many times each word or N-gram occurs, that is to say, by taking the histogram of the words over the text:\n",
    "```python\n",
    "{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\n",
    "\"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}\n",
    "```\n",
    "If you’re doing text classification, knowing how many times a word occurs in a sample is critical: any sufficiently long movie review may contain the word “terrible” regardless of sentiment, but a review that contains many instances of the word “terrible” is likely a negative one. <br>\n",
    "Here’s how you’d count bigram occurrences with the **TextVectorization** layer.\n",
    "##### Configuring the TextVectorization layer to return token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, of course, some words are bound to occur more often than others no matter what the text is about. The words “the,” “a,” “is,” and “are” will always dominate your word count histograms, drowning out other words—despite being pretty much useless features in a classification context. How could we address this? <br>\n",
    "You already guessed it: via **normalization**. We could just normalize word counts by subtracting the mean and dividing by the variance (computed across the entire training dataset). That would make sense. Except most vectorized sentences consist almost entirely of zeros (our previous example features 12 non-zero entries and 19,988 zero entries), a property called **“sparsity”**. That’s a great property to have, as it dramatically reduces compute load and reduces the risk of overfitting. If we subtracted the mean from each feature, we’d wreck sparsity. Thus, whatever normalization scheme we use should be divide-only. What, then, should we use as the denominator? The best practice is to go with something called **TF-IDF normalization**—TF-IDF stands for “term frequency, inverse document frequency.” <br>\n",
    "**TF-IDF** is so common that it’s built into the **TextVectorization** layer. All you need to do to start using it is to switch the **output_mode** argument to **\"tf_idf\"**.\n",
    "\n",
    "##### Configuring TextVectorization to return TF-IDF-weighted outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s train a new model with this scheme.\n",
    "\n",
    "##### Training and testing the TF-IDF bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 9s 13ms/step - loss: 0.5017 - accuracy: 0.7734 - val_loss: 0.3237 - val_accuracy: 0.8780\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3489 - accuracy: 0.8557 - val_loss: 0.3171 - val_accuracy: 0.8888\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3027 - accuracy: 0.8801 - val_loss: 0.3088 - val_accuracy: 0.8920\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2779 - accuracy: 0.8917 - val_loss: 0.3608 - val_accuracy: 0.8814\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2580 - accuracy: 0.8952 - val_loss: 0.3411 - val_accuracy: 0.8762\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2449 - accuracy: 0.8996 - val_loss: 0.3631 - val_accuracy: 0.8740\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2411 - accuracy: 0.9020 - val_loss: 0.3560 - val_accuracy: 0.8700\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2418 - accuracy: 0.8991 - val_loss: 0.3684 - val_accuracy: 0.8600\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2381 - accuracy: 0.9021 - val_loss: 0.3565 - val_accuracy: 0.8632\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2271 - accuracy: 0.9027 - val_loss: 0.3770 - val_accuracy: 0.8666\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.3043 - accuracy: 0.8927\n",
      "Test acc: 0.893\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds) # The adapt() call will learn the TF-IDF weights in addition to the vocabulary.\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us an 89.3% test accuracy on the IMDB classification task: it doesn’t seem to be particularly helpful in this case. However, for many text-classification datasets, it would be typical to see a one-percentage-point increase when using TF-IDF compared to plain binary encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exporting a model that processes raw strings\n",
    "In the preceding examples, we did our text standardization, splitting, and indexing as part of the tf.data pipeline. But if we want to export a standalone model independent of this pipeline, we should make sure that it incorporates its own text preprocessing (otherwise, you’d have to reimplement in the production environment, which can be challenging or can lead to subtle discrepancies between the training data and the production data). Thankfully, this is easy. <br>\n",
    "Just create a new model that reuses your TextVectorization layer and adds to it the model you just trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\") # One input sample would be one string.\n",
    "processed_inputs = text_vectorization(inputs) # Add text preprocessing.\n",
    "outputs = model(processed_inputs) # Apply the previously trained model.\n",
    "inference_model = keras.Model(inputs, outputs) # Instantiate the end-to-end model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model can process batches of raw strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.43 percent positive\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ee13a16f7ff347d089854b949fd5a4fdba136de942caaffeaf6bff99e7e7f9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
