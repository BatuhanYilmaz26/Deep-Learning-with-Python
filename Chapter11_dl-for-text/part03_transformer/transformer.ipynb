{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### The Transformer architecture\n","Starting in 2017, a new model architecture started overtaking recurrent neural networks across most natural language processing tasks: the **Transformer**. <br>\n","Transformers were introduced in the seminal paper “Attention is all you need” by Vaswani et al. The gist of the paper is right there in the title: as it turned out, a simple mechanism called “neural attention” could be used to build powerful sequence models that didn’t feature any recurrent layers or convolution layers. <br>\n","This finding unleashed nothing short of a revolution in natural language processing— and beyond. Neural attention has fast become one of the most influential ideas in deep learning. In this section, you’ll get an in-depth explanation of how it works and why it has proven so effective for sequence data. We’ll then leverage **self-attention** to create a **Transformer encoder**, one of the basic components of the **Transformer** architecture, and we’ll apply it to the IMDB movie review classification task."]},{"cell_type":"markdown","metadata":{},"source":["##### Understanding self-attention\n","As you’re going through this book, you may be skimming some parts and attentively reading others, depending on what your goals or interests are. What if your models did the same? It’s a simple yet powerful idea: not all input information seen by a model is equally important to the task at hand, so models should “pay more attention” to some features and “pay less attention” to other features. <br>\n","Does that sound familiar? You’ve already encountered a similar concept twice in this book:\n","- **Max pooling** in convnets looks at a pool of features in a spatial region and selects just one feature to keep. That’s an “all or nothing” form of attention: keep the most important feature and discard the rest.\n","- **TF-IDF** normalization assigns importance scores to tokens based on how much information different tokens are likely to carry. Important tokens get boosted while irrelevant tokens get faded out. That’s a continuous form of attention.\n","\n","There are many different forms of attention you could imagine, but they all start by computing importance scores for a set of features, with higher scores for more relevant features and lower scores for less relevant ones (see figure 11.5). How these scores should be computed, and what you should do with them, will vary from approach to approach.\n","\n","![](./images/11.5.png)\n","\n","Crucially, this kind of attention mechanism can be used for more than just highlighting or erasing certain features. It can be used to make features context-aware. You’ve just learned about word embeddings—vector spaces that capture the “shape” of the semantic relationships between different words. In an embedding space, a single word has a fixed position—a fixed set of relationships with every other word in the space. But that’s not quite how language works: the meaning of a word is usually context-specific. When you mark the date, you’re not talking about the same “date” as when you go on a date, nor is it the kind of date you’d buy at the market. When you say, “I’ll see you soon,” the meaning of the word “see” is subtly different from the “see” in “I’ll see this project to its end” or “I see what you mean.” And, of course, the meaning of pronouns like “he,” “it,” “in,” etc., is entirely sentence-specific and can even change multiple times within a single sentence. <br>\n","Clearly, a smart embedding space would provide a different vector representation for a word depending on the other words surrounding it. That’s where self-attention comes in. The purpose of self-attention is to modulate the representation of a token by using the representations of related tokens in the sequence. This produces context aware token representations. Consider an example sentence: “The train left the station on time.” Now, consider one word in the sentence: station. What kind of station are we talking about? Could it be a radio station? Maybe the International Space Station? <br>\n","Let’s figure it out algorithmically via **self-attention** (see figure 11.6).\n","\n","![](./images/11.6.png)\n","\n","- Step 1 is to compute relevancy scores between the vector for “station” and every other word in the sentence. \n","  - These are our “**attention scores**.” We’re simply going to use the dot product between two word vectors as a measure of the strength of their relationship. \n","  - It’s a very computationally efficient distance function, and it was already the standard way to relate two word embeddings to each other long before Transformers. \n","  - In practice, these scores will also go through a scaling function and a softmax, but for now, that’s just an implementation detail.\n","- Step 2 is to compute the sum of all word vectors in the sentence, weighted by our relevancy scores. \n","  - Words closely related to “station” will contribute more to the sum (including the word “station” itself), while irrelevant words will contribute almost nothing. \n","  - The resulting vector is our new representation for “station”: a representation that incorporates the surrounding context. \n","  - In particular, it includes part of the “train” vector, clarifying that it is, in fact, a “train station.”\n","- You’d repeat this process for every word in the sentence, producing a new sequence of vectors encoding the sentence. \n","\n","Let’s see it in NumPy-like pseudocode:\n","\n","```python\n","def self_attention(input_sequence):\n","  output = np.zeros(shape=input_sequence.shape)\n","  # Iterate over each token in the input sequence.\n","  for i, pivot_vector in enumerate(input_sequence):\n","    scores = np.zeros(shape=(len(input_sequence),))\n","    for j, vector in enumerate(input_sequence):\n","      # Compute the dot product (attention score) between the token and every other token.\n","      scores[j] = np.dot(pivot_vector, vector.T)\n","    # Scale by a normalization factor, and apply a softmax.\n","    scores /= np.sqrt(input_sequence.shape[1])\n","    scores = softmax(scores)\n","    new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n","    for j, vector in enumerate(input_sequence):\n","      # Take the sum of all tokens weighted by the attention scores.\n","      new_pivot_representation += vector * scores[j]\n","    output[i] = new_pivot_representation # That sum is our output.\n","  return output\n","```\n","\n","Of course, in practice you’d use a vectorized implementation. Keras has a built-in layer to handle it: the MultiHeadAttention layer. Here’s how you would use it:\n","\n","```python\n","num_heads = 4\n","embed_dim = 256\n","mha_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","outputs = mha_layer(inputs, inputs, inputs)\n","```\n","\n","Reading this, you’re probably wondering\n","- Why are we passing the inputs to the layer three times? That seems redundant.\n","- What are these “multiple heads” we’re referring to? That sounds intimidating—do they also grow back if you cut them?\n","\n","Both of these questions have simple answers. Let’s take a look."]},{"cell_type":"markdown","metadata":{},"source":["##### GENERALIZED SELF-ATTENTION: THE QUERY-KEY-VALUE MODEL\n","So far, we have only considered one input sequence. However, the **Transformer** architecture was originally developed for **machine translation**, where you have to deal with two input sequences: the source sequence you’re currently translating (such as “How’s the weather today?”), and the target sequence you’re converting it to (such as “¿Qué tiempo hace hoy?”). A **Transformer** is a **sequence-to-sequence model**: it was designed to convert one sequence into another. You’ll learn about sequence-to-sequence models in depth later in this chapter.\n","\n","Now let’s take a step back. The **self-attention** mechanism as we’ve introduced it performs the following, schematically:\n","\n","![](./images/self_attention.png)\n","\n","This means “for each token in inputs (A), compute how much the token is related to every token in inputs (B), and use these scores to weight a sum of tokens from inputs (C).” Crucially, there’s nothing that requires A, B, and C to refer to the same input sequence. In the general case, you could be doing this with three different sequences. We’ll call them “query,” “keys,” and “values.” The operation becomes “for each element in the query, compute how much the element is related to every key, and use these scores to weight a sum of values”:\n","\n","```python\n","outputs = sum(values * pairwise_scores(query, keys))\n","```\n","\n","This terminology comes from search engines and recommender systems (see figure 11.7). Imagine that you’re typing up a query to retrieve a photo from your collection— “dogs on the beach.” Internally, each of your pictures in the database is described by a set of keywords—“cat,” “dog,” “party,” etc. We’ll call those “**keys**.” The search engine will start by comparing your query to the keys in the database. “Dog” yields a match of 1, and “cat” yields a match of 0. It will then rank those keys by strength of match—relevance—and it will return the pictures associated with the top N matches, in order of relevance. <br>\n","Conceptually, this is what Transformer-style attention is doing. You’ve got a reference sequence that describes something you’re looking for: the **query**. You’ve got a body of knowledge that you’re trying to extract information from: the **values**. Each value is assigned a key that describes the value in a format that can be readily compared to a query. You simply match the query to the keys. Then you return a weighted sum of values. <br>\n","\n","![](./images/11.7.png)\n","\n","In practice, the keys and the values are often the same sequence. In machine translation, for instance, the query would be the target sequence, and the source sequence would play the roles of both keys and values: for each element of the target (like “tiempo”), you want to go back to the source (“How’s the weather today?”) and identify the different bits that are related to it (“tiempo” and “weather” should have a strong match). And naturally, if you’re just doing sequence classification, then query, keys, and values are all the same: **you’re comparing a sequence to itself**, to enrich each token with context from the whole sequence. <br>\n","That explains why we needed to pass inputs three times to our **MultiHeadAttention** layer. But why “multi-head” attention?"]},{"cell_type":"markdown","metadata":{},"source":["##### Multi-head attention\n","\n","“Multi-head attention” is an extra tweak to the self-attention mechanism, introduced in “Attention is all you need.” The “multi-head” moniker refers to the fact that the output space of the self-attention layer gets factored into a set of independent subspaces, learned separately: the initial query, key, and value are sent through three independent sets of dense projections, resulting in three separate vectors. Each vector is processed via neural attention, and the three outputs are concatenated back together into a single output sequence. Each such subspace is called a “head.” The full picture is shown in figure 11.8. <br>\n","The presence of the learnable dense projections enables the layer to actually learn something, as opposed to being a purely stateless transformation that would require additional layers before or after it to be useful. In addition, having independent heads helps the layer learn different groups of features for each token, where features within one group are correlated with each other but are mostly independent from features in a different group.\n","\n","![](./images/11.8.png)\n","\n","This is similar in principle to what makes depthwise separable convolutions work: in a depthwise separable convolution, the output space of the convolution is factored into many subspaces (one per input channel) that get learned independently. The “Attention is all you need” paper was written at a time when the idea of factoring feature spaces into independent subspaces had been shown to provide great benefits for computer vision models—both in the case of depthwise separable convolutions, and in the case of a closely related approach, grouped convolutions. Multi-head attention is simply the application of the same idea to self-attention."]},{"cell_type":"markdown","metadata":{},"source":["##### The Transformer encoder\n","If adding **extra dense projections** is so useful, why don’t we also apply one or two to the **output of the attention mechanism**? Actually, that’s a great idea—let’s do that. And our model is starting to do a lot, so we might want to add **residual connections** to make sure we don’t destroy any valuable information along the way—you learned in chapter 9 that they’re a must for any sufficiently deep architecture. And there’s another thing you learned in chapter 9: **normalization layers** are supposed to help gradients flow better during backpropagation. Let’s add those too. <br>\n","That’s roughly the thought process that I imagine unfolded in the minds of the inventors of the **Transformer** architecture at the time. Factoring outputs into multiple independent spaces, adding residual connections, adding normalization layers—all of these are standard architecture patterns that one would be wise to leverage in any complex model. Together, these bells and whistles form the **Transformer encoder**—one of two critical parts that make up the **Transformer** architecture (see figure 11.9).\n","\n","![](./images/11.9.png)\n","\n","The original **Transformer** architecture consists of two parts: \n","- a **Transformer encoder** that processes the source sequence, and \n","- a **Transformer decoder** that uses the source sequence to generate a translated version. \n","\n","You’ll learn about about the **decoder** part in a minute. <br>\n","Crucially, the **encoder** part can be used for **text classification**—it’s a very generic module that ingests a sequence and learns to turn it into a more useful representation. <br>\n","Let’s implement a **Transformer encoder** and try it on the movie review sentiment classification task."]},{"cell_type":"markdown","metadata":{},"source":["##### Getting the data"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T12:44:38.877954Z","iopub.status.busy":"2022-05-10T12:44:38.877426Z","iopub.status.idle":"2022-05-10T12:44:48.123855Z","shell.execute_reply":"2022-05-10T12:44:48.122911Z","shell.execute_reply.started":"2022-05-10T12:44:38.877918Z"},"trusted":true},"outputs":[],"source":["!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz\n","!rm -r aclImdb/train/unsup"]},{"cell_type":"markdown","metadata":{},"source":["##### Preparing the data"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T12:44:48.126125Z","iopub.status.busy":"2022-05-10T12:44:48.125911Z","iopub.status.idle":"2022-05-10T12:44:59.829019Z","shell.execute_reply":"2022-05-10T12:44:59.828337Z","shell.execute_reply.started":"2022-05-10T12:44:48.126098Z"},"trusted":true},"outputs":[],"source":["import os, pathlib, shutil, random\n","from tensorflow import keras\n","batch_size = 32\n","base_dir = pathlib.Path(\"aclImdb\")\n","val_dir = base_dir / \"val\"\n","train_dir = base_dir / \"train\"\n","for category in (\"neg\", \"pos\"):\n","    os.makedirs(val_dir / category)\n","    files = os.listdir(train_dir / category)\n","    random.Random(1337).shuffle(files)\n","    num_val_samples = int(0.2 * len(files))\n","    val_files = files[-num_val_samples:]\n","    for fname in val_files:\n","        shutil.move(train_dir / category / fname,\n","                    val_dir / category / fname)\n","\n","train_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/train\", batch_size=batch_size\n",")\n","val_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/val\", batch_size=batch_size\n",")\n","test_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/test\", batch_size=batch_size\n",")\n","text_only_train_ds = train_ds.map(lambda x, y: x)"]},{"cell_type":"markdown","metadata":{},"source":["##### Vectorizing the data"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T12:44:59.830535Z","iopub.status.busy":"2022-05-10T12:44:59.830297Z","iopub.status.idle":"2022-05-10T12:45:03.486949Z","shell.execute_reply":"2022-05-10T12:45:03.486101Z","shell.execute_reply.started":"2022-05-10T12:44:59.830501Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import layers\n","\n","max_length = 600\n","max_tokens = 20000\n","text_vectorization = layers.TextVectorization(\n","    max_tokens=max_tokens,\n","    output_mode=\"int\",\n","    output_sequence_length=max_length,\n",")\n","text_vectorization.adapt(text_only_train_ds)\n","\n","int_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)"]},{"cell_type":"markdown","metadata":{},"source":["##### Transformer encoder implemented as a subclassed Layer"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T12:45:03.489136Z","iopub.status.busy":"2022-05-10T12:45:03.488905Z","iopub.status.idle":"2022-05-10T12:45:03.502603Z","shell.execute_reply":"2022-05-10T12:45:03.501814Z","shell.execute_reply.started":"2022-05-10T12:45:03.489106Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim # Size of the input token vectors\n","        self.dense_dim = dense_dim # Size of the inner dense layer\n","        self.num_heads = num_heads # Number of attention heads\n","        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","        [layers.Dense(dense_dim, activation=\"relu\"),\n","         layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        \n","    def call(self, inputs, mask=None): # Computation goes in call().\n","        # The mask that will be generated by the Embedding layer will be 2D, but the attention layer expects to be 3D or 4D, so we expand its rank.\n","        if mask is not None:\n","            mask = mask[:, tf.newaxis, :]\n","        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","    \n","    # Implement serialization so we can save the model.\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config"]},{"cell_type":"markdown","metadata":{},"source":["You’ll note that the normalization layers we’re using here aren’t **BatchNormalization** layers like those we’ve used before in image models. That’s because **BatchNormalization** doesn’t work well for sequence data. Instead, we’re using the **LayerNormalization** layer, which normalizes each sequence independently from other sequences in the batch.<br>\n","Like this, in NumPy-like pseudocode:\n","```python\n","def layer_normalization(batch_of_sequences): # Input shape (batch_size, sequence_length, embedding_dim)\n","    # To compute mean and variance, we only pool data over the last axis (axis -1).\n","    mean = np.mean(batch_of_sequences, keepdims=True, axis=-1)\n","    variance = np.var(batch_of_sequences, keepdims=True, axis=-1)\n","    return (batch_of_sequences - mean) / variance\n","```\n","Compare to BatchNormalization (during training):\n","```python\n","def batch_normalization(batch_of_images): # Input shape: (batch_size, height, width, channels)\n","    # Pool data over the batch axis (axis 0), which creates interactions between samples in a batch.\n","    mean = np.mean(batch_of_images, keepdims=True, axis=(0, 1, 2))\n","    variance = np.var(batch_of_images, keepdims=True, axis=(0, 1, 2))\n","    return (batch_of_images - mean) / variance\n","```\n","While **BatchNormalization** collects information from many samples to obtain accurate statistics for the feature means and variances, **LayerNormalization** pools data within each sequence separately, which is more appropriate for sequence data. <br>\n","Now that we’ve implemented our **TransformerEncoder**, we can use it to assemble a text-classification model similar to the GRU-based one you’ve seen previously.\n","\n","##### Using the Transformer encoder for text classification"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T12:45:03.504164Z","iopub.status.busy":"2022-05-10T12:45:03.503918Z","iopub.status.idle":"2022-05-10T12:45:03.764359Z","shell.execute_reply":"2022-05-10T12:45:03.763643Z","shell.execute_reply.started":"2022-05-10T12:45:03.504129Z"},"trusted":true},"outputs":[],"source":["vocab_size = 20000\n","embed_dim = 256\n","num_heads = 2\n","dense_dim = 32\n","\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","x = layers.Embedding(vocab_size, embed_dim)(inputs)\n","x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","x = layers.GlobalMaxPooling1D()(x) # Since TransformerEncoder returns full sequences, we need to reduce each sequence to a single vector for classification via a global pooling layer.\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["##### Training and evaluating the Transformer encoder based model"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T12:45:03.766312Z","iopub.status.busy":"2022-05-10T12:45:03.765873Z","iopub.status.idle":"2022-05-10T12:58:12.921971Z","shell.execute_reply":"2022-05-10T12:58:12.921065Z","shell.execute_reply.started":"2022-05-10T12:45:03.766277Z"},"trusted":true},"outputs":[],"source":["callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n","                                    save_best_only=True)\n","]\n","\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n","\n","model = keras.models.load_model(\n","    \"transformer_encoder.keras\",\n","    custom_objects={\"TransformerEncoder\": TransformerEncoder}) # Provide the custom TransformerEncoder class to the model-loading process.\n","print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"]},{"cell_type":"markdown","metadata":{},"source":["It gets to 88.3% test accuracy—slightly worse than the GRU model.\n","\n","At this point, you should start to feel a bit uneasy. Something’s off here. Can you tell what it is? <br>\n","This section is ostensibly about “sequence models.” I started off by highlighting the importance of word order. I said that **Transformer** was a sequence-processing architecture, originally developed for machine translation. And yet . . . the **Transformer encoder** you just saw in action wasn’t a sequence model at all. Did you notice? It’s composed of dense layers that process sequence tokens independently from each other, and an attention layer that looks at the tokens as a set. You could change the order of the tokens in a sequence, and you’d get the exact same pairwise attention scores and the exact same context-aware representations. If you were to completely scramble the words in every movie review, the model wouldn’t notice, and you’d still get the exact same accuracy. Self-attention is a set-processing mechanism, focused on the relationships between pairs of sequence elements (see figure 11.10)—it’s blind to whether these elements occur at the beginning, at the end, or in the middle of a sequence. So why do we say that Transformer is a sequence model? And how could it possibly be good for machine translation if it doesn’t look at word order?\n","\n","![](./images/11.10.png)\n","\n","I hinted at the solution earlier in the chapter: I mentioned in passing that **Transformer** was a hybrid approach that is technically order-agnostic, but that manually injects order information in the representations it processes. This is the missing ingredient! <br>\n","It’s called **positional encoding**. Let’s take a look."]},{"cell_type":"markdown","metadata":{},"source":["##### USING POSITIONAL ENCODING TO RE-INJECT ORDER INFORMATION\n","The idea behind positional encoding is very simple: to give the model access to word order information, we’re going to add the word’s position in the sentence to each word embedding. Our input word embeddings will have two components: the usual word vector, which represents the word independently of any specific context, and a position vector, which represents the position of the word in the current sentence. Hopefully, the model will then figure out how to best leverage this additional information. <br>\n","The simplest scheme you could come up with would be to concatenate the word’s position to its embedding vector. You’d add a “position” axis to the vector and fill it with 0 for the first word in the sequence, 1 for the second, and so on. <br>\n","That may not be ideal, however, because the positions can potentially be very large integers, which will disrupt the range of values in the embedding vector. As you know, neural networks don’t like very large input values, or discrete input distributions. <br>\n","The original “Attention is all you need” paper used an interesting trick to encode word positions: it added to the word embeddings a vector containing values in the range [-1, 1] that varied cyclically depending on the position (it used cosine functions to achieve this). This trick offers a way to uniquely characterize any integer in a large range via a vector of small values. It’s clever, but it’s not what we’re going to use in our case. We’ll do something simpler and more effective: we’ll learn position embedding vectors the same way we learn to embed word indices. We’ll then proceed to add our position embeddings to the corresponding word embeddings, to obtain a position-aware word embedding. This technique is called “positional embedding.” <br>\n","Let’s implement it.\n","\n","##### Implementing positional embedding as a subclassed layer"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T12:58:12.924221Z","iopub.status.busy":"2022-05-10T12:58:12.923958Z","iopub.status.idle":"2022-05-10T12:58:12.934413Z","shell.execute_reply":"2022-05-10T12:58:12.933666Z","shell.execute_reply.started":"2022-05-10T12:58:12.924186Z"},"trusted":true},"outputs":[],"source":["class PositionalEmbedding(layers.Layer):\n","    # downside of position embeddings is that the sequence length needs to be known in advance.\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        # Prepare an Embedding layer for the token indices.\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=input_dim, output_dim=output_dim)\n","        # And another one for the token positions\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        # Add both embedding vectors together.\n","        return embedded_tokens + embedded_positions\n","\n","    # Like the Embedding layer, this layer should be able to generate a mask so we can ignore padding 0s in the inputs. \n","    # The compute_mask method will called automatically by the framework, and the mask will get propagated to the next layer.\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    # Implement serialization so we can save the model.\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config"]},{"cell_type":"markdown","metadata":{},"source":["You would use this PositionEmbedding layer just like a regular Embedding layer. Let’s see it in action!\n","##### PUTTING IT ALL TOGETHER: A TEXT-CLASSIFICATION TRANSFORMER\n","All you have to do to start taking word order into account is swap the old Embedding layer with our position-aware version.\n","\n","##### Combining the Transformer encoder with positional embedding"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-10T12:58:12.936165Z","iopub.status.busy":"2022-05-10T12:58:12.935675Z","iopub.status.idle":"2022-05-10T13:11:55.055734Z","shell.execute_reply":"2022-05-10T13:11:55.054704Z","shell.execute_reply.started":"2022-05-10T12:58:12.936128Z"},"trusted":true},"outputs":[],"source":["vocab_size = 20000\n","sequence_length = 600\n","embed_dim = 256\n","num_heads = 2\n","dense_dim = 32\n","\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n","x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","x = layers.GlobalMaxPooling1D()(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n","model = keras.models.load_model(\n","    \"full_transformer_encoder.keras\",\n","    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n","                    \"PositionalEmbedding\": PositionalEmbedding})\n","print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"]},{"cell_type":"markdown","metadata":{},"source":["##### When to use sequence models over bag-of-words models\n","You may sometimes hear that bag-of-words methods are outdated, and that Transformer based sequence models are the way to go, no matter what task or dataset you’re looking at. This is definitely not the case: a small stack of Dense layers on top of a bag-of bigrams remains a perfectly valid and relevant approach in many cases. In fact, among the various techniques that we’ve tried on the IMDB dataset throughout this chapter, the best performing so far was the bag-of-bigrams! <br>\n","So, when should you prefer one approach over the other? <br>\n","In 2017, my team and I ran a systematic analysis of the performance of various text classification techniques across many different types of text datasets, and we discovered a remarkable and surprising rule of thumb for deciding whether to go with a bag-of-words model or a sequence model (http://mng.bz/AOzK)—a golden constant of sorts. <br>\n","It turns out that when approaching a new text-classification task, you should pay close attention to the ratio between the number of samples in your training data and the mean number of words per sample (see figure 11.11). If that ratio is small—less than 1,500—then the bag-of-bigrams model will perform better (and as a bonus, it will be much faster to train and to iterate on too). If that ratio is higher than 1,500, then you should go with a sequence model. In other words, sequence models work best when lots of training data is available and when each sample is relatively short.\n","\n","![](./images/11.11.png)\n","\n","So if you’re classifying 1,000-word long documents, and you have 100,000 of them (a ratio of 100), you should go with a bigram model. If you’re classifying tweets that are 40 words long on average, and you have 50,000 of them (a ratio of 1,250), you should also go with a bigram model. But if you increase your dataset size to 500,000 tweets (a ratio of 12,500), go with a Transformer encoder. What about the IMDB movie review classification task? We had 20,000 training samples and an average word count of 233, so our rule of thumb points toward a bigram model, which confirms what we found in practice. <br>\n","This intuitively makes sense: the input of a sequence model represents a richer and more complex space, and thus it takes more data to map out that space; meanwhile, a plain set of terms is a space so simple that you can train a logistic regression on top using just a few hundreds or thousands of samples. In addition, the shorter a sample is, the less the model can afford to discard any of the information it contains— in particular, word order becomes more important, and discarding it can create ambiguity. The sentences “this movie is the bomb” and “this movie was a bomb” have very close unigram representations, which could confuse a bag-of-words model, but a sequence model could tell which one is negative and which one is positive. With a longer sample, word statistics would become more reliable and the topic or sentiment would be more apparent from the word histogram alone. <br>\n","Now, keep in mind that this heuristic rule was developed specifically for text classification. It may not necessarily hold for other NLP tasks—when it comes to machine translation, for instance, Transformer shines especially for very long sequences, compared to RNNs. Our heuristic is also just a rule of thumb, rather than a scientific law, so expect it to work most of the time, but not necessarily every time."]}],"metadata":{"interpreter":{"hash":"f8ee13a16f7ff347d089854b949fd5a4fdba136de942caaffeaf6bff99e7e7f9"},"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
