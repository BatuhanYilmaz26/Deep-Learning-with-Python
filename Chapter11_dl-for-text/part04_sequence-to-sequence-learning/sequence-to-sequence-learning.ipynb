{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### Beyond text classification: Sequence-to-sequence learning\n","You now possess all of the tools you will need to tackle most natural language processing tasks. However, you’ve only seen these tools in action on a single problem: text classification. This is an extremely popular use case, but there’s a lot more to NLP than classification. In this section, you’ll deepen your expertise by learning about **sequence-to-sequence models**. <br>\n","A sequence-to-sequence model takes a sequence as input (often a sentence or paragraph) and translates it into a different sequence. This is the task at the heart of many of the most successful applications of NLP:\n","- **Machine translation**—Convert a paragraph in a source language to its equivalent in a target language.\n","- **Text summarization**—Convert a long document to a shorter version that retains the most important information.\n","- **Question answering**—Convert an input question into its answer.\n","- **Chatbots**—Convert a dialogue prompt into a reply to this prompt, or convert the history of a conversation into the next reply in the conversation.\n","- **Text generation**—Convert a text prompt into a paragraph that completes the prompt.\n","- Etc.\n","\n","The general template behind sequence-to-sequence models is described in figure 11.12. <br>\n","**During training**,\n","- An **encoder** model turns the source sequence into an intermediate representation.\n","- A **decoder** is trained to predict the next token i in the target sequence by looking at both previous tokens (0 to i - 1) and the encoded source sequence.\n","\n","![](./images/11.12.png)\n","\n","**During inference**, we don’t have access to the target sequence—we’re trying to predict it from scratch. We’ll have to generate it one token at a time:\n","1. We obtain the encoded source sequence from the encoder.\n","2. The decoder starts by looking at the encoded source sequence as well as an initial “seed” token (such as the string \"[start]\"), and uses them to predict the first real token in the sequence.\n","3. The predicted sequence so far is fed back into the decoder, which generates the next token, and so on, until it generates a stop token (such as the string \"[end]\").\n","\n","Everything you’ve learned so far can be repurposed to build this new kind of model. <br>\n","Let’s dive in."]},{"cell_type":"markdown","metadata":{},"source":["##### A machine translation example\n","We’ll demonstrate **sequence-to-sequence** modeling on a **machine translation** task. Machine translation is precisely what Transformer was developed for! We’ll start with a recurrent sequence model, and we’ll follow up with the full **Transformer** architecture. <br>\n","We’ll be working with an English-to-Spanish translation dataset available at http://www.manythings.org/anki/ . Let’s download it:"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T20:48:31.801129Z","iopub.status.busy":"2022-05-14T20:48:31.800792Z","iopub.status.idle":"2022-05-14T20:48:32.552374Z","shell.execute_reply":"2022-05-14T20:48:32.551493Z","shell.execute_reply.started":"2022-05-14T20:48:31.801025Z"},"trusted":true},"outputs":[],"source":["!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T20:48:32.555074Z","iopub.status.busy":"2022-05-14T20:48:32.554573Z","iopub.status.idle":"2022-05-14T20:48:33.288864Z","shell.execute_reply":"2022-05-14T20:48:33.287796Z","shell.execute_reply.started":"2022-05-14T20:48:32.555025Z"},"trusted":true},"outputs":[],"source":["!unzip -q spa-eng.zip"]},{"cell_type":"markdown","metadata":{},"source":["The text file contains one example per line: an English sentence, followed by a tab character, followed by the corresponding Spanish sentence. Let’s parse this file."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T20:48:33.290736Z","iopub.status.busy":"2022-05-14T20:48:33.290484Z","iopub.status.idle":"2022-05-14T20:48:33.493571Z","shell.execute_reply":"2022-05-14T20:48:33.492791Z","shell.execute_reply.started":"2022-05-14T20:48:33.290707Z"},"trusted":true},"outputs":[],"source":["text_file = \"spa-eng/spa.txt\"\n","with open(text_file, encoding=\"utf8\") as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines: # Iterate over the lines in the file.\n","    english, spanish = line.split(\"\\t\") # Each line contains an English phrase and its Spanish translation, tab-separated.\n","    spanish = \"[start]\" + spanish + \"[end]\" # We prepend \"[start]\" and append \"[end]\" to the Spanish sentence, to match the template from figure 11.12.\n","    text_pairs.append((english, spanish))"]},{"cell_type":"markdown","metadata":{},"source":["Our text_pairs look like this:"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T20:48:33.495793Z","iopub.status.busy":"2022-05-14T20:48:33.495592Z","iopub.status.idle":"2022-05-14T20:48:33.501085Z","shell.execute_reply":"2022-05-14T20:48:33.500386Z","shell.execute_reply.started":"2022-05-14T20:48:33.495769Z"},"trusted":true},"outputs":[],"source":["import random\n","\n","print(random.choice(text_pairs))"]},{"cell_type":"markdown","metadata":{},"source":["Let’s shuffle them and split them into the usual training, validation, and test sets:"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T20:48:33.502878Z","iopub.status.busy":"2022-05-14T20:48:33.502438Z","iopub.status.idle":"2022-05-14T20:48:33.627994Z","shell.execute_reply":"2022-05-14T20:48:33.627223Z","shell.execute_reply.started":"2022-05-14T20:48:33.502842Z"},"trusted":true},"outputs":[],"source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples:]"]},{"cell_type":"markdown","metadata":{},"source":["Next, let’s prepare two separate **TextVectorization** layers: one for English and one for Spanish. We’re going to need to customize the way strings are preprocessed:\n","- We need to preserve the \"[start]\" and \"[end]\" tokens that we’ve inserted. By default, the characters [ and ] would be stripped, but we want to keep them around so we can tell apart the word “start” and the start token \"[start]\".\n","- Punctuation is different from language to language! In the Spanish **TextVectorization** layer, if we’re going to strip punctuation characters, we need to also strip the character ¿.\n","\n","Note that for a non-toy translation model, we would treat punctuation characters as separate tokens rather than stripping them, since we would want to be able to generate correctly punctuated sentences. In our case, for simplicity, we’ll get rid of all punctuation.\n","\n","##### Vectorizing the English and Spanish text pairs"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T20:48:33.629756Z","iopub.status.busy":"2022-05-14T20:48:33.629492Z","iopub.status.idle":"2022-05-14T20:48:51.600067Z","shell.execute_reply":"2022-05-14T20:48:51.599344Z","shell.execute_reply.started":"2022-05-14T20:48:33.629720Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","import string\n","import re\n","\n","# Prepare a custom string standardization function for the Spanish TextVectorization layer: \n","# it preserves [ and ] but strips ¿ (as well as all other characters from strings.punctuation).\n","strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","\n","# To keep things simple, we’ll only look at the top 15,000 words in each language, and we’ll restrict sentences to 20 words.\n","vocab_size = 15000\n","sequence_length = 20\n","\n","# The English layer\n","source_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","\n","# The Spanish layer\n","target_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1, # Generate Spanish sentences that have one extra token, since we’ll need to offset the sentence by one step during training.\n","    standardize=custom_standardization,\n",")\n","\n","train_english_texts = [pair[0] for pair in train_pairs]\n","train_spanish_texts = [pair[1] for pair in train_pairs]\n","\n","# Learn the vocabulary of each language.\n","source_vectorization.adapt(train_english_texts)\n","target_vectorization.adapt(train_spanish_texts)"]},{"cell_type":"markdown","metadata":{},"source":["Finally, we can turn our data into a **tf.data** pipeline. We want it to return a tuple (inputs, target) where **inputs** is a dict with two keys, “encoder_inputs” (the English sentence) and “decoder_inputs” (the Spanish sentence), and **target** is the Spanish sentence offset by one step ahead.\n","##### Preparing datasets for the translation task"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T20:48:51.603513Z","iopub.status.busy":"2022-05-14T20:48:51.603301Z","iopub.status.idle":"2022-05-14T20:48:52.749797Z","shell.execute_reply":"2022-05-14T20:48:52.749111Z","shell.execute_reply.started":"2022-05-14T20:48:51.603489Z"},"trusted":true},"outputs":[],"source":["batch_size = 64\n","\n","def format_dataset(eng, spa):\n","    eng = source_vectorization(eng)\n","    spa = target_vectorization(spa)\n","    return ({\n","        \"english\": eng,\n","        \"spanish\": spa[:, :-1], # The input Spanish sentence doesn’t include the last token to keep inputs and targets at the same length.\n","    }, spa[:, 1:]) # The target Spanish sentence is one step ahead. Both are still the same length (20 words).\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n","    return dataset.shuffle(2048).prefetch(16).cache() # Use in-memory caching to speed up preprocessing.\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"]},{"cell_type":"markdown","metadata":{},"source":["Here’s what our dataset outputs look like:"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T20:48:52.751141Z","iopub.status.busy":"2022-05-14T20:48:52.750893Z","iopub.status.idle":"2022-05-14T20:48:53.356447Z","shell.execute_reply":"2022-05-14T20:48:53.355723Z","shell.execute_reply.started":"2022-05-14T20:48:52.751091Z"},"trusted":true},"outputs":[],"source":["for inputs, targets in train_ds.take(1):\n","    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n","    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n","    print(f\"targets.shape: {targets.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["The data is now ready—time to build some models. We’ll start with a recurrent sequence-to-sequence model before moving on to a Transformer."]},{"cell_type":"markdown","metadata":{},"source":["##### Sequence-to-sequence learning with RNNs\n","\n","**Recurrent neural networks** dominated sequence-to-sequence learning from 2015–2017 before being overtaken by **Transformer**. They were the basis for many real world machine-translation systems—as mentioned in chapter 10, Google Translate circa 2017 was powered by a stack of seven large LSTM layers. It’s still worth learning about this approach today, as it provides an easy entry point to understanding sequence-to-sequence models. <br>\n","The simplest, naive way to use RNNs to turn a sequence into another sequence is to keep the output of the RNN at each time step. In Keras, it would look like this:\n","\n","```python\n","inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n","x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n","x = layers.LSTM(32, return_sequences=True)(x)\n","outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","model = keras.Model(inputs, outputs)\n","```\n","\n","However, there are two major issues with this approach:\n","- The target sequence must always be the same length as the source sequence. In practice, this is rarely the case. Technically, this isn’t critical, as you could always pad either the source sequence or the target sequence to make their lengths match.\n","- Due to the step-by-step nature of RNNs, the model will only be looking at tokens 0…N in the source sequence in order to predict token N in the target sequence. This constraint makes this setup unsuitable for most tasks, and particularly translation. Consider translating “The weather is nice today” to French—that would be “Il fait beau aujourd’hui.” You’d need to be able to predict “Il” from just “The,” “Il fait” from just “The weather,” etc., which is simply impossible.\n","\n","If you’re a human translator, you’d start by reading the entire source sentence before starting to translate it. This is especially important if you’re dealing with languages that have wildly different word ordering, like English and Japanese. And that’s exactly what standard sequence-to-sequence models do. <br>\n","In a proper sequence-to-sequence setup (see figure 11.13), you would first use an RNN (**the encoder**) to turn the entire source sequence into a single vector (or set of vectors). This could be the last output of the RNN, or alternatively, its final internal state vectors. Then you would use this vector (or vectors) as the initial state of another RNN (**the decoder**), which would look at elements 0…N in the target sequence, and try to predict step N+1 in the target sequence.\n","\n","![](./images/11.13.png)\n","\n","Let’s implement this in Keras with **GRU-based encoders and decoders**. The choice of GRU rather than LSTM makes things a bit simpler, since **GRU** only has a single state vector, whereas **LSTM** has multiple. <br>\n","Let’s start with the encoder.\n","\n","##### GRU-based encoder"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T20:48:53.358343Z","iopub.status.busy":"2022-05-14T20:48:53.358080Z","iopub.status.idle":"2022-05-14T20:48:56.491461Z","shell.execute_reply":"2022-05-14T20:48:56.490746Z","shell.execute_reply.started":"2022-05-14T20:48:53.358298Z"},"trusted":true},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","embed_dim = 256\n","latent_dim = 1024\n","\n","# The English source sentence goes here.\n","# Specifying the name of the input enables us to fit() the model with a dict of inputs.\n","source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n","# Don’t forget masking: it’s critical in this setup.\n","x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n","# Our encoded source sentence is the last output of a bidirectional GRU.\n","encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)"]},{"cell_type":"markdown","metadata":{},"source":["Next, let’s add the decoder—a simple GRU layer that takes as its initial state the encoded source sentence. On top of it, we add a Dense layer that produces for each output step a probability distribution over the Spanish vocabulary.\n","\n","##### GRU-based decoder and the end-to-end model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T20:48:56.494130Z","iopub.status.busy":"2022-05-14T20:48:56.493855Z","iopub.status.idle":"2022-05-14T20:48:58.020185Z","shell.execute_reply":"2022-05-14T20:48:58.019424Z","shell.execute_reply.started":"2022-05-14T20:48:56.494080Z"},"trusted":true},"outputs":[],"source":["past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\") # The Spanish target sentence goes here.\n","x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target) # Don’t forget masking.\n","decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n","# The encoded source sentence serves as the initial state of the decoder GRU.\n","x = decoder_gru(x, initial_state=encoded_source)\n","x = layers.Dropout(0.5)(x)\n","target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x) # Predicts the next token.\n","# End-to-end model: maps the source sentence and the target sentence to the target sentence one step in the future\n","seq2seq_rnn = keras.Model(inputs=[source, past_target], outputs=target_next_step)"]},{"cell_type":"markdown","metadata":{},"source":["During training, the decoder takes as input the entire target sequence, but thanks to the step-by-step nature of RNNs, it only looks at tokens 0…N in the input to predict token N in the output (which corresponds to the next token in the sequence, since the output is intended to be offset by one step). This means we only use information from the past to predict the future, as we should; otherwise we’d be cheating, and our model would not work at inference time. <br>\n","Let’s start training.\n","\n","##### Training our recurrent sequence-to-sequence model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T20:48:58.026815Z","iopub.status.busy":"2022-05-14T20:48:58.024658Z","iopub.status.idle":"2022-05-14T21:21:04.652838Z","shell.execute_reply":"2022-05-14T21:21:04.651973Z","shell.execute_reply.started":"2022-05-14T20:48:58.026776Z"},"trusted":true},"outputs":[],"source":["seq2seq_rnn.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","\n","seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"]},{"cell_type":"markdown","metadata":{},"source":["We picked accuracy as a crude way to monitor validation-set performance during training. We get to 64% accuracy: on average, the model predicts the next word in the Spanish sentence correctly 64% of the time. However, in practice, next-token accuracy isn’t a great metric for machine translation models, in particular because it makes the assumption that the correct target tokens from 0 to N are already known when predicting token N+1. In reality, during inference, you’re generating the target sentence from scratch, and you can’t rely on previously generated tokens being 100% correct. If you work on a real-world machine translation system, you will likely use **“BLEU scores”** to evaluate your models—a metric that looks at entire generated sequences and that seems to correlate well with human perception of translation quality. <br>\n","At last, let’s use our model for inference. We’ll pick a few sentences in the test set and check how our model translates them. We’ll start from the seed token, \"[start]\", and feed it into the decoder model, together with the encoded English source sentence. We’ll retrieve a next-token prediction, and we’ll re-inject it into the decoder repeatedly, sampling one new target token at each iteration, until we get to \"[end]\" or reach the maximum sentence length.\n","\n","##### Translating new sentences with our RNN encoder and decoder\n","\n","Note that this inference setup, while very simple, is rather inefficient, since we reprocess the entire source sentence and the entire generated target sentence every time we sample a new word. In a practical application, you’d factor the encoder and the decoder as two separate models, and your decoder would only run a single step at\n","each token-sampling iteration, reusing its previous internal state. <br>\n","Here are our translation results. Our model works decently well for a toy model, though it still makes many basic mistakes.\n","\n","##### Some sample results from the recurrent translation model"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T21:21:04.654672Z","iopub.status.busy":"2022-05-14T21:21:04.654402Z","iopub.status.idle":"2022-05-14T21:21:26.535739Z","shell.execute_reply":"2022-05-14T21:21:26.535022Z","shell.execute_reply.started":"2022-05-14T21:21:04.654636Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","spa_vocab = target_vectorization.get_vocabulary()\n","# Prepare a dict to convert token index predictions to string tokens.\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\" # Seed token\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization([decoded_sentence])\n","        # Sample the next token.\n","        next_token_predictions = seq2seq_rnn.predict(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n","        # Convert the next token prediction to a string and append it to the generated sentence.\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        # Exit condition: either hit max length or sample a stop character\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(20):\n","    input_sentence = random.choice(test_eng_texts)\n","    print(\"-\")\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"]},{"cell_type":"markdown","metadata":{},"source":["There are many ways this toy model could be improved: We could use a deep stack of recurrent layers for both the encoder and the decoder (note that for the decoder, this makes state management a bit more involved). We could use an LSTM instead of a GRU. And so on. Beyond such tweaks, however, the RNN approach to sequence-to-sequence learning has a few fundamental limitations:\n","- The source sequence representation has to be held entirely in the encoder state vector(s), which puts significant limitations on the size and complexity of the sentences you can translate. It’s a bit as if a human were translating a sentence entirely from memory, without looking twice at the source sentence while producing the translation.\n","- RNNs have trouble dealing with very long sequences, since they tend to progressively forget about the past—by the time you’ve reached the 100th token in either sequence, little information remains about the start of the sequence. That means RNN-based models can’t hold onto long-term context, which can be essential for translating long documents.\n","\n","These limitations are what has led the machine learning community to embrace the Transformer architecture for sequence-to-sequence problems. Let’s take a look."]},{"cell_type":"markdown","metadata":{},"source":["#### Sequence-to-sequence learning with Transformer\n","Sequence-to-sequence learning is the task where Transformer really shines. Neural attention enables Transformer models to successfully process sequences that are considerably longer and more complex than those RNNs can handle. <br>\n","As a human translating English to Spanish, you’re not going to read the English sentence one word at a time, keep its meaning in memory, and then generate the Spanish sentence one word at a time. That may work for a five-word sentence, but it’s unlikely to work for an entire paragraph. Instead, you’ll probably want to go back and forth between the source sentence and your translation in progress, and pay attention to different words in the source as you’re writing down different parts of your translation. <br>\n","That’s exactly what you can achieve with neural attention and Transformers. You’re already familiar with the **Transformer encoder**, which uses self-attention to produce context-aware representations of each token in an input sequence. In a sequence-to-sequence Transformer, the Transformer encoder would naturally play the role of the encoder, which reads the source sequence and produces an encoded representation of it. Unlike our previous RNN encoder, though, the Transformer encoder keeps the encoded representation in a sequence format: it’s a sequence of context-aware embedding vectors.\n","\n","The second half of the model is the **Transformer decoder**. Just like the RNN decoder, it reads tokens 0…N in the target sequence and tries to predict token N+1. Crucially, while doing this, it uses neural attention to identify which tokens in the encoded source sentence are most closely related to the target token it’s currently trying to predict—perhaps not unlike what a human translator would do. Recall the query-key-value model: in a Transformer decoder, the target sequence serves as an attention “query” that is used to to pay closer attention to different parts of the source sequence (the source sequence plays the roles of both keys and values)."]},{"cell_type":"markdown","metadata":{},"source":["##### THE TRANSFORMER DECODER\n","Figure 11.14 shows the full sequence-to-sequence Transformer. Look at the **decoder** internals: you’ll recognize that it looks very similar to the **Transformer encoder**, except that an extra attention block is inserted between the **self-attention block** applied to the target sequence and the dense layers of the exit block. <br>\n","Let’s implement it. Like for the **TransformerEncoder**, we’ll use a **Layer** subclass. Before we focus on the **call()**, method, where the action happens, let’s start by defining the class constructor, containing the layers we’re going to need.\n","\n","![](./images/11.14.png)\n","\n","The **call()** method is almost a straightforward rendering of the connectivity diagram from figure 11.14. But there’s an additional detail we need to take into account: **causal padding**. Causal padding is absolutely critical to successfully training a sequence-to-sequence Transformer. Unlike an RNN, which looks at its input one step at a time, and thus will only have access to steps 0...N to generate output step N (which is token N+1 in the target sequence), the **TransformerDecoder** is order-agnostic: it looks at the entire target sequence at once. If it were allowed to use its entire input, it would simply learn to copy input step N+1 to location N in the output. The model would thus achieve perfect training accuracy, but of course, when running inference, it would be completely useless, since input steps beyond N aren’t available. <br>\n","The fix is simple: we’ll mask the upper half of the pairwise attention matrix to prevent the model from paying any attention to information from the future—only information from tokens 0...N in the target sequence should be used when generating target token N+1. To do this, we’ll add a **get_causal_attention_mask(self, inputs)** method to our **TransformerDecoder** to retrieve an attention mask that we can pass to our **MultiHeadAttention** layers."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T21:43:32.931554Z","iopub.status.busy":"2022-05-14T21:43:32.931265Z","iopub.status.idle":"2022-05-14T21:43:32.948489Z","shell.execute_reply":"2022-05-14T21:43:32.947619Z","shell.execute_reply.started":"2022-05-14T21:43:32.931519Z"},"trusted":true},"outputs":[],"source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        # This attribute ensures that the layer will propagate its input mask to its outputs; masking in Keras is explicitly opt-in. \n","        # If you pass a mask to a layer that doesn’t implement compute_mask() and that doesn’t expose this supports_masking attribute, that’s an error.\n","        self.supports_masking = True\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        # Generate matrix of shape (sequence_length, sequence_length) with 1s in one half and 0s in the other.\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        # Replicate it along the batch axis to get a matrix of shape (batch_size, sequence_length, sequence_length).\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1),\n","             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n","        return tf.tile(mask, mult)\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs) # Retrieve the causal mask.\n","        # Prepare the input mask (that describes padding locations in the target sequence).\n","        if mask is not None:\n","            padding_mask = tf.cast(\n","                mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask) # Merge the two masks together.\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=causal_mask) # Pass the causal mask to the first attention layer, which performs self-attention over the target sequence.\n","        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","        attention_output_2 = self.attention_2(\n","            query=attention_output_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask, # Pass the combined mask to the second attention layer, which relates the source sequence to the target sequence.\n","        )\n","        attention_output_2 = self.layernorm_2(\n","            attention_output_1 + attention_output_2)\n","        proj_output = self.dense_proj(attention_output_2)\n","        return self.layernorm_3(attention_output_2 + proj_output)"]},{"cell_type":"markdown","metadata":{},"source":["##### PUTTING IT ALL TOGETHER: A TRANSFORMER FOR MACHINE TRANSLATION\n","The **end-to-end Transformer** is the model we’ll be training. It maps the source sequence and the target sequence to the target sequence one step in the future. It straightforwardly combines the pieces we’ve built so far: **PositionalEmbedding** layers, the **TransformerEncoder**, and the **TransformerDecoder**. Note that both the TransformerEncoder and the TransformerDecoder are shape-invariant, so you could be stacking many of them to create a more powerful encoder or decoder. In our example, we’ll stick to a single instance of each.\n","\n","##### PositionalEmbedding layer"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-05-14T21:46:21.914697Z","iopub.status.busy":"2022-05-14T21:46:21.914391Z","iopub.status.idle":"2022-05-14T21:46:21.926626Z","shell.execute_reply":"2022-05-14T21:46:21.925814Z","shell.execute_reply.started":"2022-05-14T21:46:21.914667Z"},"trusted":true},"outputs":[],"source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=input_dim, output_dim=output_dim)\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super(PositionalEmbedding, self).get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config"]},{"cell_type":"markdown","metadata":{},"source":["##### End-to-end Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embed_dim = 256\n","dense_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) # Encode the source sentence.\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs) # Encode the target sentence and combine it with the encoded source sentence.\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) # Predict a word for each output position.\n","transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"]},{"cell_type":"markdown","metadata":{},"source":["##### Training the sequence-to-sequence Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transformer.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","transformer.fit(train_ds, epochs=30, validation_data=val_ds)"]},{"cell_type":"markdown","metadata":{},"source":["Finally, let’s try using our model to translate never-seen-before English sentences from the test set. The setup is identical to what we used for the sequence-to-sequence RNN model.\n","\n","##### Translating new sentences with our Transformer model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","spa_vocab = target_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization(\n","            [decoded_sentence])[:, :-1]\n","        # Sample the next token.\n","        predictions = transformer(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        # Convert the next token prediction to a string, and append it to the generated sentence.\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(20):\n","    input_sentence = random.choice(test_eng_texts)\n","    print(\"-\")\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"]},{"cell_type":"markdown","metadata":{},"source":["Subjectively, the Transformer seems to perform significantly better than the GRU based translation model. It’s still a toy model, but it’s a better toy model.\n","\n","That concludes this chapter on natural language processing—you just went from the very basics to a fully fledged Transformer that can translate from English to Spanish. Teaching machines to make sense of language is the latest superpower you can add to your collection."]},{"cell_type":"markdown","metadata":{},"source":["#### Summary\n","- There are two kinds of NLP models: **bag-of-words models** that process sets of words or **N-grams** without taking into account their order, and **sequence models** that process word order. A bag-of-words model is made of Dense layers, while a sequence model could be an RNN, a 1D convnet, or a Transformer.\n","- When it comes to text classification, the ratio between the number of samples in your training data and the mean number of words per sample can help you determine whether you should use a bag-of-words model or a sequence model.\n","- **Word embeddings** are vector spaces where semantic relationships between words are modeled as distance relationships between vectors that represent those words.\n","- **Sequence-to-sequence learning** is a generic, powerful learning framework that can be applied to solve many NLP problems, including machine translation. A sequence-to-sequence model is made of an **encoder**, which processes a source sequence, and a **decoder**, which tries to predict future tokens in target sequence by looking at past tokens, with the help of the encoder-processed source sequence.\n","- **Neural attention** is a way to create context-aware word representations. It’s the basis for the **Transformer** architecture.\n","- The Transformer architecture, which consists of a TransformerEncoder and a TransformerDecoder, yields excellent results on sequence-to-sequence tasks. The first half, the TransformerEncoder, can also be used for text classification or any sort of single-input NLP task."]}],"metadata":{"interpreter":{"hash":"f8ee13a16f7ff347d089854b949fd5a4fdba136de942caaffeaf6bff99e7e7f9"},"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
