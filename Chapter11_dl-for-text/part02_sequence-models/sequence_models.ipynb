{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH2LEpwPimH9"
      },
      "source": [
        "#### Processing words as a sequence: The sequence model approach\n",
        "These past few examples clearly show that word order matters: manual engineering of order-based features, such as bigrams, yields a nice accuracy boost. Now remember: the history of deep learning is that of a move away from manual feature engineering, toward letting models learn their own features from exposure to data alone. What if, instead of manually crafting order-based features, we exposed the model to raw word sequences and let it figure out such features on its own? This is what **sequence models** are about. <br>\n",
        "To implement a sequence model, you’d start by representing your input samples as sequences of integer indices (one integer standing for one word). Then, you’d map each integer to a vector to obtain vector sequences. Finally, you’d feed these sequences of vectors into a stack of layers that could cross-correlate features from adjacent vectors, such as a 1D convnet, a RNN, or a Transformer. <br>\n",
        "For some time around 2016–2017, bidirectional RNNs (in particular, bidirectional LSTMs) were considered to be the state of the art for sequence modeling. Since you’re already familiar with this architecture, this is what we’ll use in our first sequence model examples. However, nowadays sequence modeling is almost universally done with **Transformers**, which we will cover shortly. Oddly, one-dimensional convnets were never very popular in NLP, even though, in my own experience, a residual stack of depthwise- separable 1D convolutions can often achieve comparable performance to a bidirectional LSTM, at a greatly reduced computational cost.\n",
        "\n",
        "##### A FIRST PRACTICAL EXAMPLE\n",
        "Let’s try out a first sequence model in practice. First, let’s prepare datasets that return integer sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzA4BrGCimIH"
      },
      "source": [
        "**Downloading the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYzECm35imIH",
        "outputId": "7a70bdb1-0ed9-4504-9da4-92dd0e31482e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  7640k      0  0:00:10  0:00:10 --:--:-- 16.7M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KpSx6XYRimIJ"
      },
      "outputs": [],
      "source": [
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlRl57BzimIJ"
      },
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "DX30CgLAi35J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ndMOxkFpimIK"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgIo2SY9imIK",
        "outputId": "d97096ad-5bd7-4d87-d1ad-b01fc6af1c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i6FZHbwimIL"
      },
      "source": [
        "##### Preparing integer sequence datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_yC6-P2RimIL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "# In order to keep a manageable input size, we’ll truncate the inputs after the first 600 words. \n",
        "# This is a reasonable choice, since the average review length is 233 words, and only 5% of reviews are longer than 600 words.\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG8QEeo4imIM"
      },
      "source": [
        "Next, let’s make a model. The simplest way to convert our integer sequences to vector sequences is to **one-hot encode** the integers (each dimension would represent one possible term in the vocabulary). On top of these one-hot vectors, we’ll add a simple **bidirectional LSTM**.\n",
        "\n",
        "##### A sequence model built on one-hot encoded vector sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxHKiSqOimIM",
        "outputId": "c6b7afb5-9bc9-4b07-c807-1ff1a9363d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               5128448   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,128,513\n",
            "Trainable params: 5,128,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\") # One input is a sequence of integers.\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens) # Encode the integers into binary 20,000 dimensional vectors.\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded) # Add a bidirectional LSTM layer with 32 units.\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # Finally, add a classification layer.\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-nas7DcimIM"
      },
      "source": [
        "Now, let’s train our model.\n",
        "\n",
        "##### Training a first basic sequence model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzWWtZFoimIN",
        "outputId": "7bb1995c-75b1-40ef-f0b0-6b7b3efceeb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 165s 250ms/step - loss: 0.5137 - accuracy: 0.7594 - val_loss: 0.3851 - val_accuracy: 0.8546\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 160s 256ms/step - loss: 0.3294 - accuracy: 0.8828 - val_loss: 0.3140 - val_accuracy: 0.8746\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 162s 260ms/step - loss: 0.2743 - accuracy: 0.9041 - val_loss: 0.3573 - val_accuracy: 0.8506\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 162s 258ms/step - loss: 0.2340 - accuracy: 0.9183 - val_loss: 0.3387 - val_accuracy: 0.8806\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 163s 261ms/step - loss: 0.1992 - accuracy: 0.9324 - val_loss: 0.2855 - val_accuracy: 0.8888\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 164s 262ms/step - loss: 0.1791 - accuracy: 0.9404 - val_loss: 0.4167 - val_accuracy: 0.7988\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 162s 259ms/step - loss: 0.1547 - accuracy: 0.9485 - val_loss: 0.4332 - val_accuracy: 0.8560\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 162s 259ms/step - loss: 0.1424 - accuracy: 0.9534 - val_loss: 0.4484 - val_accuracy: 0.8828\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 162s 260ms/step - loss: 0.1211 - accuracy: 0.9615 - val_loss: 0.3748 - val_accuracy: 0.8672\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 162s 259ms/step - loss: 0.0979 - accuracy: 0.9679 - val_loss: 0.4682 - val_accuracy: 0.8830\n",
            "782/782 [==============================] - 99s 125ms/step - loss: 0.3117 - accuracy: 0.8772\n",
            "Test acc: 0.877\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A first observation: this model trains very slowly, especially compared to the lightweight model of the previous section. This is because our inputs are quite large: each input sample is encoded as a matrix of size (600, 20000) (600 words per sample, 20,000 possible words). That’s 12,000,000 floats for a single movie review. Our bidirectional LSTM has a lot of work to do. Second, the model only gets to 87% test accuracy—it doesn’t perform nearly as well as our (very fast) binary unigram model. Clearly, using one-hot encoding to turn words into vectors, which was the simplest thing we could do, wasn’t a great idea. There’s a better way: **word embeddings**."
      ],
      "metadata": {
        "id": "Xb3WlPyLpYXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### UNDERSTANDING WORD EMBEDDINGS\n",
        "Crucially, when you encode something via one-hot encoding, you’re making a feature engineering decision. You’re injecting into your model a fundamental assumption about the structure of your feature space. That assumption is that **the different tokens you’re encoding are all independent from each other:** indeed, one-hot vectors are all orthogonal to one another. And in the case of words, that assumption is **clearly wrong**. Words form a structured space: they share information with each other. The words “movie” and “film” are interchangeable in most sentences, so the vector that represents “movie” should not be orthogonal to the vector that represents “film”—they should be the same vector, or close enough. <br>\n",
        "To get a bit more abstract, the **geometric relationship** between two word vectors should reflect the **semantic relationship** between these words. For instance, in a reasonable word vector space, you would expect synonyms to be embedded into similar word vectors, and in general, you would expect the geometric distance (such as the cosine distance or L2 distance) between any two word vectors to relate to the “semantic distance” between the associated words. Words that mean different things should lie far away from each other, whereas related words should be closer. <br>\n",
        "Word embeddings are vector representations of words that achieve exactly this: they map human language into a structured geometric space. <br>\n",
        "Whereas the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (the same dimensionality as the number of words in the vocabulary), word embeddings are low-dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors); see figure 11.2. It’s common to see word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large vocabularies. On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or greater (capturing a vocabulary of 20,000 tokens, in this case). So, word embeddings pack more information into far fewer dimensions.\n",
        "\n",
        "![](./images/11.2.png)\n",
        "\n",
        "Besides being dense representations, word embeddings are also structured representations, and their structure is learned from data. Similar words get embedded in close locations, and further, specific directions in the embedding space are meaningful. To make this clearer, let’s look at a concrete example.<br>\n",
        "In figure 11.3, four words are embedded on a 2D plane: cat, dog, wolf, and tiger. With the vector representations we chose here, some semantic relationships between these words can be encoded as geometric transformations. For instance, the same vector allows us to go from cat to tiger and from dog to wolf: this vector could be interpreted as the “from pet to wild animal” vector. Similarly, another vector lets us go from dog to cat and from wolf to tiger, which could be interpreted as a “from canine to feline” vector.\n",
        "\n",
        "![](./images/11.3.png)\n",
        "\n",
        "In real-world word-embedding spaces, common examples of meaningful geometric transformations are “gender” vectors and “plural” vectors. For instance, by adding a “female” vector to the vector “king,” we obtain the vector “queen.” By adding a “plural” vector, we obtain “kings.” Word-embedding spaces typically feature thousands of such interpretable and potentially useful vectors. <br>\n",
        "Let’s look at how to use such an embedding space in practice. There are two ways to obtain word embeddings:\n",
        "- Learn word embeddings jointly with the main task you care about (such as document classification or sentiment prediction). In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of a neural network.\n",
        "- Load into your model word embeddings that were precomputed using a different machine learning task than the one you’re trying to solve. These are called **pretrained word embeddings**.\n",
        "\n",
        "Let’s review each of these approaches."
      ],
      "metadata": {
        "id": "2xgrFENSptf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER\n",
        "Is there some ideal word-embedding space that would perfectly map human language and could be used for any natural language processing task? Possibly, but we have yet to compute anything of the sort. Also, there is no such a thing as human language—there are many different languages, and they aren’t isomorphic to one another, because a language is the reflection of a specific culture and a specific context. But more pragmatically, what makes a good word-embedding space depends heavily on your task: the perfect word-embedding space for an English-language movie-review sentiment-analysis model may look different from the perfect embedding space for an English-language legal-document classification model, because the importance of certain semantic relationships varies from task to task. <br>\n",
        "It’s thus reasonable to learn a new embedding space with every new task. Fortunately, backpropagation makes this easy, and Keras makes it even easier. It’s about learning the weights of a layer: the **Embedding** layer.\n",
        "\n",
        "##### Instantiating an Embedding layer"
      ],
      "metadata": {
        "id": "IdifbIfY2F00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Embedding layer takes at least two arguments: the number of possible tokens and the dimensionality of the embeddings (here, 256).\n",
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ],
      "metadata": {
        "id": "fxZh8nXzzN6n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Embedding** layer is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, looks up these integers in an internal dictionary, and returns the associated vectors. It’s effectively a dictionary lookup (see figure 11.4).\n",
        "\n",
        "![](./images/11.4.png)\n",
        "\n",
        "The **Embedding** layer takes as input a rank-2 tensor of integers, of shape (batch_size, sequence_length), where each entry is a sequence of integers. The layer then returns a 3D floating-point tensor of shape (batch_size, sequence_length, embedding_dimensionality). <br>\n",
        "When you instantiate an Embedding layer, its weights (its internal dictionary of token vectors) are initially random, just as with any other layer. During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure—a kind of structure specialized for the specific problem for which you’re training your model. <br>\n",
        "Let’s build a model that includes an **Embedding** layer and benchmark it on our task.\n",
        "\n",
        "##### Model that uses an Embedding layer trained from scratch"
      ],
      "metadata": {
        "id": "3qol-jyJ74Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe3U6dTX-dcb",
        "outputId": "92dff673-6bfe-4789-a7e8-0f26b37990a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 64)               73984     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 36s 53ms/step - loss: 0.4804 - accuracy: 0.7808 - val_loss: 0.3679 - val_accuracy: 0.8468\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.2968 - accuracy: 0.8925 - val_loss: 0.3406 - val_accuracy: 0.8492\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.2392 - accuracy: 0.9148 - val_loss: 0.4613 - val_accuracy: 0.8120\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.1949 - accuracy: 0.9341 - val_loss: 0.3247 - val_accuracy: 0.8794\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.1696 - accuracy: 0.9438 - val_loss: 0.3495 - val_accuracy: 0.8838\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.1449 - accuracy: 0.9538 - val_loss: 0.3505 - val_accuracy: 0.8790\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.1206 - accuracy: 0.9603 - val_loss: 0.3449 - val_accuracy: 0.8784\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 32s 50ms/step - loss: 0.0974 - accuracy: 0.9685 - val_loss: 0.4121 - val_accuracy: 0.8856\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 32s 50ms/step - loss: 0.0807 - accuracy: 0.9737 - val_loss: 0.4232 - val_accuracy: 0.8642\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.0687 - accuracy: 0.9785 - val_loss: 0.5079 - val_accuracy: 0.8770\n",
            "782/782 [==============================] - 20s 25ms/step - loss: 0.3545 - accuracy: 0.8660\n",
            "Test acc: 0.866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It trains much faster than the one-hot model (since the LSTM only has to process 256-dimensional vectors instead of 20,000-dimensional), and its test accuracy is comparable (87%). However, we’re still some way off from the results of our basic bigram model. Part of the reason why is simply that the model is looking at slightly less data: the bigram model processed full reviews, while our sequence model **truncates** sequences after 600 words.\n",
        "\n",
        "##### UNDERSTANDING PADDING AND MASKING\n",
        "One thing that’s slightly hurting model performance here is that our input sequences are full of zeros. This comes from our use of the **output_sequence_length=max_length** option in **TextVectorization** (with max_length equal to 600): sentences longer than 600 tokens are truncated to a length of 600 tokens, and sentences shorter than 600 tokens are padded with zeros at the end so that they can be concatenated together with other sequences to form contiguous batches. <br>\n",
        "We’re using a bidirectional RNN: two RNN layers running in parallel, with one processing the tokens in their natural order, and the other processing the same tokens in reverse. The RNN that looks at the tokens in their natural order will spend its last iterations seeing only vectors that encode padding—possibly for several hundreds of iterations if the original sentence was short. The information stored in the internal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs. <br>\n",
        "We need some way to tell the RNN that it should skip these iterations. There’s an API for that: **masking**. <br>\n",
        "The **Embedding** layer is capable of generating a “mask” that corresponds to its input data. This mask is a tensor of ones and zeros (or True/False booleans), of shape (batch_size, sequence_length), where the entry mask[i, t] indicates where timestep t of sample i should be skipped or not (the timestep will be skipped if mask[i, t] is 0 or False, and processed otherwise). <br>\n",
        "By default, this option isn’t active—you can turn it on by passing **mask_zero=True** to your **Embedding** layer. You can retrieve the mask with **the compute_mask()** method:\n",
        "\n",
        "```python\n",
        "embedding_layer = Embedding(input_dim=10, output_dim=256, mask_zero=True)\n",
        "some_input = [\n",
        "... [4, 3, 2, 1, 0, 0, 0],\n",
        "... [5, 4, 3, 2, 1, 0, 0],\n",
        "... [2, 1, 0, 0, 0, 0, 0]]\n",
        "mask = embedding_layer.compute_mask(some_input)\n",
        "<tf.Tensor: shape=(3, 7), dtype=bool, numpy=\n",
        "array([[ True, True, True, True, False, False, False],\n",
        "[ True, True, True, True, True, False, False],\n",
        "[ True, True, False, False, False, False, False]])>\n",
        "```\n",
        "In practice, you will almost never have to manage masks by hand. Instead, Keras will automatically pass on the mask to every layer that is able to process it (as a piece of metadata attached to the sequence it represents). This mask will be used by RNN layers to skip masked steps. If your model returns an entire sequence, the mask will also be used by the loss function to skip masked steps in the output sequence. <br>\n",
        "Let’s try retraining our model with masking enabled.\n",
        "\n",
        "##### Using an Embedding layer with masking enabled\n"
      ],
      "metadata": {
        "id": "OE9if3OXBI8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXFXdH9Aew2Q",
        "outputId": "291baf6c-f76a-4b6c-e26b-878d2ecc6eae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 64)               73984     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 47s 65ms/step - loss: 0.3958 - accuracy: 0.8193 - val_loss: 0.2904 - val_accuracy: 0.8812\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.2328 - accuracy: 0.9095 - val_loss: 0.2701 - val_accuracy: 0.8950\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.1688 - accuracy: 0.9367 - val_loss: 0.3590 - val_accuracy: 0.8932\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.1225 - accuracy: 0.9550 - val_loss: 0.3304 - val_accuracy: 0.8880\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.0921 - accuracy: 0.9677 - val_loss: 0.3746 - val_accuracy: 0.8864\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.0687 - accuracy: 0.9762 - val_loss: 0.3969 - val_accuracy: 0.8850\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.0496 - accuracy: 0.9842 - val_loss: 0.4371 - val_accuracy: 0.8864\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.0354 - accuracy: 0.9887 - val_loss: 0.5420 - val_accuracy: 0.8810\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 37s 58ms/step - loss: 0.0287 - accuracy: 0.9902 - val_loss: 0.5582 - val_accuracy: 0.8774\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.0224 - accuracy: 0.9932 - val_loss: 0.5341 - val_accuracy: 0.8768\n",
            "782/782 [==============================] - 23s 27ms/step - loss: 0.2931 - accuracy: 0.8855\n",
            "Test acc: 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we get to 88% test accuracy—a small but noticeable improvement."
      ],
      "metadata": {
        "id": "0bEMAhQusDzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### USING PRETRAINED WORD EMBEDDINGS\n",
        "Sometimes you have so little training data available that you can’t use your data alone to learn an appropriate task-specific embedding of your vocabulary. In such cases, instead of learning word embeddings jointly with the problem you want to solve, you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties—one that captures generic aspects of language structure. The rationale behind using pretrained word embeddings in natural language processing is much the same as for using pretrained convnets in image classification: you don’t have enough data available to learn truly powerful features on your own, but you expect that the features you need are fairly generic—that is, common visual features or semantic features. In this case, it makes sense to reuse features learned on a different problem. <br>\n",
        "Such word embeddings are generally computed using word-occurrence statistics (observations about what words co-occur in sentences or documents), using a variety of techniques, some involving neural networks, others not. The idea of a dense, low dimensional embedding space for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s, but it only started to take off in research and industry applications after the release of one of the most famous and successful word-embedding schemes: the **Word2Vec** algorithm (https://code.google.com/archive/p/word2vec), developed by Tomas Mikolov at Google in 2013. **Word2Vec** dimensions capture specific semantic properties, such as gender. <br>\n",
        "There are various precomputed databases of word embeddings that you can download and use in a Keras **Embedding** layer. **Word2vec** is one of them. Another popular one is called **Global Vectors for Word Representation** (**GloVe**, https://nlp.stanford.edu/projects/glove), which was developed by Stanford researchers in 2014. This embedding technique is based on factorizing a matrix of word co-occurrence statistics. Its developers have made available precomputed embeddings for millions of English tokens, obtained from Wikipedia data and Common Crawl data. <br>\n",
        "Let’s look at how you can get started using **GloVe** embeddings in a Keras model. The same method is valid for **Word2Vec** embeddings or any other word-embedding database. We’ll start by downloading the **GloVe** files and parse them. We’ll then load the word vectors into a Keras **Embedding** layer, which we’ll use to build a new model.\n",
        "\n",
        "First, let’s download the **GloVe** word embeddings precomputed on the 2014 English Wikipedia dataset. It’s an 822 MB zip file containing 100-dimensional embedding vectors for 400,000 words (or non-word tokens)."
      ],
      "metadata": {
        "id": "bv_53lMdsNYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHpB-DtGwu_i",
        "outputId": "f56ac3ce-ae2c-4f2a-8829-e6772a11c82f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-09 16:59:46--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-05-09 16:59:46--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-05-09 16:59:47--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.13MB/s    in 2m 44s  \n",
            "\n",
            "2022-05-09 17:02:31 (5.03 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "tSLQViz0xp5B"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation.\n",
        "\n",
        "##### Parsing the GloVe word-embeddings file"
      ],
      "metadata": {
        "id": "8zhqFcaTybht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X77QXSrqyiZn",
        "outputId": "ab060bb3-8f6c-40bc-9917-c1b712071837"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s build an embedding matrix that you can load into an **Embedding** layer. It must be a matrix of shape (max_words, embedding_dim), where each entry i contains the embedding_dim-dimensional vector for the word of index i in the reference word index (built during tokenization).\n",
        "\n",
        "##### Preparing the GloVe word-embeddings matrix"
      ],
      "metadata": {
        "id": "4TtNhr5ozd3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "# Retrieve the vocabulary indexed by our previous TextVectorization layer.\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "# Use it to create a mapping from words to their index in the vocabulary.\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "# Prepare a matrix that we’ll fill with the GloVe vectors.\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    # Fill entry i in the matrix with the word vector for index i. Words not found in the embedding index will be all zeros.\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "PYVKF6z22Hfp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we use a **Constant** initializer to load the pretrained embeddings in an **Embedding** layer. So as not to disrupt the pretrained representations during training, we freeze the layer via **trainable=False**:"
      ],
      "metadata": {
        "id": "8DXkYqzo2_dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True\n",
        ")"
      ],
      "metadata": {
        "id": "4KpFTLrR3ND-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re now ready to train a new model—identical to our previous model, but leveraging the 100-dimensional pretrained **GloVe** embeddings instead of 128-dimensional learned embeddings.\n",
        "\n",
        "##### Model that uses a pretrained Embedding layer"
      ],
      "metadata": {
        "id": "_p8J9czE3yYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvl2gWsr37Oj",
        "outputId": "f2bf95f5-51e2-47f4-f72a-5aa815daadbd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, None, 100)         2000000   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 64)               34048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,034,113\n",
            "Trainable params: 34,113\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 44s 55ms/step - loss: 0.5733 - accuracy: 0.6962 - val_loss: 0.4711 - val_accuracy: 0.7886\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4559 - accuracy: 0.7911 - val_loss: 0.3917 - val_accuracy: 0.8248\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.4054 - accuracy: 0.8209 - val_loss: 0.4072 - val_accuracy: 0.8256\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3753 - accuracy: 0.8360 - val_loss: 0.3422 - val_accuracy: 0.8518\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.3491 - accuracy: 0.8493 - val_loss: 0.3272 - val_accuracy: 0.8588\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.3274 - accuracy: 0.8618 - val_loss: 0.3162 - val_accuracy: 0.8662\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3084 - accuracy: 0.8727 - val_loss: 0.3040 - val_accuracy: 0.8722\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.2933 - accuracy: 0.8791 - val_loss: 0.3008 - val_accuracy: 0.8750\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.2772 - accuracy: 0.8870 - val_loss: 0.3021 - val_accuracy: 0.8768\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.2658 - accuracy: 0.8931 - val_loss: 0.2981 - val_accuracy: 0.8740\n",
            "782/782 [==============================] - 20s 23ms/step - loss: 0.2987 - accuracy: 0.8746\n",
            "Test acc: 0.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ll find that on this particular task, pretrained embeddings aren’t very helpful, because the dataset contains enough samples that it is possible to learn a specialized enough embedding space from scratch. However, leveraging pretrained embeddings can be very helpful when you’re working with a smaller dataset."
      ],
      "metadata": {
        "id": "du1T9AhY6cvH"
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f8ee13a16f7ff347d089854b949fd5a4fdba136de942caaffeaf6bff99e7e7f9"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "sequence-models.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}