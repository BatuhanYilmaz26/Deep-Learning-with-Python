{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### Generating images with variational autoencoders\n","The most popular and successful application of creative AI today is image generation: learning latent visual spaces and sampling from them to create entirely new pictures interpolated from real ones—pictures of imaginary people, imaginary places, imaginary cats and dogs, and so on. <br>\n","In this section and the next, we’ll review some high-level concepts pertaining to image generation, alongside implementation details relative to the two main techniques in this domain: **variational autoencoders** (VAEs) and **generative adversarial networks** (GANs). Note that the techniques I’ll present here aren’t specific to images—you could develop latent spaces of sound, music, or even text, using GANs and VAEs—but in practice, the most interesting results have been obtained with pictures, and that’s what we’ll focus on here.\n","\n","##### Sampling from latent spaces of images\n","The key idea of image generation is to develop a low-dimensional latent space of representations (which, like everything else in deep learning, is a vector space), where any point can be mapped to a “valid” image: an image that looks like the real thing. The module capable of realizing this mapping, taking as input a latent point and outputting an image (a grid of pixels), is called a **generator** (in the case of GANs) or a **decoder** (in the case of VAEs). Once such a latent space has been learned, you can sample points from it, and, by mapping them back to image space, generate images that have never been seen before (see figure 12.13). These new images are the in-betweens of the training images. <br>\n","\n","![](./images/12.13.png)\n","\n","GANs and VAEs are two different strategies for learning such latent spaces of image representations, each with its own characteristics. VAEs are great for learning latent spaces that are well structured, where specific directions encode a meaningful axis of variation in the data (see figure 12.14). GANs generate images that can potentially be highly realistic, but the latent space they come from may not have as much structure and continuity.\n","\n","![](./images/12.14.png)"]},{"cell_type":"markdown","metadata":{},"source":["##### Concept vectors for image editing\n","We already hinted at the idea of a **concept vector** when we covered word embeddings in chapter 11. The idea is still the same: given a latent space of representations, or an embedding space, certain directions in the space may encode interesting axes of variation in the original data. In a latent space of images of faces, for instance, there may be a **smile vector**, such that if latent point z is the embedded representation of a certain face, then latent point z + s is the embedded representation of the same face, smiling. Once you’ve identified such a vector, it then becomes possible to edit images by projecting them into the latent space, moving their representation in a meaningful way, and then decoding them back to image space. There are concept vectors for essentially any independent dimension of variation in image space—in the case of faces, you may discover vectors for adding sunglasses to a face, removing glasses, turning a male face into a female face, and so on. Figure 12.15 is an example of a smile vector, a concept vector discovered by Tom White, from the Victoria University School of Design in New Zealand, using VAEs trained on a dataset of faces of celebrities (the CelebA dataset).\n","\n","![](./images/12.15.png)"]},{"cell_type":"markdown","metadata":{},"source":["##### Variational autoencoders\n","Variational autoencoders, simultaneously discovered by Kingma and Welling in December 2013 and Rezende, Mohamed, and Wierstra in January 2014, are a kind of generative model that’s especially appropriate for the task of image editing via concept vectors. They’re a modern take on autoencoders (a type of network that aims to encode an input to a low-dimensional latent space and then decode it back) that mixes ideas from deep learning with Bayesian inference. <br>\n","A classical image autoencoder takes an image, maps it to a latent vector space via an encoder module, and then decodes it back to an output with the same dimensions as the original image, via a decoder module (see figure 12.16). It’s then trained by using as target data the same images as the input images, meaning the autoencoder learns to reconstruct the original inputs. By imposing various constraints on the code (the output of the encoder), you can get the autoencoder to learn more- or less-interesting latent representations of the data. Most commonly, you’ll constrain the code to be low-dimensional and sparse (mostly zeros), in which case the encoder acts as a way to compress the input data into fewer bits of information.\n","\n","![](./images/12.16.png)\n","\n","In practice, such classical autoencoders don’t lead to particularly useful or nicely structured latent spaces. They’re not much good at compression, either. For these reasons, they have largely fallen out of fashion. VAEs, however, augment autoencoders with a little bit of statistical magic that forces them to learn continuous, highly structured latent spaces. They have turned out to be a powerful tool for image generation. <br>\n","A VAE, instead of compressing its input image into a fixed code in the latent space, turns the image into the parameters of a statistical distribution: a mean and a variance. Essentially, this means we’re assuming the input image has been generated by a statistical process, and that the randomness of this process should be taken into account during encoding and decoding. The VAE then uses the mean and variance parameters to randomly sample one element of the distribution, and decodes that element back to the original input (see figure 12.17). The stochasticity of this process improves robustness and forces the latent space to encode meaningful representations everywhere: every point sampled in the latent space is decoded to a valid output. <br>\n","In technical terms, here’s how a VAE works:\n","1. An encoder module turns the input sample, input_img, into two parameters in a latent space of representations, z_mean and z_log_variance.\n","2. You randomly sample a point z from the latent normal distribution that’s assumed to generate the input image, via z = z_mean + exp(z_log_variance) * epsilon, where epsilon is a random tensor of small values.\n","3. A decoder module maps this point in the latent space back to the original input image.\n","\n","![](./images/12.17.png)\n","\n","Because **epsilon** is random, the process ensures that every point that’s close to the latent location where you encoded input_img (z-mean) can be decoded to something similar to input_img, thus forcing the latent space to be continuously meaningful. Any two close points in the latent space will decode to highly similar images. Continuity, combined with the low dimensionality of the latent space, forces every direction in the latent space to encode a meaningful axis of variation of the data, making the latent space very structured and thus highly suitable to manipulation via concept vectors. <br>\n","The parameters of a VAE are trained via two loss functions: a reconstruction loss that forces the decoded samples to match the initial inputs, and a regularization loss that helps learn well-rounded latent distributions and reduces overfitting to the training data. Schematically, the process looks like this:\n","\n","```python\n","z_mean, z_log_variance = encoder(input_img) # Encodes the input into mean and variance parameters\n","z = z_mean + exp(z_log_variance) * epsilon # Draws a latent point using a small random epsilon\n","reconstructed_img = decoder(z) # Decodes z back to an image\n","model = Model(input_img, reconstructed_img) # Instantiates the autoencoder model, which maps an input image to its reconstruction\n","```\n","\n","You can then train the model using the reconstruction loss and the regularization loss. For the regularization loss, we typically use an expression (the Kullback–Leibler divergence) meant to nudge the distribution of the encoder output toward a well-rounded normal distribution centered around 0. This provides the encoder with a sensible assumption about the structure of the latent space it’s modeling. <br>\n","Now let’s see what implementing a VAE looks like in practice!"]},{"cell_type":"markdown","metadata":{},"source":["##### Implementing a VAE with Keras\n","We’re going to be implementing a VAE that can generate MNIST digits. It’s going to have three parts:\n","- An encoder network that turns a real image into a mean and a variance in the latent space\n","- A sampling layer that takes such a mean and variance, and uses them to sample a random point from the latent space\n","- A decoder network that turns points from the latent space back into images\n","\n","The following listing shows the encoder network we’ll use, mapping images to the parameters of a probability distribution over the latent space. It’s a simple convnet that maps the input image x to two vectors, z_mean and z_log_var. One important detail is that **we use strides for downsampling feature maps instead of max pooling**. The last time we did this was in the image segmentation example in chapter 9. Recall that, in general, strides are preferable to max pooling for any model that cares about **information location**—that is to say, where stuff is in the image—and this one does, since it will have to produce an image encoding that can be used to reconstruct a valid image.\n","\n","##### VAE encoder network"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-05-21T05:54:55.344793Z","iopub.status.busy":"2022-05-21T05:54:55.344026Z","iopub.status.idle":"2022-05-21T05:55:03.480638Z","shell.execute_reply":"2022-05-21T05:55:03.479166Z","shell.execute_reply.started":"2022-05-21T05:54:55.344685Z"},"trusted":true},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","latent_dim = 2 # Dimensionality of the latent space: a 2D plane\n","\n","encoder_inputs = keras.Input(shape=(28, 28, 1))\n","x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n","x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","x = layers.Flatten()(x)\n","x = layers.Dense(16)(x)\n","# The input image ends up being encoded into these two parameters\n","z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n","z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n","\n","encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")"]},{"cell_type":"markdown","metadata":{},"source":["Its summary looks like this:"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-21T05:55:03.482919Z","iopub.status.busy":"2022-05-21T05:55:03.482441Z","iopub.status.idle":"2022-05-21T05:55:03.495506Z","shell.execute_reply":"2022-05-21T05:55:03.494758Z","shell.execute_reply.started":"2022-05-21T05:55:03.482882Z"},"trusted":true},"outputs":[],"source":["encoder.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Next is the code for using z_mean and z_log_var, the parameters of the statistical distribution assumed to have produced input_img, to generate a latent space point z.\n","\n","##### Latent-space-sampling layer"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-21T05:55:03.501420Z","iopub.status.busy":"2022-05-21T05:55:03.500857Z","iopub.status.idle":"2022-05-21T05:55:03.508062Z","shell.execute_reply":"2022-05-21T05:55:03.507226Z","shell.execute_reply.started":"2022-05-21T05:55:03.501367Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","class Sampler(layers.Layer):\n","    def call(self, z_mean, z_log_var):\n","        batch_size = tf.shape(z_mean)[0]\n","        z_size = tf.shape(z_mean)[1]\n","        # Draw a batch of random normal vectors.\n","        epsilon = tf.random.normal(shape=(batch_size, z_size))\n","        # Apply the VAE sampling formula.\n","        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"]},{"cell_type":"markdown","metadata":{},"source":["The following listing shows the decoder implementation. We reshape the vector z to the dimensions of an image and then use a few convolution layers to obtain a final image output that has the same dimensions as the original input_img.\n","\n","##### VAE decoder network, mapping latent space points to images"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-21T05:55:03.509996Z","iopub.status.busy":"2022-05-21T05:55:03.509507Z","iopub.status.idle":"2022-05-21T05:55:03.563352Z","shell.execute_reply":"2022-05-21T05:55:03.562646Z","shell.execute_reply.started":"2022-05-21T05:55:03.509959Z"},"trusted":true},"outputs":[],"source":["latent_inputs = keras.Input(shape=(latent_dim,)) # Input where we’ll feed z\n","# Produce the same number of coefficients that we had at the level of the Flatten layer in the encoder.\n","x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n","x = layers.Reshape((7, 7, 64))(x) # Revert the Flatten layer of the encoder.\n","# Revert the Conv2D layers of the encoder.\n","x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","# The output ends up with shape (28, 28, 1).\n","decoder_outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n","decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"]},{"cell_type":"markdown","metadata":{},"source":["Its summary looks like this:"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-21T05:55:03.565093Z","iopub.status.busy":"2022-05-21T05:55:03.564832Z","iopub.status.idle":"2022-05-21T05:55:03.574566Z","shell.execute_reply":"2022-05-21T05:55:03.573795Z","shell.execute_reply.started":"2022-05-21T05:55:03.565047Z"},"trusted":true},"outputs":[],"source":["decoder.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Now let’s create the VAE model itself. This is your first example of a model that isn’t doing supervised learning (an autoencoder is an example of **self-supervised** learning, because it uses its inputs as targets). Whenever you depart from classic supervised learning, it’s common to subclass the **Model** class and implement a custom **train_step()** to specify the new training logic, a workflow you learned about in chapter 7. <br>\n","That’s what we’ll do here.\n","\n","##### VAE model with custom train_step()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-21T05:55:03.576172Z","iopub.status.busy":"2022-05-21T05:55:03.575926Z","iopub.status.idle":"2022-05-21T05:55:03.590155Z","shell.execute_reply":"2022-05-21T05:55:03.589347Z","shell.execute_reply.started":"2022-05-21T05:55:03.576139Z"},"trusted":true},"outputs":[],"source":["class VAE(keras.Model):\n","    def __init__(self, encoder, decoder, **kwargs):\n","        super().__init__(**kwargs)\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.sampler = Sampler()\n","        # We use these metrics to keep track of the loss averages over each epoch.\n","        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n","        self.reconstruction_loss_tracker = keras.metrics.Mean(\n","            name=\"reconstruction_loss\")\n","        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n","\n","    @property\n","    def metrics(self):\n","        # We list the metrics in the metrics property to enable the model to reset them after each epoch (or between multiple calls to fit()/evaluate()).\n","        return [self.total_loss_tracker,\n","                self.reconstruction_loss_tracker,\n","                self.kl_loss_tracker]\n","\n","    def train_step(self, data):\n","        with tf.GradientTape() as tape:\n","            z_mean, z_log_var = self.encoder(data)\n","            z = self.sampler(z_mean, z_log_var)\n","            reconstruction = decoder(z)\n","            reconstruction_loss = tf.reduce_mean(\n","                # We sum the reconstruction loss over the spatial dimensions (axes 1 and 2) and take its mean over the batch dimension.\n","                tf.reduce_sum(\n","                    keras.losses.binary_crossentropy(data, reconstruction),\n","                    axis=(1, 2)\n","                )\n","            )\n","            # Add the regularization term (Kullback–Leibler divergence).\n","            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n","            total_loss = reconstruction_loss + tf.reduce_mean(kl_loss)\n","        grads = tape.gradient(total_loss, self.trainable_weights)\n","        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n","        self.total_loss_tracker.update_state(total_loss)\n","        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n","        self.kl_loss_tracker.update_state(kl_loss)\n","        return {\n","            \"total_loss\": self.total_loss_tracker.result(),\n","            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n","            \"kl_loss\": self.kl_loss_tracker.result(),\n","        }"]},{"cell_type":"markdown","metadata":{},"source":["Finally, we’re ready to instantiate and train the model on MNIST digits. Because the loss is taken care of in the custom layer, we don’t specify an external loss at compile time (loss=None), which in turn means we won’t pass target data during training (as you can see, we only pass x_train to the model in fit()).\n","\n","##### Training the VAE"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-21T05:55:03.591999Z","iopub.status.busy":"2022-05-21T05:55:03.591694Z","iopub.status.idle":"2022-05-21T06:01:49.259979Z","shell.execute_reply":"2022-05-21T06:01:49.258807Z","shell.execute_reply.started":"2022-05-21T05:55:03.591963Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n","# We train on all MNIST digits, so we concatenate the training and test samples.\n","mnist_digits = np.concatenate([x_train, x_test], axis=0)\n","mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n","\n","vae = VAE(encoder, decoder)\n","# Note that we don’t pass a loss argument in compile(), since the loss is already part of the train_step().\n","vae.compile(optimizer=keras.optimizers.Adam(), run_eagerly=True)\n","# Note that we don’t pass targets in fit(), since train_step() doesn’t expect any.\n","vae.fit(mnist_digits, epochs=30, batch_size=128)"]},{"cell_type":"markdown","metadata":{},"source":["Once the model is trained, we can use the decoder network to turn arbitrary latent space vectors into images.\n","\n","##### Sampling a grid of images from the 2D latent space"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-21T06:01:49.261869Z","iopub.status.busy":"2022-05-21T06:01:49.261598Z","iopub.status.idle":"2022-05-21T06:02:21.767582Z","shell.execute_reply":"2022-05-21T06:02:21.766923Z","shell.execute_reply.started":"2022-05-21T06:01:49.261834Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","n = 30 # We’ll display a grid of 30 × 30 digits (900 digits total).\n","digit_size = 28\n","figure = np.zeros((digit_size * n, digit_size * n))\n","\n","# Sample points linearly on a 2D grid.\n","grid_x = np.linspace(-1, 1, n)\n","grid_y = np.linspace(-1, 1, n)[::-1]\n","\n","# Iterate over grid locations.\n","for i, yi in enumerate(grid_y):\n","    for j, xi in enumerate(grid_x):\n","        # For each location, sample a digit and add it to our figure.\n","        z_sample = np.array([[xi, yi]])\n","        x_decoded = vae.decoder.predict(z_sample)\n","        digit = x_decoded[0].reshape(digit_size, digit_size)\n","        figure[\n","            i * digit_size : (i + 1) * digit_size,\n","            j * digit_size : (j + 1) * digit_size,\n","        ] = digit\n","\n","plt.figure(figsize=(15, 15))\n","start_range = digit_size // 2\n","end_range = n * digit_size + start_range\n","pixel_range = np.arange(start_range, end_range, digit_size)\n","sample_range_x = np.round(grid_x, 1)\n","sample_range_y = np.round(grid_y, 1)\n","plt.xticks(pixel_range, sample_range_x)\n","plt.yticks(pixel_range, sample_range_y)\n","plt.xlabel(\"z[0]\")\n","plt.ylabel(\"z[1]\")\n","plt.axis(\"off\")\n","plt.imshow(figure, cmap=\"Greys_r\")"]},{"cell_type":"markdown","metadata":{},"source":["The grid of sampled digits (see figure 12.18) shows a completely continuous distribution of the different digit classes, with one digit morphing into another as you follow a path through latent space. Specific directions in this space have a meaning: for example, there are directions for “five-ness,” “one-ness,” and so on.\n","\n","![](./images/12.18.png)\n","\n","In the next section, we’ll cover in detail the other major tool for generating artificial images: generative adversarial networks (GANs)."]},{"cell_type":"markdown","metadata":{},"source":["#### Wrapping up\n","- Image generation with deep learning is done by learning latent spaces that capture statistical information about a dataset of images. By sampling and decoding points from the latent space, you can generate never-before-seen images. There are two major tools to do this: VAEs and GANs.\n","- VAEs result in highly structured, continuous latent representations. For this reason, they work well for doing all sorts of image editing in latent space: face swapping, turning a frowning face into a smiling face, and so on. They also work nicely for doing latent-space-based animations, such as animating a walk along a cross section of the latent space or showing a starting image slowly morphing into different images in a continuous way.\n","- GANs enable the generation of realistic single-frame images but may not induce latent spaces with solid structure and high continuity.\n","\n","Most successful practical applications I have seen with images rely on VAEs, but GANs have enjoyed enduring popularity in the world of academic research. You’ll find out how they work and how to implement one in the next section."]}],"metadata":{"interpreter":{"hash":"f8ee13a16f7ff347d089854b949fd5a4fdba136de942caaffeaf6bff99e7e7f9"},"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
