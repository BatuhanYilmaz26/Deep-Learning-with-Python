{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Generative deep learning\n","#### Text generation\n","In this section, we’ll explore how recurrent neural networks can be used to generate sequence data. We’ll use text generation as an example, but the exact same techniques can be generalized to any kind of sequence data: you could apply it to sequences of musical notes in order to generate new music, to timeseries of brush stroke data (perhaps recorded while an artist paints on an iPad) to generate paintings stroke by stroke, and so on. <br>\n","Sequence data generation is in no way limited to artistic content generation. It has been successfully applied to speech synthesis and to dialogue generation for chatbots. The Smart Reply feature that Google released in 2016, capable of automatically generating a selection of quick replies to emails or text messages, is powered by similar techniques.\n","\n","##### How do you generate sequence data?\n","The universal way to generate sequence data in deep learning is to train a model (usually a Transformer or an RNN) to predict the next token or next few tokens in a sequence, using the previous tokens as input. For instance, given the input “the cat is on the,” the model is trained to predict the target “mat,” the next word. As usual when working with text data, tokens are typically words or characters, and any network that can model the probability of the next token given the previous ones is called a language model. A language model captures the latent space of language: its statistical structure. <br>\n","Once you have such a trained language model, you can sample from it (generate new sequences): you feed it an initial string of text (called conditioning data), ask it to generate the next character or the next word (you can even generate several tokens at once), add the generated output back to the input data, and repeat the process many times (see figure 12.1). This loop allows you to generate sequences of arbitrary length that reflect the structure of the data on which the model was trained: sequences that look almost like human-written sentences.\n","\n","![](./images/12.1.png)\n","\n","##### The importance of the sampling strategy\n","When generating text, the way you choose the next token is crucially important. A naive approach is **greedy sampling**, consisting of always choosing the most likely next character. But such an approach results in repetitive, predictable strings that don’t look like coherent language. A more interesting approach makes slightly more surprising choices: it introduces randomness in the sampling process by sampling from the probability distribution for the next character. This is called **stochastic sampling** (recall that **stochasticity** is what we call **randomness** in this field). In such a setup, if a word has probability 0.3 of being next in the sentence according to the model, you’ll choose it 30% of the time. Note that greedy sampling can also be cast as sampling from a probability distribution: one where a certain word has probability 1 and all others have probability 0. <br>\n","Sampling probabilistically from the **softmax** output of the model is neat: it allows even unlikely words to be sampled some of the time, generating more interesting looking sentences and sometimes showing creativity by coming up with new, realistic sounding sentences that didn’t occur in the training data. But there’s one issue with this strategy: it doesn’t offer a way to control the amount of randomness in the sampling process. <br>\n","\n","Why would you want more or less randomness? Consider an extreme case: pure random sampling, where you draw the next word from a uniform probability distribution, and every word is equally likely. This scheme has maximum randomness; in other words, this probability distribution has maximum entropy. Naturally, it won’t produce anything interesting. At the other extreme, greedy sampling doesn’t produce anything interesting, either, and has no randomness: the corresponding probability distribution has minimum entropy. Sampling from the “real” probability distribution— the distribution that is output by the model’s softmax function—constitutes an intermediate point between these two extremes. But there are many other intermediate points of higher or lower entropy that you may want to explore. Less entropy will give the generated sequences a more predictable structure (and thus they will potentially be more realistic looking), whereas more entropy will result in more surprising and creative sequences. When sampling from generative models, it’s always good to explore different amounts of randomness in the generation process. Because we— humans—are the ultimate judges of how interesting the generated data is, interestingness is highly subjective, and there’s no telling in advance where the point of optimal entropy lies. <br>\n","\n","In order to control the amount of stochasticity in the sampling process, we’ll introduce a parameter called the **softmax temperature**, which characterizes the entropy of the probability distribution used for sampling: it characterizes how surprising or predictable the choice of the next word will be. Given a **temperature** value, a new probability distribution is computed from the original one (the softmax output of the model) by reweighting it in the following way.\n","\n","##### Reweighting a probability distribution to a different temperature"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-05-17T20:23:12.437809Z","iopub.status.busy":"2022-05-17T20:23:12.437542Z","iopub.status.idle":"2022-05-17T20:23:12.442453Z","shell.execute_reply":"2022-05-17T20:23:12.441730Z","shell.execute_reply.started":"2022-05-17T20:23:12.437780Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","# original_distribution is a 1D NumPy array of probability values that must sum to 1. \n","# temperature is a factor quantifying the entropy of the output distribution.\n","def reweight_distribution(original_distribution, temperature=0.5):\n","    distribution = np.log(original_distribution) / temperature\n","    distribution = np.exp(distribution)\n","    # Returns a reweighted version of the original distribution. \n","    # The sum of the distribution may no longer be 1, so you divide it by its sum to obtain the new distribution.\n","    return distribution / np.sum(distribution)"]},{"cell_type":"markdown","metadata":{},"source":["Higher temperatures result in sampling distributions of higher entropy that will generate more surprising and unstructured generated data, whereas a lower temperature will result in less randomness and much more predictable generated data (see figure 12.2).\n","\n","![](./images/12.2.png)"]},{"cell_type":"markdown","metadata":{},"source":["##### Implementing text generation with Keras\n","\n","Let’s put these ideas into practice in a Keras implementation. The first thing you need is a lot of text data that you can use to learn a language model. You can use any sufficiently large text file or set of text files—Wikipedia, The Lord of the Rings, and so on. <br>\n","In this example, we’ll keep working with the IMDB movie review dataset from the last chapter, and we’ll learn to generate never-read-before movie reviews. As such, our language model will be a model of the style and topics of these movie reviews specifically, rather than a general model of the English language.\n","\n","##### PREPARING THE DATA\n","Just like in the previous chapter, let’s download and uncompress the IMDB movie\n","reviews dataset.\n","\n","##### Downloading and uncompressing the IMDB movie reviews dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-17T20:23:12.448082Z","iopub.status.busy":"2022-05-17T20:23:12.447340Z","iopub.status.idle":"2022-05-17T20:23:27.229226Z","shell.execute_reply":"2022-05-17T20:23:27.228253Z","shell.execute_reply.started":"2022-05-17T20:23:12.448037Z"},"trusted":true},"outputs":[],"source":["!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz"]},{"cell_type":"markdown","metadata":{},"source":["You’re already familiar with the structure of the data: we get a folder named aclImdb containing two subfolders, one for negative-sentiment movie reviews, and one for positive-sentiment reviews. There’s one text file per review. We’ll call **text_dataset_from_directory** with **label_mode=None** to create a dataset that reads from these files and yields the text content of each file.\n","\n","##### Creating a dataset from text files (one file = one sample)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-17T20:23:27.231774Z","iopub.status.busy":"2022-05-17T20:23:27.231486Z","iopub.status.idle":"2022-05-17T20:23:40.080894Z","shell.execute_reply":"2022-05-17T20:23:40.080176Z","shell.execute_reply.started":"2022-05-17T20:23:27.231736Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","dataset = keras.utils.text_dataset_from_directory(\n","    directory=\"aclImdb\", label_mode=None, batch_size=256)\n","# Strip the <br /> HTML tag that occurs in many of the reviews. \n","# This did not matter much for text classification, but we wouldn’t want to generate <br /> tags in this example!\n","dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"]},{"cell_type":"markdown","metadata":{},"source":["Now let’s use a **TextVectorization** layer to compute the vocabulary we’ll be working with. We’ll only use the first **sequence_length** words of each review: our **TextVectorization** layer will cut off anything beyond that when vectorizing a text.\n","\n","##### Preparing a TextVectorization layer"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-17T20:23:40.082624Z","iopub.status.busy":"2022-05-17T20:23:40.082399Z","iopub.status.idle":"2022-05-17T20:23:51.284478Z","shell.execute_reply":"2022-05-17T20:23:51.283677Z","shell.execute_reply.started":"2022-05-17T20:23:40.082592Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import TextVectorization\n","\n","sequence_length = 100\n","# We’ll only consider the top 15,000 most common words—anything else will be treated as the out-of-vocabulary token, \"[UNK]\".\n","vocab_size = 15000\n","text_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\", # We want to return integer word index sequences.\n","    output_sequence_length=sequence_length, # We’ll work with inputs and targets of length 100 (but since we’ll offset the targets by 1, the model will actually see sequences of length 99).\n",")\n","text_vectorization.adapt(dataset)"]},{"cell_type":"markdown","metadata":{},"source":["Let’s use the layer to create a language modeling dataset where input samples are vectorized texts, and corresponding targets are the same texts offset by one word.\n","\n","##### Setting up a language modeling dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-17T20:23:51.286843Z","iopub.status.busy":"2022-05-17T20:23:51.286569Z","iopub.status.idle":"2022-05-17T20:23:51.365429Z","shell.execute_reply":"2022-05-17T20:23:51.364648Z","shell.execute_reply.started":"2022-05-17T20:23:51.286807Z"},"trusted":true},"outputs":[],"source":["def prepare_lm_dataset(text_batch):\n","    vectorized_sequences = text_vectorization(text_batch) # Convert a batch of texts (strings) to a batch of integer sequences.\n","    x = vectorized_sequences[:, :-1] # Create inputs by cutting off the last word of the sequences.\n","    y = vectorized_sequences[:, 1:] # Create targets by offsetting the sequences by 1.\n","    return x, y\n","\n","lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"]},{"cell_type":"markdown","metadata":{},"source":["##### A TRANSFORMER-BASED SEQUENCE-TO-SEQUENCE MODEL\n","\n","We’ll train a model to predict a probability distribution over the next word in a sentence, given a number of initial words. When the model is trained, we’ll feed it with a prompt, sample the next word, add that word back to the prompt, and repeat, until we’ve generated a short paragraph. <br>\n","Like we did for temperature forecasting in chapter 10, we could train a model that takes as input a sequence of N words and simply predicts word N+1. However, there are several issues with this setup in the context of sequence generation. <br>\n","First, the model would only learn to produce predictions when N words were available, but it would be useful to be able to start predicting with fewer than N words. Otherwise we’d be constrained to only use relatively long prompts (in our implementation, N=100 words). We didn’t have this need in chapter 10. <br>\n","Second, many of our training sequences will be mostly overlapping. Consider N = 4. The text “A complete sentence must have, at minimum, three things: a subject, verb, and an object” would be used to generate the following training sequences:\n","- “A complete sentence must”\n","- “complete sentence must have”\n","- “sentence must have at”\n","- and so on, until “verb and an object”\n","\n","A model that treats each such sequence as an independent sample would have to do a lot of redundant work, re-encoding multiple times subsequences that it has largely seen before. In chapter 10, this wasn’t much of a problem, because we didn’t have that many training samples in the first place, and we needed to benchmark dense and convolutional models, for which redoing the work every time is the only option. We could try to alleviate this redundancy problem by using strides to sample our sequences— skipping a few words between two consecutive samples. But that would reduce our number of training samples while only providing a partial solution. <br>\n","To address these two issues, we’ll use a **sequence-to-sequence model**: we’ll feed sequences of N words (indexed from 0 to N) into our model, and we’ll predict the sequence offset by one (from 1 to N+1). We’ll use **causal masking** to make sure that, for any i, the model will only be using words from 0 to i in order to predict the word i + 1. This means that we’re simultaneously training the model to solve N mostly overlapping but different problems: predicting the next words given a sequence of 1 <= i <= N prior words (see figure 12.3). At generation time, even if you only prompt the model with a single word, it will be able to give you a probability distribution for the next possible words.\n","\n","![](./images/12.3.png)\n","\n","Note that we could have used a similar sequence-to-sequence setup on our temperature forecasting problem in chapter 10: given a sequence of 120 hourly data points, learn to generate a sequence of 120 temperatures offset by 24 hours in the future. You’d be not only solving the initial problem, but also solving the 119 related problems of forecasting temperature in 24 hours, given 1 <= i < 120 prior hourly data points. If you try to retrain the RNNs from chapter 10 in a sequence-to-sequence setup, you’ll find that you get similar but incrementally worse results, because the constraint of solving these additional 119 related problems with the same model interferes slightly with the task we actually do care about. <br>\n","In the previous chapter, you learned about the setup you can use for sequence-tosequence learning in the general case: feed the source sequence into an encoder, and then feed both the encoded sequence and the target sequence into a decoder that tries to predict the same target sequence offset by one step. When you’re doing text generation, there is no source sequence: you’re just trying to predict the next tokens in the target sequence given past tokens, which we can do using only the decoder. And thanks to causal padding, the decoder will only look at words 0…N to predict the word N+1.\n","\n","Let’s implement our model—we’re going to reuse the building blocks we created in chapter 11: **PositionalEmbedding** and **TransformerDecoder**."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-17T20:23:51.367392Z","iopub.status.busy":"2022-05-17T20:23:51.366971Z","iopub.status.idle":"2022-05-17T20:23:51.388772Z","shell.execute_reply":"2022-05-17T20:23:51.387808Z","shell.execute_reply.started":"2022-05-17T20:23:51.367354Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=input_dim, output_dim=output_dim)\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super(PositionalEmbedding, self).get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","          num_heads=num_heads, key_dim=embed_dim)\n","        self.attention_2 = layers.MultiHeadAttention(\n","          num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def get_config(self):\n","        config = super(TransformerDecoder, self).get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1),\n","             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n","        return tf.tile(mask, mult)\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(\n","                mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=causal_mask)\n","        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","        attention_output_2 = self.attention_2(\n","            query=attention_output_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        attention_output_2 = self.layernorm_2(\n","            attention_output_1 + attention_output_2)\n","        proj_output = self.dense_proj(attention_output_2)\n","        return self.layernorm_3(attention_output_2 + proj_output)"]},{"cell_type":"markdown","metadata":{},"source":["##### A simple Transformer-based language model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-17T20:23:51.390515Z","iopub.status.busy":"2022-05-17T20:23:51.390224Z","iopub.status.idle":"2022-05-17T20:23:51.901488Z","shell.execute_reply":"2022-05-17T20:23:51.900746Z","shell.execute_reply.started":"2022-05-17T20:23:51.390466Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import layers\n","embed_dim = 256\n","latent_dim = 2048\n","num_heads = 2\n","\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n","outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) # Softmax over possible vocabulary words, computed for each output sequence timestep.\n","model = keras.Model(inputs, outputs)\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"]},{"cell_type":"markdown","metadata":{},"source":["##### text-generation callback with variable-temperature sampling\n","We’ll use a callback to generate text using a range of different temperatures after every epoch. This allows you to see how the generated text evolves as the model begins to converge, as well as the impact of temperature in the sampling strategy. To seed text generation, we’ll use the prompt “this movie”: all of our generated texts will start with this.\n","\n","##### The text-generation callback"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-17T20:23:51.904091Z","iopub.status.busy":"2022-05-17T20:23:51.903895Z","iopub.status.idle":"2022-05-17T20:23:51.952782Z","shell.execute_reply":"2022-05-17T20:23:51.952052Z","shell.execute_reply.started":"2022-05-17T20:23:51.904067Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","tokens_index = dict(enumerate(text_vectorization.get_vocabulary())) # Dict that maps word indices back to strings, to be used for text decoding\n","\n","def sample_next(predictions, temperature=1.0): # Implements variable temperature sampling from a probability distribution\n","    predictions = np.asarray(predictions).astype(\"float64\")\n","    predictions = np.log(predictions) / temperature\n","    exp_preds = np.exp(predictions)\n","    predictions = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, predictions, 1)\n","    return np.argmax(probas)\n","\n","class TextGenerator(keras.callbacks.Callback):\n","    def __init__(self,\n","                 prompt, # Prompt that we use to seed text generation\n","                 generate_length, # Number of words to generate\n","                 model_input_length,\n","                 temperatures=(1.,), # Range of temperatures to use for sampling\n","                 print_freq=1):\n","        self.prompt = prompt\n","        self.generate_length = generate_length\n","        self.model_input_length = model_input_length\n","        self.temperatures = temperatures\n","        self.print_freq = print_freq\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if (epoch + 1) % self.print_freq != 0:\n","            return\n","        for temperature in self.temperatures:\n","            print(\"== Generating with temperature\", temperature)\n","            sentence = self.prompt # When generating text, we start from our prompt\n","            for i in range(self.generate_length):\n","                # Feed the current sequence into our model.\n","                tokenized_sentence = text_vectorization([sentence])\n","                predictions = self.model(tokenized_sentence)\n","                # Retrieve the predictions for the last timestep, and use them to sample a new word.\n","                next_token = sample_next(predictions[0, i, :])\n","                sampled_token = tokens_index[next_token]\n","                # Append the new word to the current sequence and repeat.\n","                sentence += \" \" + sampled_token\n","            print(sentence)\n","\n","prompt = \"This movie\"\n","text_gen_callback = TextGenerator(\n","    prompt,\n","    generate_length=50,\n","    model_input_length=sequence_length,\n","    temperatures=(0.2, 0.5, 0.7, 1., 1.5)) # We’ll use a diverse range of temperatures to sample text, to demonstrate the effect of temperature on text generation."]},{"cell_type":"markdown","metadata":{},"source":["Let’s fit() this thing.\n","\n","##### Fitting the language model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-05-17T20:39:24.033020Z","iopub.status.busy":"2022-05-17T20:39:24.032755Z","iopub.status.idle":"2022-05-17T21:16:58.524487Z","shell.execute_reply":"2022-05-17T21:16:58.523798Z","shell.execute_reply.started":"2022-05-17T20:39:24.032990Z"},"trusted":true},"outputs":[],"source":["model.fit(lm_dataset, epochs=20, callbacks=[text_gen_callback])"]},{"cell_type":"markdown","metadata":{},"source":["#### Wrapping up\n","- You can generate discrete sequence data by training a model to predict the next token(s), given previous tokens.\n","- In the case of text, such a model is called a **language model**. It can be based on either words or characters.\n","- Sampling the next token requires a balance between adhering to what the model judges likely, and introducing randomness.\n","- One way to handle this is the notion of **softmax temperature**. Always experiment with different temperatures to find the right one."]}],"metadata":{"interpreter":{"hash":"f8ee13a16f7ff347d089854b949fd5a4fdba136de942caaffeaf6bff99e7e7f9"},"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
