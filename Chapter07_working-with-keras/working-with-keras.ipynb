{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different ways to build Keras models\n",
    "There are three APIs for building models in Keras (see figure 7.1):\n",
    "- The Sequential model, the most approachable API—it’s basically a Python list. As such, it’s limited to simple stacks of layers.\n",
    "- The Functional API, which focuses on graph-like model architectures. It represents a nice mid-point between usability and flexibility, and as such, it’s the most commonly used model-building API.\n",
    "- Model subclassing, a low-level option where you write everything yourself from scratch. This is ideal if you want full control over every little thing. However, you won’t get access to many built-in Keras features, and you will be more at risk of making mistakes.\n",
    "\n",
    "![](./images/7.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Sequential model\n",
    "The simplest way to build a Keras model is to use the Sequential model, which you already know about.\n",
    "\n",
    "##### The Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it’s possible to build the same model incrementally via the **add()** method, which is similar to the append() method of a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You saw in chapter 4 that layers only get built (which is to say, create their weights) when they are called for the first time. That’s because the shape of the layers' weights depends on the shape of their input: until the input shape is known, they can’t be created. <br>\n",
    "As such, the preceding Sequential model does not have any weights until you actually call it on some data, or call its **build()** method with an input shape.\n",
    "\n",
    "##### Models that aren’t yet built have no weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential_1 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\batuh\\Deep-Learning-with-Python\\Chapter07_working-with-keras\\working-with-keras.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/batuh/Deep-Learning-with-Python/Chapter07_working-with-keras/working-with-keras.ipynb#ch0000006?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mweights\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:2542\u001b[0m, in \u001b[0;36mModel.weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2531'>2532</a>\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2532'>2533</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2533'>2534</a>\u001b[0m   \u001b[39m\"\"\"Returns the list of all layer variables/weights.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2534'>2535</a>\u001b[0m \n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2535'>2536</a>\u001b[0m \u001b[39m  Note: This will not track the weights of nested `tf.Modules` that are not\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2539'>2540</a>\u001b[0m \u001b[39m    A list of variables.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2540'>2541</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2541'>2542</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dedup_weights(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_undeduplicated_weights)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:2547\u001b[0m, in \u001b[0;36mModel._undeduplicated_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2543'>2544</a>\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2544'>2545</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_undeduplicated_weights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2545'>2546</a>\u001b[0m   \u001b[39m\"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2546'>2547</a>\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_assert_weights_created()\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2547'>2548</a>\u001b[0m   weights \u001b[39m=\u001b[39m []\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2548'>2549</a>\u001b[0m   \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_tracked_trackables:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\sequential.py:471\u001b[0m, in \u001b[0;36mSequential._assert_weights_created\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/sequential.py?line=467'>468</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/sequential.py?line=468'>469</a>\u001b[0m \u001b[39m# When the graph has not been initialized, use the Model's implementation to\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/sequential.py?line=469'>470</a>\u001b[0m \u001b[39m# to check if the weights has been created.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/sequential.py?line=470'>471</a>\u001b[0m \u001b[39msuper\u001b[39;49m(functional\u001b[39m.\u001b[39;49mFunctional, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_assert_weights_created()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:2736\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2727'>2728</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2729'>2730</a>\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mbuild\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2730'>2731</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39m!=\u001b[39m Model \u001b[39mand\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2731'>2732</a>\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt):\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2732'>2733</a>\u001b[0m   \u001b[39m# For any model that has customized build() method but hasn't\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2733'>2734</a>\u001b[0m   \u001b[39m# been invoked yet, this will cover both sequential and subclass model.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2734'>2735</a>\u001b[0m   \u001b[39m# Also make sure to exclude Model class itself which has build() defined.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2735'>2736</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWeights for model \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m have not yet been \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2736'>2737</a>\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mcreated. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2737'>2738</a>\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mWeights are created when the Model is first called on \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/batuh/AppData/Local/Programs/Python/Python39/lib/site-packages/keras/engine/training.py?line=2738'>2739</a>\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Weights for model sequential_1 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "model.weights # At that point, the model isn’t built yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calling a model for the first time to build it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_2/kernel:0' shape=(3, 64) dtype=float32, numpy=\n",
       " array([[ 0.2724895 , -0.17667829,  0.08634672,  0.03094366,  0.22193986,\n",
       "          0.00050122, -0.04347321,  0.07433555,  0.24487841,  0.00639936,\n",
       "         -0.17905024,  0.03159165,  0.09238872,  0.09666827, -0.15783143,\n",
       "          0.27647716, -0.00293496, -0.00031772,  0.02567136, -0.17179483,\n",
       "          0.25215816,  0.25621974,  0.00085109, -0.20776993, -0.00292468,\n",
       "         -0.10972515, -0.03282073, -0.11589427, -0.00592282, -0.258493  ,\n",
       "         -0.23956133,  0.153824  ,  0.10854927,  0.18189877,  0.25752586,\n",
       "         -0.03285849,  0.03740495,  0.21729326,  0.09508508,  0.23891473,\n",
       "         -0.04067102,  0.11505315,  0.08846611, -0.20691112,  0.00076127,\n",
       "          0.03209037, -0.12196486,  0.079833  , -0.24508905, -0.0773312 ,\n",
       "          0.28201836,  0.04100344,  0.21173775, -0.07057916,  0.09090418,\n",
       "         -0.02585095, -0.24240218, -0.01259881,  0.07089615, -0.17753294,\n",
       "          0.18190491,  0.00064918, -0.15706758, -0.24461588],\n",
       "        [-0.14459442, -0.09314057,  0.00384849,  0.07634252,  0.12540829,\n",
       "         -0.12394077,  0.24829048,  0.02343756, -0.27074403,  0.13653779,\n",
       "          0.11554566,  0.20024511,  0.10681796,  0.20467252,  0.16980389,\n",
       "         -0.24133354,  0.03336349, -0.08226922,  0.28962737, -0.26840952,\n",
       "         -0.138797  , -0.13421564, -0.24460325, -0.17226002, -0.10894403,\n",
       "         -0.13386954, -0.13118759, -0.02179798, -0.15860398, -0.22506711,\n",
       "          0.10344094, -0.05578719,  0.0583241 , -0.01851687, -0.00532302,\n",
       "         -0.29487592,  0.01126489,  0.17474219, -0.1835937 , -0.29222536,\n",
       "          0.05429989, -0.2800233 , -0.20437464,  0.02624488,  0.17416978,\n",
       "          0.17987666, -0.04873627,  0.28471684,  0.02279136, -0.12071885,\n",
       "         -0.28662553, -0.18753114, -0.12370867,  0.1003617 , -0.20662737,\n",
       "          0.16730002,  0.24747223, -0.2822266 , -0.08441514, -0.15738073,\n",
       "         -0.25782135, -0.054456  ,  0.01017851,  0.27539802],\n",
       "        [-0.22951663, -0.17474812,  0.21468425,  0.01001391, -0.12105155,\n",
       "          0.23726743,  0.05976623, -0.06456564, -0.17110205,  0.26282024,\n",
       "          0.08508381, -0.10389405,  0.07730702, -0.21299541,  0.14612702,\n",
       "         -0.11914015, -0.20393315,  0.09786478, -0.03035441,  0.2907132 ,\n",
       "         -0.10428669, -0.10463029,  0.19630724,  0.08735716, -0.04260781,\n",
       "          0.23763126, -0.19858572,  0.27003872,  0.08552489,  0.08013144,\n",
       "          0.10593045,  0.17590365,  0.0474377 ,  0.18668562, -0.20901129,\n",
       "          0.13874534,  0.14477849, -0.00671557,  0.01553461,  0.16489667,\n",
       "          0.04806212,  0.02098957,  0.25173455, -0.2630584 , -0.22335327,\n",
       "         -0.13291569, -0.15686652,  0.20620322, -0.20847861,  0.2944879 ,\n",
       "         -0.21392122, -0.2068158 , -0.03982443,  0.11405143,  0.06460872,\n",
       "          0.02965164,  0.05618623, -0.1494765 , -0.23689452, -0.00899491,\n",
       "          0.15266424, -0.15614727, -0.1523237 ,  0.1109561 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/kernel:0' shape=(64, 10) dtype=float32, numpy=\n",
       " array([[ 0.25512466,  0.26146945, -0.07784791, -0.26962695,  0.13933524,\n",
       "          0.20612994,  0.03281868,  0.15757522, -0.20374289,  0.2761307 ],\n",
       "        [ 0.08198753, -0.14537425, -0.20318127, -0.1675052 , -0.22403657,\n",
       "          0.2745917 , -0.06501722, -0.2042858 ,  0.09933883, -0.26212403],\n",
       "        [ 0.10307679,  0.28218463, -0.17474063,  0.1354222 , -0.06213574,\n",
       "          0.17867011,  0.10079566, -0.1797023 ,  0.20369619,  0.2219483 ],\n",
       "        [-0.28039414,  0.1859912 ,  0.03106359,  0.26494482, -0.10558441,\n",
       "         -0.02563581, -0.12089658, -0.28054804, -0.25685537,  0.24305114],\n",
       "        [-0.16287783,  0.19781083, -0.22615248, -0.10090359, -0.11246103,\n",
       "          0.02853832,  0.16791987,  0.18894914,  0.1081585 , -0.10345975],\n",
       "        [ 0.26356313, -0.01516846,  0.27223912, -0.01480219,  0.1071946 ,\n",
       "          0.12355986,  0.23600301,  0.11918896, -0.06379651,  0.04806656],\n",
       "        [-0.22310433, -0.09151562, -0.08862531,  0.26039872,  0.2325466 ,\n",
       "          0.14399889, -0.08051834,  0.15285745,  0.26578215,  0.2264615 ],\n",
       "        [-0.25610134,  0.20590511, -0.12376156,  0.02971008,  0.02225098,\n",
       "          0.17257816,  0.02386829, -0.18893759,  0.07089439,  0.03545052],\n",
       "        [ 0.09282541, -0.20321418,  0.2571554 , -0.25915036,  0.06949085,\n",
       "          0.04282701, -0.20974553,  0.09157562, -0.20957226, -0.05278355],\n",
       "        [-0.16586909, -0.04259707, -0.22325361, -0.23893124,  0.03779221,\n",
       "          0.17104813, -0.16425645,  0.05873564, -0.16727683, -0.25637615],\n",
       "        [-0.12611358, -0.07356086, -0.2242617 ,  0.10174569, -0.10364386,\n",
       "          0.1865918 , -0.21917164,  0.12166175, -0.25588965, -0.26041406],\n",
       "        [-0.23075359, -0.03955503, -0.21284825, -0.02373871, -0.19605698,\n",
       "          0.26241252,  0.07977462,  0.2296246 , -0.26513615,  0.28161237],\n",
       "        [ 0.15399426,  0.27164784,  0.20714197,  0.28015223,  0.14834332,\n",
       "         -0.00645015,  0.1324678 ,  0.11556029,  0.2625269 , -0.26804343],\n",
       "        [-0.18615183, -0.095928  , -0.06169304, -0.23692524,  0.01492026,\n",
       "         -0.19226864, -0.19788054,  0.12018564, -0.06061916,  0.24687073],\n",
       "        [ 0.24330857, -0.05569191,  0.11218247, -0.16529882, -0.20382565,\n",
       "         -0.21159108,  0.19696268,  0.07361665,  0.00083661, -0.07771608],\n",
       "        [-0.06967211, -0.27514747, -0.14642219, -0.02112696, -0.03307304,\n",
       "         -0.08707356,  0.19046876, -0.08477207,  0.1687471 ,  0.22743914],\n",
       "        [ 0.13280216,  0.11531216,  0.19206402,  0.06880897,  0.04017574,\n",
       "         -0.08108066,  0.28166983,  0.24186549,  0.06789193, -0.09703554],\n",
       "        [-0.00979781,  0.13143569,  0.06111088, -0.24352589, -0.00280645,\n",
       "         -0.14020158, -0.15202983,  0.17163736,  0.1838772 , -0.20213963],\n",
       "        [-0.07333852, -0.03513625, -0.08629625, -0.23205346,  0.17420682,\n",
       "          0.14644697,  0.02817884, -0.24833176,  0.2024987 , -0.03697728],\n",
       "        [-0.17611042,  0.01658204,  0.20769432, -0.22810449,  0.02005762,\n",
       "          0.1655041 , -0.08117931,  0.10779643, -0.10105804,  0.1276848 ],\n",
       "        [ 0.01590106,  0.14307514, -0.0567795 ,  0.19016734,  0.09416628,\n",
       "         -0.14499368,  0.05087063,  0.06089383,  0.13200343,  0.12789914],\n",
       "        [ 0.07280925, -0.17337415,  0.2663152 ,  0.1312128 , -0.2634457 ,\n",
       "         -0.20977736,  0.09663406, -0.03863424,  0.1964694 ,  0.1786272 ],\n",
       "        [ 0.21411031, -0.23058333, -0.14548796, -0.11116074, -0.03442308,\n",
       "         -0.05893844,  0.03405139,  0.01623574,  0.05933544, -0.2660183 ],\n",
       "        [ 0.17066485,  0.0003489 , -0.07178013, -0.02205145,  0.0511435 ,\n",
       "          0.17105749,  0.16918221, -0.0346455 ,  0.00211856,  0.01400906],\n",
       "        [-0.20980051,  0.26542005,  0.24884495,  0.1041736 ,  0.25983933,\n",
       "          0.18367177,  0.19481298, -0.25370517,  0.0248127 ,  0.10117379],\n",
       "        [ 0.13545614, -0.18599662,  0.17354748,  0.05058327, -0.22916335,\n",
       "          0.08826834, -0.22710244,  0.12284833,  0.2400653 ,  0.06809214],\n",
       "        [-0.16157538,  0.04317936,  0.26006016,  0.24952838, -0.27176103,\n",
       "         -0.13012746, -0.03731617,  0.28080812, -0.27889997, -0.01084223],\n",
       "        [ 0.18459982,  0.00604829,  0.06980342, -0.2540407 , -0.17149784,\n",
       "         -0.09545855, -0.04511148,  0.15320283, -0.27920872,  0.14394337],\n",
       "        [-0.12682207,  0.10625795,  0.03554815,  0.28269747,  0.00581652,\n",
       "         -0.20184669,  0.24438551,  0.18430212,  0.01952034,  0.15076771],\n",
       "        [ 0.06863347, -0.11379308, -0.18217272, -0.17922905,  0.28245208,\n",
       "         -0.14092515, -0.05802982, -0.00512522, -0.06033376, -0.24911147],\n",
       "        [-0.27950528,  0.12635186, -0.23021068, -0.01824763, -0.04022963,\n",
       "         -0.17161766, -0.06476726,  0.05140066,  0.02522472, -0.15749802],\n",
       "        [-0.22206452, -0.20931394,  0.23265001, -0.19015077, -0.19423558,\n",
       "         -0.20715216, -0.23617561, -0.1884868 ,  0.2176362 , -0.01492894],\n",
       "        [-0.11140271, -0.04894584,  0.05239788,  0.13141885, -0.21080513,\n",
       "          0.1764318 ,  0.03479829, -0.19973439,  0.18089184,  0.09042376],\n",
       "        [ 0.17761302,  0.21497548, -0.1490182 ,  0.26295182,  0.24391165,\n",
       "          0.12007052,  0.07732871, -0.26162267,  0.10041058,  0.16757399],\n",
       "        [ 0.14695334, -0.21663035,  0.11788529, -0.0468588 , -0.21631894,\n",
       "         -0.18892843, -0.10261881,  0.18901646,  0.2806988 ,  0.05057368],\n",
       "        [ 0.02875015,  0.00926197, -0.08165643,  0.07900935, -0.17004386,\n",
       "         -0.24365468,  0.04434663, -0.14817251,  0.09329656, -0.2676439 ],\n",
       "        [ 0.07053709,  0.14874631, -0.25028038,  0.00375527,  0.22468385,\n",
       "         -0.15842728, -0.15583779, -0.21690904,  0.16988593, -0.10500416],\n",
       "        [-0.07013945, -0.02060318,  0.08617783,  0.27679828,  0.20692024,\n",
       "         -0.04643707, -0.07889728, -0.23850693,  0.21941188, -0.23568404],\n",
       "        [ 0.11970228,  0.05051395,  0.27358153,  0.27770934,  0.09906495,\n",
       "          0.04061234, -0.19890893, -0.11223081,  0.12630287, -0.14123505],\n",
       "        [-0.2557707 ,  0.04924199, -0.16826177, -0.14992812, -0.18001133,\n",
       "          0.15427288,  0.0182378 ,  0.19241989,  0.13463095,  0.13210854],\n",
       "        [-0.27345568, -0.00038528, -0.04771699,  0.18612713, -0.27180326,\n",
       "          0.28058746, -0.04412219, -0.00989965,  0.26702544, -0.06030653],\n",
       "        [ 0.20106867,  0.17467234,  0.23123518, -0.2182603 , -0.05314133,\n",
       "          0.2623385 ,  0.09700465,  0.2084893 , -0.17443946,  0.11121586],\n",
       "        [ 0.05889118, -0.08288942, -0.23179778,  0.1963284 , -0.01525086,\n",
       "          0.17749918,  0.17487526,  0.12011233,  0.26427516,  0.10258296],\n",
       "        [ 0.2729095 , -0.19671705,  0.06471118,  0.07468647, -0.22667013,\n",
       "          0.24653247, -0.02869257, -0.21605302,  0.2412639 ,  0.0677892 ],\n",
       "        [-0.18629712,  0.00695264,  0.23093858, -0.22294274, -0.27072132,\n",
       "          0.28368542, -0.22938596,  0.10086259,  0.24562207, -0.04890497],\n",
       "        [ 0.18039191, -0.26570818, -0.15541966, -0.26143584, -0.09817961,\n",
       "          0.20926112,  0.22865507, -0.05410101, -0.16507648, -0.11359212],\n",
       "        [-0.2728892 , -0.09887649, -0.25723466, -0.16956191,  0.03393522,\n",
       "          0.22203389, -0.02505207,  0.11035341, -0.15933646,  0.01399484],\n",
       "        [ 0.27128795,  0.15357432, -0.2543149 ,  0.2121898 , -0.11169653,\n",
       "          0.10913411,  0.12639505,  0.20095524, -0.2544409 , -0.26591307],\n",
       "        [-0.22905561,  0.26986673, -0.15951976, -0.11315364, -0.2723507 ,\n",
       "         -0.03533334, -0.1153605 , -0.06278197,  0.19891009, -0.25485733],\n",
       "        [-0.17550737,  0.22157672, -0.21838549, -0.24871595,  0.26460645,\n",
       "         -0.2615146 ,  0.08994386, -0.2107046 , -0.00324199, -0.16402468],\n",
       "        [-0.16471082,  0.2526935 , -0.06644331, -0.25322968, -0.25779006,\n",
       "          0.02290985, -0.22701745, -0.1169284 ,  0.2585412 ,  0.1643506 ],\n",
       "        [ 0.19371384, -0.25505435, -0.00168732,  0.25089762,  0.01741329,\n",
       "         -0.03664699,  0.20021173,  0.10807011,  0.06197682,  0.17017528],\n",
       "        [-0.1775845 ,  0.2689717 , -0.24297185,  0.08164161,  0.03652227,\n",
       "          0.0918825 ,  0.12649968, -0.22022128,  0.1891407 , -0.2683116 ],\n",
       "        [ 0.07614017, -0.23754114,  0.19001383, -0.19886255, -0.04450095,\n",
       "         -0.19265634, -0.0767623 , -0.20898977, -0.10505542, -0.22071198],\n",
       "        [-0.20945176,  0.14955989, -0.06872566, -0.04979405,  0.13202205,\n",
       "         -0.27567258,  0.1254654 ,  0.1307405 , -0.09922904,  0.22133407],\n",
       "        [-0.09735693, -0.19430469,  0.19338527,  0.23121229, -0.03370047,\n",
       "          0.25302818,  0.090671  , -0.15038943,  0.09580058, -0.253283  ],\n",
       "        [-0.12277099,  0.0198175 , -0.23844454, -0.03948021, -0.02997521,\n",
       "          0.21569762,  0.13222313,  0.15995777, -0.03326848, -0.22582057],\n",
       "        [-0.13946879, -0.19794728, -0.04529409, -0.25400126, -0.04576226,\n",
       "          0.11152974,  0.03925842,  0.08356425,  0.09372765, -0.08641633],\n",
       "        [-0.25801024, -0.08667018,  0.2593285 , -0.263407  ,  0.05079141,\n",
       "          0.05188245,  0.08998609, -0.25045812,  0.25541613,  0.21199596],\n",
       "        [ 0.04837579,  0.24941203, -0.0328812 ,  0.18045083,  0.26048264,\n",
       "         -0.21656077, -0.15694296,  0.28023735, -0.22257811,  0.13238356],\n",
       "        [-0.09602766,  0.14003411,  0.0556778 ,  0.12443945,  0.16957256,\n",
       "          0.08104992,  0.19880703,  0.13492137, -0.13016988,  0.13926932],\n",
       "        [-0.1430519 , -0.18887004, -0.27860466,  0.09624374, -0.22391662,\n",
       "         -0.05528194,  0.03800267,  0.13505349, -0.28230026, -0.06019872],\n",
       "        [ 0.18380767,  0.08777797,  0.06770521,  0.10864988, -0.27816603,\n",
       "          0.1038751 , -0.23008563,  0.20597774, -0.0884376 , -0.2401053 ],\n",
       "        [-0.04823531,  0.1885879 ,  0.0849272 , -0.21720904, -0.16840413,\n",
       "         -0.01881865,  0.06139004, -0.15686211,  0.13938347,  0.23220316]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.build(input_shape=(None, 3))\n",
    "model.weights # Now you can retrieve the model’s weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds the model—now the model will expect samples of shape (3,). **The None in the input shape signals that the batch size could be anything.**\n",
    "\n",
    "After the model is built, you can display its contents via the **summary()** method, which comes in handy for debugging.\n",
    "\n",
    "##### The summary() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 64)                256       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 906\n",
      "Trainable params: 906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this model happens to be named “sequential_1.” You can give names to everything in Keras—every model, every layer.\n",
    "\n",
    "##### Naming models and layers with the `name` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_example_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " my_first_layer (Dense)      (None, 64)                256       \n",
      "                                                                 \n",
      " my_last_layer (Dense)       (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 906\n",
      "Trainable params: 906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(name=\"my_example_model\")\n",
    "model.add(layers.Dense(64, activation=\"relu\", name=\"my_first_layer\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\", name=\"my_last_layer\"))\n",
    "model.build(input_shape=(None, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a Sequential model incrementally, it’s useful to be able to print a summary of what the current model looks like after you add each layer. But you can’t print a summary until the model is built! There’s actually a way to have your Sequential built on the fly: just declare the shape of the model’s inputs in advance. You can do this via the **Input** class.\n",
    "\n",
    "##### Specifying the input shape of your model in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(3,))) # Use Input to declare the shape of the inputs. \n",
    "# Note that the shape argument must be the shape of each sample, not the shape of one batch.\n",
    "model.add(layers.Dense(64, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use **summary()** to follow how the output shape of your model changes as you add more layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 64)                256       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256\n",
      "Trainable params: 256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 64)                256       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 906\n",
      "Trainable params: 906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty common debugging workflow when dealing with layers that transform their inputs in complex ways, such as the convolutional layers you’ll learn about in chapter 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Functional API\n",
    "The **Sequential** model is easy to use, but its applicability is extremely limited: it can only express models with a single input and a single output, applying one layer after the other in a sequential fashion. In practice, it’s pretty common to encounter models with multiple inputs (say, an image and its metadata), multiple outputs (different things you want to predict about the data), or a nonlinear topology. <br>\n",
    "In such cases, you’d build your model using the **Functional API**. This is what most Keras models you’ll encounter in the wild use. It’s fun and powerful—it feels like playing with LEGO bricks.\n",
    "\n",
    "##### A SIMPLE EXAMPLE\n",
    "Let’s start with something simple: the stack of two layers we used in the previous section. <br>\n",
    "Its Functional API version looks like the following listing.\n",
    "\n",
    "##### A simple Functional model with two Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(3,), name=\"my_input\")\n",
    "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go over this step by step.\n",
    "We started by declaring an **Input** (note that you can also give names to these input objects, like everything else):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(3,), name=\"my_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This inputs object holds information about the shape and dtype of the data that the model will process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will process batches where each sample has shape (3,). The number of samples per batch is variable (indicated by the None batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These batches will have dtype float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call such an object a **symbolic tensor**. It doesn’t contain any actual data, but it encodes the specifications of the actual tensors of data that the model will see when you use it. It stands for future tensors of data. <br>\n",
    "Next, we created a layer and called it on the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = layers.Dense(64, activation=\"relu\")(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Keras layers can be called both on real tensors of data and on these symbolic tensors. <br>\n",
    "In the latter case, they return a new symbolic tensor, with updated shape and dtype information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the final outputs, we instantiated the model by specifying its inputs and outputs in the Model constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the summary of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " my_input (InputLayer)       [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                256       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 906\n",
      "Trainable params: 906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MULTI-INPUT, MULTI-OUTPUT MODELS\n",
    "Unlike this toy model, most deep learning models don’t look like lists—they look like graphs. They may, for instance, have multiple inputs or multiple outputs. It’s for this kind of model that the **Functional API** really shines. <br>\n",
    "Let’s say you’re building a system to rank customer support tickets by priority and route them to the appropriate department. Your model has three inputs:\n",
    "- The title of the ticket (text input)\n",
    "- The text body of the ticket (text input)\n",
    "- Any tags added by the user (categorical input, assumed here to be one-hot encoded)\n",
    "\n",
    "We can encode the text inputs as arrays of ones and zeros of size vocabulary_size (see chapter 11 for detailed information about text encoding techniques). <br>\n",
    "Your model also has two outputs:\n",
    "- The priority score of the ticket, a scalar between 0 and 1 (**sigmoid** output)\n",
    "- The department that should handle the ticket (a **softmax** over the set of departments)\n",
    "\n",
    "You can build this model in a few lines with the Functional API.\n",
    "\n",
    "\n",
    "##### A multi-input, multi-output Functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "num_tags = 100\n",
    "num_departments = 4\n",
    "\n",
    "# Define model inputs\n",
    "title = keras.Input(shape=(vocabulary_size,), name=\"title\")\n",
    "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
    "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
    "\n",
    "features = layers.Concatenate()([title, text_body, tags]) # Combine input features into a single tensor, features, by concatenating them.\n",
    "features = layers.Dense(64, activation=\"relu\")(features) # Apply an intermediate layer to recombine input features into richer representations.\n",
    "\n",
    "# Define model outputs\n",
    "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\n",
    "department = layers.Dense(num_departments, activation=\"softmax\", name=\"department\")(features)\n",
    "\n",
    "# Create the model by specifying its inputs and outputs.\n",
    "model  = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Functional API is a simple, LEGO-like, yet very flexible way to define arbitrary graphs of layers like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRAINING A MULTI-INPUT, MULTI-OUTPUT MODEL\n",
    "You can train your model in much the same way as you would train a Sequential model, by calling **fit()** with lists of input and output data. These lists of data should be in the same order as the inputs you passed to the **Model** constructor.\n",
    "\n",
    "##### Training a model by providing lists of input and target arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 2s 16ms/step - loss: 30.6476 - priority_loss: 0.3170 - department_loss: 30.3306 - priority_mean_absolute_error: 0.4812 - department_accuracy: 0.2430\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 33.7820 - priority_loss: 0.3264 - department_loss: 33.4556 - priority_mean_absolute_error: 0.4906 - department_accuracy: 0.2477\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 1280\n",
    "\n",
    "# Dummy input data\n",
    "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n",
    "\n",
    "# Dummy target(output) data\n",
    "priority_data = np.random.random(size=(num_samples, 1))\n",
    "department_data = np.random.randint(0, 2, size=(num_samples, num_departments))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n",
    "              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n",
    "\n",
    "model.fit([title_data, text_body_data, tags_data],\n",
    "          [priority_data, department_data],\n",
    "          epochs=1)\n",
    "\n",
    "model.evaluate([title_data, text_body_data, tags_data],\n",
    "                [priority_data, department_data])\n",
    "\n",
    "priority_preds, department_preds = model.predict([title_data, text_body_data, tags_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don’t want to rely on input order (for instance, because you have many inputs or outputs), you can also leverage the names you gave to the Input objects and the output layers, and pass data via dictionaries.\n",
    "\n",
    "##### Training a model by providing dicts of input and target arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 14ms/step - loss: 38.0288 - priority_loss: 0.3264 - department_loss: 37.7025 - priority_mean_absolute_error: 0.4906 - department_accuracy: 0.2734\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 37.4851 - priority_loss: 0.3264 - department_loss: 37.1588 - priority_mean_absolute_error: 0.4906 - department_accuracy: 0.3242\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss={\"priority\": \"mean_squared_error\", \"department\": \"categorical_crossentropy\"},\n",
    "              metrics={\"priority\": [\"mean_absolute_error\"], \"department\": [\"accuracy\"]})\n",
    "\n",
    "model.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
    "          {\"priority\": priority_data, \"department\": department_data},\n",
    "          epochs=1)\n",
    "\n",
    "model.evaluate({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
    "          {\"priority\": priority_data, \"department\": department_data})\n",
    "\n",
    "priority_preds, department_preds = model.predict({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THE POWER OF THE FUNCTIONAL API: ACCESS TO LAYER CONNECTIVITY\n",
    "- A Functional model is an explicit graph data structure. \n",
    "- This makes it possible to inspect how layers are connected and reuse previous graph nodes (which are layer outputs) as part of new models. \n",
    "- It also nicely fits the “mental model” that most researchers use when thinking about a deep neural network: a graph of layers. \n",
    "- This enables two important use cases: **model visualization** and **feature extraction**.\n",
    "\n",
    "Let’s visualize the connectivity of the model we just defined (the topology of the model). You can plot a Functional model as a graph with the **plot_model()** utility (see figure 7.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, \"ticket_classifier.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/7.2.png)\n",
    "\n",
    "You can add to this plot the input and output shapes of each layer in the model, which can be helpful during debugging (see figure 7.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, \"ticket_classifier_with_shape_info.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/7.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “None” in the tensor shapes represents the batch size: this model allows batches of any size. <br>\n",
    "Access to layer connectivity also means that you can inspect and reuse individual nodes (layer calls) in the graph. The **model.layers** model property provides the list of layers that make up the model, and for each layer you can query **layer.input** and **layer.output**.\n",
    "\n",
    "##### Retrieving the inputs or outputs of a layer in a Functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x2c05388f640>,\n",
       " <keras.engine.input_layer.InputLayer at 0x2c054971e80>,\n",
       " <keras.engine.input_layer.InputLayer at 0x2c0538064f0>,\n",
       " <keras.layers.merge.Concatenate at 0x2c055c34700>,\n",
       " <keras.layers.core.dense.Dense at 0x2c055c342e0>,\n",
       " <keras.layers.core.dense.Dense at 0x2c055c019a0>,\n",
       " <keras.layers.core.dense.Dense at 0x2c055bf2f10>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 10000) dtype=float32 (created by layer 'title')>,\n",
       " <KerasTensor: shape=(None, 10000) dtype=float32 (created by layer 'text_body')>,\n",
       " <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'tags')>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 20100) dtype=float32 (created by layer 'concatenate')>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3].output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This enables you to do **feature extraction**, creating models that reuse intermediate features from another model. <br>\n",
    "Let’s say you want to add another output to the previous model—you want to estimate how long a given issue ticket will take to resolve, a kind of difficulty rating. You could do this via a classification layer over three categories: “quick,” “medium,” and “difficult.” You don’t need to recreate and retrain a model from scratch. You can start from the intermediate features of your previous model, since you have access to them, like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.layers[4].output # layer[4] is our intermediate Dense layer.\n",
    "difficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)\n",
    "\n",
    "new_model = keras.Model(\n",
    "    inputs=[title, text_body, tags],\n",
    "    outputs=[priority, department, difficulty])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot our new model (see figure 7.4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(new_model, \"updated_ticket_classifier.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/7.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subclassing the Model class\n",
    "The last model-building pattern you should know about is the most advanced one: **Model** subclassing. You learned in chapter 3 how to subclass the Layer class to create custom layers. Subclassing Model is pretty similar:\n",
    "- In the **__init__()** method, define the layers the model will use.\n",
    "- In the **call()** method, define the forward pass of the model, reusing the layers previously created.\n",
    "- Instantiate your subclass, and call it on data to create its weights.\n",
    "\n",
    "##### REWRITING OUR PREVIOUS EXAMPLE AS A SUBCLASSED MODEL\n",
    "Let’s take a look at a simple example: we will reimplement the customer support ticket management model using a **Model** subclass.\n",
    "\n",
    "##### A simple subclassed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerTicketModel(keras.Model):\n",
    "\n",
    "    def __init__(self, num_departments):\n",
    "        super().__init__() # Don't forget to call the super() constructor!\n",
    "        # Define sublayers in the constructor.\n",
    "        self.concat_layer = layers.Concatenate()\n",
    "        self.mixing_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.priority_scorer = layers.Dense(1, activation=\"sigmoid\")\n",
    "        self.department_classifier = layers.Dense(num_departments, activation=\"softmax\")\n",
    "    \n",
    "    # Define the forward pass in the call() method.\n",
    "    def call(self, inputs):\n",
    "        title = inputs[\"title\"]\n",
    "        text_body = inputs[\"text_body\"]\n",
    "        tags = inputs[\"tags\"]\n",
    "\n",
    "        features = self.concat_layer([title, text_body, tags])\n",
    "        features = self.mixing_layer(features)\n",
    "        priority = self.priority_scorer(features)\n",
    "        department = self.department_classifier(features)\n",
    "        return priority, department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’ve defined the model, you can instantiate it. Note that it will only create its weights the first time you call it on some data, much like **Layer** subclasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomerTicketModel(num_departments=4)\n",
    "\n",
    "priority, department = model({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, everything looks very similar to **Layer** subclassing, a workflow you encountered in chapter 3. What, then, is the difference between a **Layer** subclass and a **Model** subclass? <br>\n",
    "It’s simple: a “layer” is a building block you use to create models, and a “model” is the top-level object that you will actually train, export for inference, etc. In short, a **Model** has **fit()**, **evaluate()**, and **predict()** methods. Layers don’t. Other than that, the two classes are virtually identical. (Another difference is that you can **save** a model to a file on disk, which we will cover in a few sections.)\n",
    "\n",
    "You can compile and train a **Model** subclass just like a **Sequential** or **Functional** model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 14ms/step - loss: 37.5252 - output_1_loss: 0.3079 - output_2_loss: 37.2174 - output_1_mean_absolute_error: 0.4723 - output_2_accuracy: 0.2547\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 41.5722 - output_1_loss: 0.3264 - output_2_loss: 41.2458 - output_1_mean_absolute_error: 0.4906 - output_2_accuracy: 0.2477\n"
     ]
    }
   ],
   "source": [
    "# The structure of what you pass as the loss and metrics arguments must match exactly what gets returned by call() method—here, a list of two elements.\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n",
    "              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n",
    "\n",
    "# The structure of the input data must match exactly what is expected by the call() method— here, a dict with keys title, text_body, and tags.\n",
    "model.fit({\"title\": title_data,\n",
    "           \"text_body\": text_body_data,\n",
    "           \"tags\": tags_data},\n",
    "          [priority_data, department_data], # The structure of the target data must match exactly what is returned by the call() method—here a list of two elements.\n",
    "          epochs=1)\n",
    "\n",
    "model.evaluate({\"title\": title_data,\n",
    "                \"text_body\": text_body_data,\n",
    "                \"tags\": tags_data},\n",
    "               [priority_data, department_data])\n",
    "\n",
    "priority_preds, department_preds = model.predict({\"title\": title_data,\n",
    "                                                  \"text_body\": text_body_data,\n",
    "                                                  \"tags\": tags_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Model** subclassing workflow is the most flexible way to build a model. It enables you to build models that cannot be expressed as directed acyclic graphs of layers—imagine, for instance, a model where the **call()** method uses layers inside a **for** loop, or even calls them recursively. Anything is possible—you’re in charge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BEWARE: WHAT SUBCLASSED MODELS DON’T SUPPORT\n",
    "This freedom comes at a cost: with subclassed models, you are responsible for more of the model logic, which means your potential error surface is much larger. As a result, you will have more debugging work to do. You are developing a new Python object, not just snapping together LEGO bricks.\n",
    "\n",
    "Functional and subclassed models are also substantially different in nature. A Functional model is an explicit data structure—a graph of layers, which you can view, inspect, and modify. A subclassed model is a piece of bytecode—a Python class with a **call()** method that contains raw code. This is the source of the subclassing workflow’s flexibility—you can code up whatever functionality you like—but it introduces new limitations.\n",
    "\n",
    "For instance, because the way layers are connected to each other is hidden inside the body of the **call()** method, you cannot access that information. Calling **summary()** will not display layer connectivity, and you cannot plot the model topology via **plot_model()**. Likewise, if you have a subclassed model, you cannot access the nodes of the graph of layers to do feature extraction because there is simply no graph. Once the model is instantiated, its forward pass becomes a complete black box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixing and matching different components\n",
    "Crucially, choosing one of these patterns—the Sequential model, the Functional API, or Model subclassing—does not lock you out of the others. All models in the Keras API can smoothly interoperate with each other, whether they’re Sequential models, Functional models, or subclassed models written from scratch. They’re all part of the same spectrum of workflows. <br>\n",
    "For instance, you can use a subclassed layer or model in a Functional model.\n",
    "\n",
    "##### Creating a Functional model that includes a subclassed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        if num_classes == 2:\n",
    "            num_units = 1\n",
    "            activation = \"sigmoid\"\n",
    "        else:\n",
    "            num_units = num_classes\n",
    "            activation = \"softmax\"\n",
    "        self.dense = layers.Dense(num_units, activation=activation)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "outputs = Classifier(num_classes=10)(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inversely, you can use a Functional model as part of a subclassed layer or model.\n",
    "\n",
    "##### Creating a subclassed model that includes a Functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(64,))\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
    "binary_classifier = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "class MyModel(keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.dense = layers.Dense(64, activation=\"relu\")\n",
    "        self.classifier = binary_classifier\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        features = self.dense(inputs)\n",
    "        return self.classifier(features)\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remember: Use the right tool for the job\n",
    "You’ve learned about the spectrum of workflows for building Keras models, from the simplest workflow, the Sequential model, to the most advanced one, model subclassing. <br>\n",
    "When should you use one over the other? Each one has its pros and cons—pick the one most suitable for the job at hand.\n",
    "\n",
    "In general, the Functional API provides you with a pretty good trade-off between ease of use and flexibility. It also gives you direct access to layer connectivity, which is very powerful for use cases such as model plotting or feature extraction. <br> If you can use the Functional API—that is, if your model can be expressed as a directed acyclic graph of layers—I recommend using it over model subclassing.\n",
    "\n",
    "Going forward, all examples in this book will use the Functional API, simply because all the models we will work with are expressible as graphs of layers. We will, however, make frequent use of subclassed layers. In general, using Functional models that include subclassed layers provides the best of both worlds: high development flexibility while retaining the advantages of the Functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using built-in training and evaluation loops\n",
    "The principle of progressive disclosure of complexity—access to a spectrum of workflows that go from dead easy to arbitrarily flexible, one step at a time—also applies to model training. Keras provides you with different workflows for training models. They can be as simple as calling **fit()** on your data, or as advanced as writing a new training algorithm from scratch. <br>\n",
    "You are already familiar with the **compile()**, **fit()**, **evaluate()**, **predict()** workflow. As a reminder, take a look at the following listing.\n",
    "\n",
    "##### The standard workflow: compile(), fit(), evaluate(), predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.2992 - accuracy: 0.9122 - val_loss: 0.1492 - val_accuracy: 0.9572\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1665 - accuracy: 0.9529 - val_loss: 0.1227 - val_accuracy: 0.9664\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1380 - accuracy: 0.9631 - val_loss: 0.1239 - val_accuracy: 0.9679\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9699\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Create a model (we factor this into a separate function so as to reuse it later).\n",
    "def get_mnist_model():\n",
    "    inputs = keras.Input(shape=(28 * 28,))\n",
    "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(0.5)(features)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Load your data, reserving some for validation.\n",
    "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
    "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
    "train_images, val_images = images[10000:], images[:10000]\n",
    "train_labels, val_labels = labels[10000:], labels[:10000]\n",
    "\n",
    "model = get_mnist_model()\n",
    "\n",
    "# Compile the model by specifying its optimizer, the loss function to minimize, and the metrics to monitor.\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Use fit() to train the model, optionally providing validation data to monitor performance on unseen data.\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=3,\n",
    "          validation_data=(val_images, val_labels))\n",
    "\n",
    "# Use evaluate() to compute the loss and metrics on new data.\n",
    "test_metrics = model.evaluate(test_images, test_labels)\n",
    "# Use predict() to compute classification probabilities on new data.\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways you can customize this simple workflow:\n",
    "- Provide your own custom metrics.\n",
    "- Pass **callbacks** to the **fit()** method to schedule actions to be taken at specific points during training.\n",
    "\n",
    "Let’s take a look at these.\n",
    "\n",
    "##### Writing your own metrics\n",
    "Metrics are key to measuring the performance of your model—in particular, to measuring the difference between its performance on the training data and its performance on the test data. Commonly used metrics for classification and regression are already part of the built-in **keras.metrics** module, and most of the time that’s what you will use. But if you’re doing anything out of the ordinary, you will need to be able to write your own metrics. It’s simple!\n",
    "\n",
    "A Keras metric is a subclass of the **keras.metrics.Metric** class. Like layers, a metric has an internal state stored in TensorFlow variables. Unlike layers, these variables aren’t updated via backpropagation, so you have to write the state-update logic yourself, which happens in the **update_state()** method.\n",
    "\n",
    "For example, here’s a simple custom metric that measures the root mean squared error (RMSE).\n",
    "\n",
    "##### Implementing a custom metric by subclassing the Metric class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class RootMeanSquaredError(keras.metrics.Metric):\n",
    "\n",
    "    # Define the state variables in the constructor. Like for layers, you have access to the add_weight() method.\n",
    "    def __init__(self, name=\"rmse\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\")\n",
    "        self.total_samples = self.add_weight(name=\"total_samples\", initializer=\"zeros\", dtype=\"int32\")\n",
    "    \n",
    "    # Implement the state update logic in update_state(). The y_true argument is the targets (or labels) for one batch, while y_pred represents the corresponding predictions from the model. \n",
    "    # You can ignore the sample_weight argument—we won’t use it here.\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # To match our MNIST model, we expect categorical predictions and integer labels.\n",
    "        y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1])\n",
    "        mse = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "        self.mse_sum.assign_add(mse)\n",
    "        num_samples = tf.shape(y_pred)[0]\n",
    "        self.total_samples.assign_add(num_samples)\n",
    "\n",
    "    # You use the result() method to return the current value of the metric:\n",
    "    def result(self):\n",
    "        return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))\n",
    "    \n",
    "    # Meanwhile, you also need to expose a way to reset the metric state without having to reinstantiate it—this enables the same metric objects to be used across different epochs of training or across both training and evaluation. \n",
    "    # You do this with the reset_state() method:\n",
    "    def reset_states(self):\n",
    "        self.mse_sum.assign(0.)\n",
    "        self.total_samples.assign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom metrics can be used just like built-in ones. Let’s test-drive our own metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1555/1563 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.9123 - rmse: 7.1825"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\batuh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1841: UserWarning: Metric RootMeanSquaredError implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.2962 - accuracy: 0.9126 - rmse: 7.1826 - val_loss: 0.1428 - val_accuracy: 0.9592 - val_rmse: 7.3539\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1636 - accuracy: 0.9538 - rmse: 7.3533 - val_loss: 0.1194 - val_accuracy: 0.9678 - val_rmse: 7.4045\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.1398 - accuracy: 0.9624 - rmse: 7.3887 - val_loss: 0.1162 - val_accuracy: 0.9721 - val_rmse: 7.4198\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9715 - rmse: 7.4326\n"
     ]
    }
   ],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\", RootMeanSquaredError()])\n",
    "\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=3,\n",
    "          validation_data=(val_images, val_labels))\n",
    "\n",
    "test_metrics = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the **fit()** progress bar displaying the RMSE of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using callbacks\n",
    "Launching a training run on a large dataset for tens of epochs using **model.fit()** can be a bit like launching a paper airplane: past the initial impulse, you don’t have any control over its trajectory or its landing spot. If you want to avoid bad outcomes (and thus wasted paper airplanes), it’s smarter to use, not a paper plane, but a drone that can sense its environment, send data back to its operator, and automatically make steering decisions based on its current state. The Keras **callbacks** API will help you transform your call to **model.fit()** from a paper airplane into a smart, autonomous drone that can self-introspect and dynamically take action.\n",
    "\n",
    "A **callback** is an object (a class instance implementing specific methods) that is passed to the model in the call to **fit()** and that is called by the model at various points during training. It has access to all the available data about the state of the model and its performance, and it can take action: interrupt training, save a model, load a different weight set, or otherwise alter the state of the model. <br>\n",
    "Here are some examples of ways you can use callbacks:\n",
    "- **Model checkpointing**—Saving the current state of the model at different points during training.\n",
    "- **Early stopping**—Interrupting training when the validation loss is no longer improving (and of course, saving the best model obtained during training).\n",
    "- **Dynamically adjusting the value of certain parameters during training**—Such as the learning rate of the optimizer.\n",
    "- **Logging training and validation metrics during training, or visualizing the representations learned by the model as they’re updated**—The **fit()** progress bar that you’re familiar with is in fact a callback!\n",
    "\n",
    "The **keras.callbacks** module includes a number of built-in callbacks (this is not an exhaustive list): <br>\n",
    "keras.callbacks.ModelCheckpoint <br>\n",
    "keras.callbacks.EarlyStopping <br>\n",
    "keras.callbacks.LearningRateScheduler <br>\n",
    "keras.callbacks.ReduceLROnPlateau <br>\n",
    "keras.callbacks.CSVLogger <br>\n",
    "\n",
    "Let’s review two of them to give you an idea of how to use them: **EarlyStopping** and **ModelCheckpoint**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THE EARLYSTOPPING AND MODELCHECKPOINT CALLBACKS\n",
    "When you’re training a model, there are many things you can’t predict from the start. In particular, you can’t tell how many epochs will be needed to get to an optimal validation loss. Our examples so far have adopted the strategy of training for enough epochs that you begin overfitting, using the first run to figure out the proper number of epochs to train for, and then finally launching a new training run from scratch using this optimal number. Of course, this approach is wasteful. A much better way to handle this is to stop training when you measure that the validation loss is no longer improving. This can be achieved using the **EarlyStopping** callback.\n",
    "\n",
    "The **EarlyStopping** callback interrupts training once a target metric being monitored has stopped improving for a fixed number of epochs. For instance, this callback allows you to interrupt training as soon as you start overfitting, thus avoiding having to retrain your model for a smaller number of epochs. This callback is typically used in combination with **ModelCheckpoint**, which lets you continually save the model during training (and, optionally, save only the current best model so far: the version of the model that achieved the best performance at the end of an epoch).\n",
    "\n",
    "##### Using the callbacks argument in the fit() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.2954 - accuracy: 0.9118 - val_loss: 0.1542 - val_accuracy: 0.9570\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1664 - accuracy: 0.9527 - val_loss: 0.1196 - val_accuracy: 0.9654\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1407 - accuracy: 0.9626 - val_loss: 0.1232 - val_accuracy: 0.9701\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1260 - accuracy: 0.9676 - val_loss: 0.1137 - val_accuracy: 0.9746\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1163 - accuracy: 0.9706 - val_loss: 0.1126 - val_accuracy: 0.9737\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1116 - accuracy: 0.9729 - val_loss: 0.1145 - val_accuracy: 0.9749\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1050 - accuracy: 0.9750 - val_loss: 0.1190 - val_accuracy: 0.9756\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1001 - accuracy: 0.9762 - val_loss: 0.1155 - val_accuracy: 0.9770\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0970 - accuracy: 0.9782 - val_loss: 0.1199 - val_accuracy: 0.9763\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0934 - accuracy: 0.9778 - val_loss: 0.1259 - val_accuracy: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c05f33bf70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callbacks are passed to the model via the callbacks argument in fit(), which takes a list of callbacks. \n",
    "# You can pass any number of callbacks.\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping( # Interrupts training when improvement stops\n",
    "        monitor=\"val_accuracy\", # Monitors the model’s validation accuracy\n",
    "        patience=2, # Interrupts training when the model has not improved for 2 epochs\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint( # Saves the current weights after every epoch\n",
    "        filepath=\"checkpoint_path.keras\", # Path to the destination model file\n",
    "        # These two arguments mean you won’t overwrite the model file unless val_loss has improved, which allows you to keep the best model seen during training.\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"]) # You monitor accuracy, so it should be part of the model’s metrics.\n",
    "# Note that because the callback will monitor validation loss and validation accuracy, you need to pass validation_data to the call to fit().\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can always save models manually after training as well—just call **model.save('my_checkpoint_path')**. To reload the model you’ve saved, just use\n",
    "```python\n",
    "model = keras.models.load_model(\"checkpoint_path.keras\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"checkpoint_path.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing your own callbacks\n",
    "If you need to take a specific action during training that isn’t covered by one of the built-in callbacks, you can write your own callback. Callbacks are implemented by subclassing the **keras.callbacks.Callback** class. You can then implement any number of the following transparently named methods, which are called at various points during training:\n",
    "- on_epoch_begin(epoch, logs) # Called at the start of every epoch\n",
    "- on_epoch_end(epoch, logs) # Called at the end of every epoch\n",
    "- on_batch_begin(batch, logs) # Called right before processing each batch\n",
    "- on_batch_end(batch, logs) # Called right after processing each batch\n",
    "- on_train_begin(logs) # Called at the start of training\n",
    "- on_train_end(logs) # Called at the end of training\n",
    "\n",
    "These methods are all called with a **logs** argument, which is a dictionary containing information about the previous batch, epoch, or training run—training and validation metrics, and so on. The **on_epoch_*** and **on_batch_*** methods also take the epoch or batch index as their first argument (an integer).\n",
    "\n",
    "Here’s a simple example that saves a list of per-batch loss values during training and saves a graph of these values at the end of each epoch.\n",
    "\n",
    "##### Creating a custom callback by subclassing the Callback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.per_batch_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.per_batch_losses.append(logs.get(\"loss\"))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        plt.clf()\n",
    "        plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses,\n",
    "                 label=\"Training loss for each batch\")\n",
    "        plt.xlabel(f\"Batch (epoch {epoch})\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"./plots/plot_at_epoch_{epoch}\")\n",
    "        self.per_batch_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s test-drive it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.2923 - accuracy: 0.9152 - val_loss: 0.1519 - val_accuracy: 0.9577\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1647 - accuracy: 0.9532 - val_loss: 0.1285 - val_accuracy: 0.9666\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1407 - accuracy: 0.9626 - val_loss: 0.1066 - val_accuracy: 0.9716\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1257 - accuracy: 0.9666 - val_loss: 0.1012 - val_accuracy: 0.9740\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1145 - accuracy: 0.9704 - val_loss: 0.1070 - val_accuracy: 0.9754\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1113 - accuracy: 0.9721 - val_loss: 0.1123 - val_accuracy: 0.9735\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1038 - accuracy: 0.9748 - val_loss: 0.1072 - val_accuracy: 0.9779\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1003 - accuracy: 0.9765 - val_loss: 0.1083 - val_accuracy: 0.9787\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0960 - accuracy: 0.9773 - val_loss: 0.1104 - val_accuracy: 0.9788\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0962 - accuracy: 0.9787 - val_loss: 0.1172 - val_accuracy: 0.9775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c0560fa610>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          callbacks=[LossHistory()],\n",
    "          validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Monitoring and visualization with TensorBoard\n",
    "To do good research or develop good models, you need rich, frequent feedback about what’s going on inside your models during your experiments. That’s the point of running experiments: to get information about how well a model performs—as much information as possible. Making progress is an iterative process, a loop—you start with an idea and express it as an experiment, attempting to validate or invalidate your idea. You run this experiment and process the information it generates. This inspires your next idea. The more iterations of this loop you’re able to run, the more refined and powerful your ideas become. Keras helps you go from idea to experiment in the least possible time, and fast GPUs can help you get from experiment to result as quickly as possible. But what about processing the experiment’s results? That’s where TensorBoard comes in (see figure 7.6).\n",
    "\n",
    "![](./images/7.6.png)\n",
    "\n",
    "TensorBoard (www.tensorflow.org/tensorboard) is a browser-based application that you can run locally. It’s the best way to monitor everything that goes on inside your model during training. With TensorBoard, you can\n",
    "- Visually monitor metrics during training\n",
    "- Visualize your model architecture\n",
    "- Visualize histograms of activations and gradients\n",
    "- Explore embeddings in 3D\n",
    "\n",
    "If you’re monitoring more information than just the model’s final loss, you can develop a clearer vision of what the model does and doesn’t do, and you can make progress more quickly. <br>\n",
    "The easiest way to use TensorBoard with a Keras model and the **fit()** method is to use the **keras.callbacks.TensorBoard** callback. <br>\n",
    "In the simplest case, just specify where you want the callback to write logs, and you’re good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.2950 - accuracy: 0.9128 - val_loss: 0.1534 - val_accuracy: 0.9566\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1664 - accuracy: 0.9524 - val_loss: 0.1316 - val_accuracy: 0.9638\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1388 - accuracy: 0.9626 - val_loss: 0.1149 - val_accuracy: 0.9706\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1279 - accuracy: 0.9671 - val_loss: 0.1137 - val_accuracy: 0.9713\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1207 - accuracy: 0.9706 - val_loss: 0.1105 - val_accuracy: 0.9750\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1125 - accuracy: 0.9717 - val_loss: 0.1144 - val_accuracy: 0.9763\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1077 - accuracy: 0.9747 - val_loss: 0.1165 - val_accuracy: 0.9759\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1069 - accuracy: 0.9751 - val_loss: 0.1201 - val_accuracy: 0.9766\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0966 - accuracy: 0.9774 - val_loss: 0.1223 - val_accuracy: 0.9765\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0972 - accuracy: 0.9780 - val_loss: 0.1203 - val_accuracy: 0.9781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c0569e2ca0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          validation_data=(val_images, val_labels),\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model starts running, it will write logs at the target location. If you are running your Python script on a local machine, you can then launch the local TensorBoard server using the following command (note that the tensorboard executable should be already available if you have installed TensorFlow via pip; if not, you can install TensorBoard manually via pip install tensorboard):\n",
    "\n",
    "tensorboard --logdir /full_path_to_your_log_dir\n",
    "\n",
    "You can then navigate to the URL that the command returns in order to access the TensorBoard interface. <br>\n",
    "If you are running your script in a Colab notebook, you can run an embedded TensorBoard instance as part of your notebook, using the following commands:\n",
    "\n",
    "In the TensorBoard interface, you will be able to monitor live graphs of your training and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 16296), started 11:38:39 ago. (Use '!kill 16296' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1a6aab7238bf6a12\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1a6aab7238bf6a12\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing your own training and evaluation loops\n",
    "The **fit()** workflow strikes a nice balance between ease of use and flexibility. It’s what you will use most of the time. However, it isn’t meant to support everything a deep learning researcher may want to do, even with custom metrics, custom losses, and custom callbacks.\n",
    "\n",
    "After all, the built-in **fit()** workflow is solely focused on **supervised learning**: a setup where there are known targets (also called labels or annotations) associated with your input data, and where you compute your loss as a function of these targets and the model’s predictions. However, not every form of machine learning falls into this category. There are other setups where no explicit targets are present, such as **generative learning** (which we will discuss in chapter 12), **self-supervised learning** (where targets are obtained from the inputs), and **reinforcement learning** (where learning is driven by occasional “rewards,” much like training a dog). Even if you’re doing regular supervised learning, as a researcher, you may want to add some novel bells and whistles that require low-level flexibility.\n",
    "\n",
    "Whenever you find yourself in a situation where the built-in **fit()** is not enough, you will need to write your own custom training logic. You already saw simple examples of low-level training loops in chapters 2 and 3. As a reminder, the contents of a typical training loop look like this:\n",
    "- Run the forward pass (compute the model’s output) inside a gradient tape to obtain a loss value for the current batch of data.\n",
    "- Retrieve the gradients of the loss with regard to the model’s weights.\n",
    "- Update the model’s weights so as to lower the loss value on the current batch of data.\n",
    "\n",
    "These steps are repeated for as many batches as necessary. This is essentially what **fit()** does under the hood. In this section, you will learn to reimplement **fit()** from scratch, which will give you all the knowledge you need to write any training algorithm you may come up with.\n",
    "\n",
    "Let’s go over the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training versus inference\n",
    "In the low-level training loop examples you’ve seen so far, step 1 (the forward pass) was done via **predictions = model(inputs)**, and step 2 (retrieving the gradients computed by the gradient tape) was done via **gradients = tape.gradient(loss, model.weights)**. In the general case, there are actually two subtleties you need to take into account.\n",
    "\n",
    "Some Keras layers, such as the **Dropout** layer, have different behaviors during **training** and during **inference** (when you use them to generate predictions). Such layers expose a training Boolean argument in their **call()** method. Calling **dropout(inputs, training=True)** will drop some activation entries, while calling **dropout(inputs, training=False)** does nothing. By extension, Functional and Sequential models also expose this **training** argument in their **call()** methods. Remember to pass **training=True** when you call a Keras model during the forward pass! Our forward pass thus becomes **predictions = model(inputs, training=True)**.\n",
    "\n",
    "In addition, note that when you retrieve the gradients of the weights of your model, you should not use **tape.gradients(loss, model.weights)**, but rather **tape.gradients(loss, model.trainable_weights)**. Indeed, layers and models own two kinds of weights:\n",
    "- **Trainable weights**—These are meant to be updated via backpropagation to minimize the loss of the model, such as the kernel and bias of a Dense layer.\n",
    "- **Non-trainable weights**—These are meant to be updated during the forward pass by the layers that own them. For instance, if you wanted a custom layer to keep a counter of how many batches it has processed so far, that information would be stored in a non-trainable weight, and at each batch, your layer would increment the counter by one.\n",
    "\n",
    "Among Keras built-in layers, the only layer that features non-trainable weights is the **BatchNormalization** layer, which we will discuss in chapter 9. The **BatchNormalization** layer needs non-trainable weights in order to track information about the mean and standard deviation of the data that passes through it, so as to perform an online approximation of **feature normalization** (a concept you learned about in chapter 6). <br>\n",
    "Taking into account these two details, a supervised-learning training step ends up looking like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "        gradients = tape.gradients(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(model.trainable_weights, gradients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Low-level usage of metrics\n",
    "In a low-level training loop, you will probably want to leverage Keras metrics (whether custom ones or the built-in ones). You’ve already learned about the metrics API: simply call **update_state(y_true, y_pred)** for each batch of targets and predictions, and then use **result()** to query the current metric value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: 1.00\n"
     ]
    }
   ],
   "source": [
    "metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "targets = [0, 1, 2]\n",
    "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "metric.update_state(targets, predictions)\n",
    "current_result = metric.result()\n",
    "print(f\"result: {current_result:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also need to track the average of a scalar value, such as the model’s loss. You can do this via the **keras.metrics.Mean** metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of values: 2.00\n"
     ]
    }
   ],
   "source": [
    "values = [0, 1, 2, 3, 4]\n",
    "mean_tracker = keras.metrics.Mean()\n",
    "for value in values:\n",
    "    mean_tracker.update_state(value)\n",
    "print(f\"Mean of values: {mean_tracker.result():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to use **metric.reset_state()** when you want to reset the current results (at the start of a training epoch or at the start of evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A complete training and evaluation loop\n",
    "Let’s combine the forward pass, backward pass, and metrics tracking into a **fit()** like training step function that takes a batch of data and targets and returns the logs that would get displayed by the **fit()** progress bar.\n",
    "\n",
    "##### Writing a step-by-step training loop: the training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mnist_model()\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy() # Prepare the loss function.\n",
    "optimizer = keras.optimizers.RMSprop() # Prepare the optimizer.\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()] # Prepare the list of metrics to monitor.\n",
    "loss_tracking_metric = keras.metrics.Mean() # Prepare a Mean metric tracker to keep track of the loss average.\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    # Run the forward pass of the model Note that we pass training=True.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    # Run the backward pass. Note that we use model.trainable_weights.\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    # Keep track of metrics.\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[metric.name] = metric.result()\n",
    "\n",
    "    # Keep track of the loss average.\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"loss\"] = loss_tracking_metric.result()\n",
    "    return logs # Return the current values of the metrics and the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to reset the state of our metrics at the start of each epoch and before running evaluation. Here’s a utility function to do it.\n",
    "\n",
    "##### Writing a step-by-step training loop: resetting the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_metrics():\n",
    "    for metric in metrics:\n",
    "        metric.reset_state()\n",
    "    loss_tracking_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now lay out our complete training loop. Note that we use a **tf.data.Dataset** object to turn our NumPy data into an iterator that iterates over the data in batches of size 32.\n",
    "\n",
    "##### Writing a step-by-step training loop: the loop itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at the end of epoch 0\n",
      "...sparse_categorical_accuracy: 0.9131\n",
      "...loss: 0.2901\n",
      "Results at the end of epoch 1\n",
      "...sparse_categorical_accuracy: 0.9522\n",
      "...loss: 0.1680\n",
      "Results at the end of epoch 2\n",
      "...sparse_categorical_accuracy: 0.9622\n",
      "...loss: 0.1395\n"
     ]
    }
   ],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "training_dataset = training_dataset.batch(32)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    reset_metrics()\n",
    "    for inputs_batch, targets_batch in training_dataset:\n",
    "        logs = train_step(inputs_batch, targets_batch)\n",
    "    print(f\"Results at the end of epoch {epoch}\")\n",
    "    for key, value in logs.items():\n",
    "        print(f\"...{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here’s the evaluation loop: a simple **for** loop that repeatedly calls a **test_step()** function, which processes a single batch of data. The **test_step()** function is just a subset of the logic of **train_step()**. It omits the code that deals with updating the weights of the model—that is to say, everything involving the **GradientTape** and the optimizer.\n",
    "\n",
    "##### Writing a step-by-step evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "...val_sparse_categorical_accuracy: 0.9665\n",
      "...val_loss: 0.1341\n"
     ]
    }
   ],
   "source": [
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False) # Note that we pass training=False.\n",
    "    loss = loss_fn(targets, predictions)\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[\"val_\" + metric.name] = metric.result()\n",
    "    \n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "\n",
    "for inputs_batch, targets_batch in val_dataset:\n",
    "    logs = test_step(inputs_batch, targets_batch)\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in logs.items():\n",
    "    print(f\"...{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats—you’ve just reimplemented **fit()** and **evaluate()**! Or almost: fit() and evaluate() support many more features, including large-scale distributed computation, which requires a bit more work. It also includes several key performance optimizations. <br>\n",
    "Let’s take a look at one of these optimizations: TensorFlow function compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make it fast with tf.function\n",
    "- You may have noticed that your custom loops are running significantly slower than the built-in **fit()** and **evaluate()**, despite implementing essentially the same logic.\n",
    "- That’s because, by default, TensorFlow code is executed line by line, **eagerly**, much like NumPy code or regular Python code. Eager execution makes it easier to debug your code, but it is far from optimal from a performance standpoint.\n",
    "- It’s more performant to **compile** your TensorFlow code into a **computation graph** that can be globally optimized in a way that code interpreted line by line cannot. \n",
    "- The syntax to do this is very simple: just add a **@tf.function** to any function you want to compile before executing, as shown in the following listing.\n",
    "\n",
    "##### Adding a @tf.function decorator to our evaluation-step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "...val_sparse_categorical_accuracy: 0.9665\n",
      "...val_loss: 0.1341\n"
     ]
    }
   ],
   "source": [
    "@tf.function # This is the only line that is different from the previous code.\n",
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, predictions)\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[\"val_\" + metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "for inputs_batch, targets_batch in val_dataset:\n",
    "    logs = test_step(inputs_batch, targets_batch)\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in logs.items():\n",
    "    print(f\"...{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Colab CPU, we go from taking 1.80 s to run the evaluation loop to only 0.8 s.\n",
    "<br>Much faster! <br>\n",
    "Remember, while you are debugging your code, prefer running it eagerly, without any **@tf.function** decorator. It’s easier to track bugs this way. Once your code is working and you want to make it fast, add a **@tf.function** decorator to your training step and your evaluation step—or any other performance-critical function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leveraging fit() with a custom training loop\n",
    "In the previous sections, we were writing our own training loop entirely from scratch. Doing so provides you with the most flexibility, but you end up writing a lot of code while simultaneously missing out on many convenient features of **fit()**, such as callbacks or built-in support for distributed training.\n",
    "\n",
    "What if you need a custom training algorithm, but you still want to leverage the power of the built-in Keras training logic? There’s actually a middle ground between **fit()** and a training loop written from scratch: you can provide a custom training step function and let the framework do the rest. <br>\n",
    "You can do this by overriding the **train_step()** method of the **Model** class. This is the function that is called by **fit()** for every batch of data. You will then be able to call **fit()** as usual, and it will be running your own learning algorithm under the hood.\n",
    "\n",
    "Here’s a simple example:\n",
    "- We create a new class that subclasses **keras.Model**.\n",
    "- We override the method **train_step(self, data)**. Its contents are nearly identical to what we used in the previous section. It returns a dictionary mapping metric names (including the loss) to their current values.\n",
    "- We implement a **metrics** property that tracks the model’s **Metric** instances.\n",
    "  - This enables the model to automatically call **reset_state()** on the model’s metrics at the start of each epoch and at the start of a call to **evaluate()**, so you don’t have to do it by hand.\n",
    "\n",
    "##### Implementing a custom training step to use with fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "# This metric object will be used to track the average of per-batch losses during training and evaluation.\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    # We override the train_step method.\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            # We use self(inputs, training=True) instead of model(inputs, training=True), since our model is the class itself.\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = loss_fn(targets, predictions)\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "\n",
    "        # We update the loss tracker metric that tracks the average of the loss.\n",
    "        loss_tracker.update_state(loss)\n",
    "        # We return the average loss so far by querying the loss tracker metric.\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # Any metric you would like to reset across epochs should be listed here.\n",
    "        return [loss_tracker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate our custom model, compile it (we only pass the optimizer, since the loss is already defined outside of the model), and train it using **fit()** as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.2948\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1643\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.1373A: 0s - loss: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c05ee3e2b0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(28 * 28,))\n",
    "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "features = layers.Dropout(0.5)(features)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = CustomModel(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop())\n",
    "model.fit(train_images, train_labels, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of points to note:\n",
    "- This pattern does not prevent you from building models with the Functional API. You can do this whether you’re building Sequential models, Functional API models, or subclassed models.\n",
    "- You don’t need to use a @tf.function decorator when you override **train_step**—the framework does it for you.\n",
    "\n",
    "Now, what about metrics, and what about configuring the loss via **compile()**? After you’ve called **compile()**, you get access to the following:\n",
    "- **self.compiled_loss**—The loss function you passed to **compile()**.\n",
    "- **self.compiled_metrics**—A wrapper for the list of metrics you passed, which allows you to call **self.compiled_metrics.update_state()** to update all of your metrics at once.\n",
    "- **self.metrics**—The actual list of metrics you passed to **compile()**. Note that it also includes a metric that tracks the loss, similar to what we did manually with our **loss_tracking_metric** earlier.\n",
    "\n",
    "We can thus write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            # Compute the loss via self.compiled_loss.\n",
    "            loss = self.compiled_loss(targets, predictions)\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "        # Update the model’s metrics via self.compiled_metrics.\n",
    "        self.compiled_metrics.update_state(targets, predictions)\n",
    "        # Return a dict mapping metric names to their current value.\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.2979 - sparse_categorical_accuracy: 0.9115A: \n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1673 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1426 - sparse_categorical_accuracy: 0.9614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c05f17f610>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(28 * 28,))\n",
    "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "features = layers.Dropout(0.5)(features)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = CustomModel(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(train_images, train_labels, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot of information, but you now know enough to use Keras to do almost anything.\n",
    "\n",
    "#### Summary\n",
    "- Keras offers a spectrum of different workflows, based on the principle of *progressive disclosure of complexity*. They all smoothly inter-operate together.\n",
    "- You can build models via the Sequential class, via the Functional API, or by subclassing the Model class. Most of the time, you’ll be using the Functional API.\n",
    "- The simplest way to train and evaluate a model is via the default **fit()** and **evaluate()** methods.\n",
    "- Keras callbacks provide a simple way to monitor models during your call to **fit()** and automatically take action based on the state of the model.\n",
    "- You can also fully take control of what **fit()** does by overriding the **train_step()** method.\n",
    "- Beyond **fit()**, you can also write your own training loops entirely from scratch. This is useful for researchers implementing brand-new training algorithms."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483abfc7fdcd927bfa336910f494643cce94b3dfa36bcded073270b8df64edb3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
