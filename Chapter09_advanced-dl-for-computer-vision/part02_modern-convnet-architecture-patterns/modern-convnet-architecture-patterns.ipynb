{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modern convnet architecture patterns\n",
    "A model’s “architecture” is the sum of the choices that went into creating it: which layers to use, how to configure them, and in what arrangement to connect them. These choices define the **hypothesis space** of your model: the space of possible functions that gradient descent can search over, parameterized by the model’s weights. Like feature engineering, a good hypothesis space encodes **prior knowledge** that you have about the problem at hand and its solution. For instance, using convolution layers means that you know in advance that the relevant patterns present in your input images are translation invariant. In order to effectively learn from data, you need to make assumptions about what you’re looking for.\n",
    "\n",
    "Model architecture is often the difference between success and failure. If you make inappropriate architecture choices, your model may be stuck with suboptimal metrics, and no amount of training data will save it. Inversely, a good model architecture will accelerate learning and will enable your model to make efficient use of the training data available, reducing the need for large datasets. **A good model architecture is one that reduces the size of the search space or otherwise makes it easier to converge to a good point of the search space.** Just like feature engineering and data curation, **model architecture is all about making the problem simpler for gradient descent to solve**. And remember that gradient descent is a pretty stupid search process, so it needs all the help it can get.\n",
    "\n",
    "Model architecture is more an art than a science. Experienced machine learning engineers are able to intuitively cobble together high-performing models on their first try, while beginners often struggle to create a model that trains at all. The keyword here is **intuitively**: no one can give you a clear explanation of what works and what doesn’t. Experts rely on pattern-matching, an ability that they acquire through extensive practical experience. You’ll develop your own intuition throughout this book. However, it’s not all about intuition either—there isn’t much in the way of actual science, but as in any engineering discipline, there are best practices.\n",
    "\n",
    "In the following sections, we’ll review a few essential convnet architecture best practices: in particular, **residual connections**, **batch normalization**, and **separable convolutions**. Once you master how to use them, you will be able to build highly effective image models. We will apply them to our cat vs. dog classification problem.\n",
    "\n",
    "Let’s start from the bird’s-eye view: the **modularity-hierarchy-reuse (MHR)** formula for system architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modularity, hierarchy, and reuse\n",
    "If you want to make a complex system simpler, there’s a universal recipe you can apply: just structure your amorphous soup of complexity into **modules**, organize the modules into a **hierarchy**, and start **reusing** the same modules in multiple places as appropriate (“reuse” is another word for **abstraction** in this context). That’s the MHR formula (modularity-hierarchy-reuse), and it underlies system architecture across pretty much every domain where the term “architecture” is used. It’s at the heart of the organization of any system of meaningful complexity, whether it’s a cathedral, your own body, the US Navy, or the Keras codebase (see figure 9.7).\n",
    "\n",
    "![](./chapter_images/9.7.png)\n",
    "\n",
    "If you’re a software engineer, you’re already keenly familiar with these principles: an effective codebase is one that is modular, hierarchical, and where you don’t reimplement the same thing twice, but instead rely on reusable classes and functions. If you factor your code by following these principles, you could say you’re doing “software architecture.”\n",
    "\n",
    "Deep learning itself is simply the application of this recipe to continuous optimization via gradient descent: you take a classic optimization technique (gradient descent over a continuous function space), and you structure the search space into modules (layers), organized into a deep hierarchy (often just a stack, the simplest kind of hierarchy), where you reuse whatever you can (for instance, convolutions are all about reusing the same information in different spatial locations). <br>\n",
    "Likewise, deep learning model architecture is primarily about making clever use of modularity, hierarchy, and reuse. You’ll notice that all popular convnet architectures are not only structured into layers, they’re structured into repeated groups of layers (called “blocks” or “modules”). For instance, the popular VGG16 architecture we used in the previous chapter is structured into repeated “conv, conv, max pooling” blocks (see figure 9.8). <br>\n",
    "Further, most convnets often feature pyramid-like structures (feature hierarchies). Recall, for example, the progression in the number of convolution filters we used in the first convnet we built in the previous chapter: 32, 64, 128. **The number of filters grows with layer depth, while the size of the feature maps shrinks accordingly.** You’ll notice the same pattern in the blocks of the VGG16 model (see figure 9.8).\n",
    "\n",
    "![](./chapter_images/9.8.png)\n",
    "\n",
    "Deeper hierarchies are intrinsically good because they encourage feature reuse, and therefore abstraction. In general, a deep stack of narrow layers performs better than a shallow stack of large layers. However, there’s a limit to how deep you can stack layers, due to the problem of **vanishing gradients**. This leads us to our first essential model architecture pattern: **residual connections**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Residual connections\n",
    "You probably know about the game of Telephone, also called Chinese whispers in the UK and téléphone arabe in France, where an initial message is whispered in the ear of a player, who then whispers it in the ear of the next player, and so on. The final message ends up bearing little resemblance to its original version. It’s a fun metaphor for the **cumulative errors** that occur in sequential transmission over a noisy channel. As it happens, backpropagation in a sequential deep learning model is pretty similar to the game of Telephone. You’ve got a chain of functions, like this one: <br>\n",
    "y = f4(f3(f2(f1(x)))) <br>\n",
    "The name of the game is to adjust the parameters of each function in the chain based on the error recorded on the output of f4 (the loss of the model). To adjust f1, you’ll need to percolate error information through f2, f3, and f4. However, each successive function in the chain introduces some amount of noise. If your function chain is too deep, this noise starts overwhelming gradient information, and backpropagation stops working. Your model won’t train at all. This is the **vanishing gradients** problem.\n",
    "\n",
    "The fix is simple: just force each function in the chain to be nondestructive—to retain a noiseless version of the information contained in the previous input. The easiest way to implement this is to use a **residual connection**. It’s dead easy: just **add the input of a layer or block of layers back to its output** (see figure 9.9). The **residual connection** acts as an **information shortcut** around destructive or noisy blocks (such as blocks that contain relu activations or dropout layers), enabling error gradient information from early layers to propagate noiselessly through a deep network. This technique was introduced in 2015 with the ResNet family of models (developed by He et al. at Microsoft).\n",
    "\n",
    "![](./chapter_images/9.9.png)\n",
    "\n",
    "In practice, you’d implement a residual connection as follows.\n",
    "\n",
    "##### A residual connection in pseudocode\n",
    "\n",
    "```python\n",
    "x = ... # Some input tensor\n",
    "residual = x # Save a pointer to the original input. This is called the residual.\n",
    "x = block(x) # This computation block can potentially be destructive or noisy, and that’s fine.\n",
    "x = add([x, residual]) # Add the original input to the layer’s output: the final output will thus always preserve full information about the original input.\n",
    "```\n",
    "\n",
    "Note that adding the input back to the output of a block implies that **the output should have the same shape as the input**. However, this is not the case if your block includes convolutional layers with an increased number of filters, or a max pooling layer. In such cases, use a **1 × 1 Conv2D layer with no activation to linearly project the residual to the desired output shape**. \n",
    "\n",
    "You’d typically use **padding=\"same\"** in the convolution layers in your target block so as to avoid spatial downsampling due to padding, and you’d use **strides** in the residual projection to match any downsampling caused by a max pooling layer.\n",
    "\n",
    "##### Residual block where the number of filters changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "residual = x # Set aside the residual\n",
    "# This is the layer around which we create a residual connection: it increases the number of output filers from 32 to 64.\n",
    "# Note that we use padding=\"same\" to avoid downsampling due to padding.\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "# The residual only had 32 filters, so we use a 1 × 1 Conv2D to project it to the correct shape.\n",
    "residual = layers.Conv2D(64, 1)(residual)\n",
    "# Now the block output and the residual have the same shape and can be added.\n",
    "x = layers.add([x, residual])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case where target block includes a max pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "residual = x # Set aside the residual\n",
    "# This is the block of two layers around which we create a residual connection: it includes a 2 × 2 max pooling layer. \n",
    "# Note that we use padding=\"same\" in both the convolution layer and the max pooling layer to avoid downsampling due to padding.\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D(2, padding=\"same\")(x)\n",
    "# We use strides=2 in the residual projection to match the downsampling created by the max pooling layer.\n",
    "residual = layers.Conv2D(64, 1, strides=2)(residual)\n",
    "# Now the block output and the residual have the same shape and can be added.\n",
    "x = layers.add([x, residual])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make these ideas more concrete, here’s an example of a simple convnet structured into a series of blocks, each made of two convolution layers and one optional max pooling layer, with a residual connection around each block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling (Rescaling)          (None, 32, 32, 3)    0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 32)   896         ['rescaling[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 32)   9248        ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 32)  0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 32)   128         ['rescaling[0][0]']              \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 16, 16, 32)   0           ['max_pooling2d_1[0][0]',        \n",
      "                                                                  'conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 64)   18496       ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 64)   36928       ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)    0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 8, 8, 64)     2112        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 8, 8, 64)     0           ['max_pooling2d_2[0][0]',        \n",
      "                                                                  'conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 8, 8, 128)    73856       ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 8, 8, 128)    147584      ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 8, 8, 128)    8320        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 8, 8, 128)    0           ['conv2d_13[0][0]',              \n",
      "                                                                  'conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 128)         0           ['add_4[0][0]']                  \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            129         ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 297,697\n",
      "Trainable params: 297,697\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "\n",
    "# Utility function to apply a convolutional block with a residual connection, with an option to add max pooling\n",
    "def residual_block(x, filters, pooling=False):\n",
    "    residual = x\n",
    "    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    if pooling:\n",
    "        x = layers.MaxPooling2D(2, padding=\"same\")(x)\n",
    "        # If we use max pooling, we add a strided convolution to project the residual to the expected shape.\n",
    "        residual = layers.Conv2D(filters, 1, strides=2)(residual)\n",
    "    elif filters != residual.shape[-1]:\n",
    "        # If we don't use max pooling, we only project the residual if the number of filters has changed.\n",
    "        residual = layers.Conv2D(filters, 1)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "    return x\n",
    "\n",
    "# First block\n",
    "x = residual_block(x, filters=32, pooling=True)\n",
    "# Second block; note the increasing filter count in each block.\n",
    "x = residual_block(x, filters=64, pooling=True)\n",
    "# The last block doesn't need a max pooling layer, since we will apply global average pooling right after it.\n",
    "x = residual_block(x, filters=128, pooling=False)\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With residual connections, you can build networks of arbitrary depth, without having to worry about vanishing gradients.\n",
    "\n",
    "Now let’s move on to the next essential convnet architecture pattern: **batch normalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch normalization\n",
    "**Normalization** is a broad category of methods that seek to make different samples seen by a machine learning model more similar to each other, which helps the model learn and generalize well to new data. The most common form of data normalization is one you’ve already seen several times in this book: **centering the data on zero by subtracting the mean from the data, and giving the data a unit standard deviation by dividing the data by its standard deviation**. In effect, this makes the assumption that the data follows a **normal (or Gaussian) distribution** and makes sure this distribution is centered and scaled to unit variance:\n",
    "\n",
    "```python\n",
    "normalized_data = (data - np.mean(data, axis=...)) / np.std(data, axis=...)\n",
    "```\n",
    "\n",
    "Previous examples in this book **normalized data before feeding it into models**. But data normalization may be of interest **after every transformation operated by the network**: even if the data entering a Dense or Conv2D network has a 0 mean and unit variance, there’s no reason to expect a priori that this will be the case for the data coming out. Could normalizing intermediate activations help? <br>\n",
    "**Batch normalization** does just that. It’s a type of layer (**BatchNormalization** in Keras) introduced in 2015 by Ioffe and Szegedy; it can adaptively normalize data even as the mean and variance change over time during training. During training, it uses the mean and variance of the current batch of data to normalize samples, and during inference (when a big enough batch of representative data may not be available), it uses an exponential moving average of the batch-wise mean and variance of the data seen during training. <br>\n",
    "Although the original paper stated that **batch normalization** operates by “reducing internal covariate shift,” no one really knows for sure why **batch normalization** helps. There are various hypotheses, but no certitudes. You’ll find that this is true of many things in deep learning—deep learning is not an exact science, but a set of everchanging, empirically derived engineering best practices, woven together by unreliable narratives. You will sometimes feel like the book you have in hand tells you how to do something but doesn’t quite satisfactorily say why it works: that’s because we know the how but we don’t know the why. Whenever a reliable explanation is available, I make sure to mention it. Batch normalization isn’t one of those cases. <br>\n",
    "In practice, the main effect of batch normalization appears to be that it helps with gradient propagation—much like residual connections—and thus allows for deeper networks. Some very deep networks can only be trained if they include multiple **BatchNormalization** layers. For instance, batch normalization is used liberally in many of the advanced convnet architectures that come packaged with Keras, such as ResNet50, EfficientNet, and Xception. **The BatchNormalization layer can be used after any layer**—Dense, Conv2D, etc.:\n",
    "\n",
    "> **NOTE** Both *Dense* and *Conv2D* involve a bias vector, a learned variable whose purpose is to make the layer affine rather than purely linear. For instance, Conv2D returns, schematically, y = conv(x, kernel) + bias, and Dense returns y = dot(x, kernel) + bias. **Because the normalization step will take care of centering the layer’s output on zero, the bias vector is no longer needed when using BatchNormalization**, and the layer can be created without it via the option **use_bias=False**. This makes the layer slightly leaner.\n",
    "\n",
    "Importantly, I would generally recommend placing the **previous layer’s activation after the batch normalization layer** (although this is still a subject of debate).\n",
    "\n",
    "##### How not to use batch normalization\n",
    "\n",
    "```python\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "```\n",
    "\n",
    "##### How to use batch normalization: the activation comes last\n",
    "\n",
    "```python\n",
    "# Because the output of the Conv2D layer gets normalized, the layer doesn’t need its own bias vector.\n",
    "x = layers.Conv2D(32, 3, use_bias=False)(x) # Note the lack of activation here.\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation(\"relu\")(x) # We place the activation after the BatchNormalization layer.\n",
    "```\n",
    "The intuitive reason for this approach is that batch normalization will center your inputs on zero, while your **relu** activation uses zero as a pivot for keeping or dropping activated channels: doing normalization before the activation maximizes the utilization of the relu. That said, this ordering best practice is not exactly critical, so if you do convolution, then activation, and then batch normalization, your model will still train, and you won’t necessarily see worse results.\n",
    "\n",
    "> ##### On batch normalization and fine-tuning \n",
    "> Batch normalization has many quirks. One of the main ones relates to fine-tuning: when fine-tuning a model that includes *BatchNormalization* layers, I recommend leaving these layers frozen (set their *trainable* attribute to *False*). Otherwise they will keep updating their internal mean and variance, which can interfere with the very small updates applied to the surrounding *Conv2D* layers.\n",
    "\n",
    "Now let’s take a look at the last architecture pattern in our series: **depthwise separable convolutions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depthwise separable convolutions\n",
    "What if I told you that there’s a layer you can use as a drop-in replacement for Conv2D that will make your model smaller (fewer trainable weight parameters) and leaner (fewer floating-point operations) and cause it to perform a few percentage points better on its task? That is precisely what the depthwise separable convolution layer does (SeparableConv2D in Keras). This layer performs a spatial convolution on each channel of its input, independently, before mixing output channels via a pointwise convolution (a 1 × 1 convolution), as shown in figure 9.10.\n",
    "\n",
    "![](./chapter_images/9.10.png)\n",
    "\n",
    "This is equivalent to separating the learning of spatial features and the learning of channel-wise features. In much the same way that convolution relies on the assumption that the patterns in images are not tied to specific locations, depthwise separable convolution relies on the assumption that **spatial locations** in intermediate activations are **highly correlated**, but different channels are **highly independent**. Because this assumption is generally true for the image representations learned by deep neural networks, it serves as a useful prior that helps the model make more efficient use of its training data. A model with stronger priors about the structure of the information it will have to process is a better model—as long as the priors are accurate. <br>\n",
    "**Depthwise separable convolution requires significantly fewer parameters and involves fewer computations compared to regular convolution, while having comparable representational power.** It results in smaller models that converge faster and are less prone to overfitting. These advantages become especially important when you’re training **small models from scratch on limited data**. <br>\n",
    "When it comes to larger-scale models, depthwise separable convolutions are the basis of the **Xception** architecture, a high-performing convnet that comes packaged with Keras. You can read more about the theoretical grounding for depthwise separable convolutions and Xception in the paper “Xception: Deep Learning with Depthwise Separable Convolutions.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it together: A mini Xception-like model\n",
    "As a reminder, here are the convnet architecture principles you’ve learned so far:\n",
    "- Your model should be organized into repeated blocks of layers, usually made of multiple convolution layers and a max pooling layer.\n",
    "- The number of filters in your layers should increase as the size of the spatial feature maps decreases.\n",
    "- Deep and narrow is better than broad and shallow.\n",
    "- Introducing residual connections around blocks of layers helps you train deeper networks.\n",
    "- It can be beneficial to introduce batch normalization layers after your convolution layers.\n",
    "- It can be beneficial to replace Conv2D layers with SeparableConv2D layers, which are more parameter-efficient.\n",
    "\n",
    "Let’s bring these ideas together into a single model. Its architecture will resemble a smaller version of Xception, and we’ll apply it to the dogs vs. cats task from the last chapter. For data loading and model training, we’ll simply reuse the setup we used in Chapter 8, but we’ll replace the model definition with the following convnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n",
      "Found 2000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, pathlib\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "original_dir = pathlib.Path(\"train\")\n",
    "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
    "\n",
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        dir = new_base_dir / subset_name / category\n",
    "        os.makedirs(dir)\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        for fname in fnames:\n",
    "            shutil.copyfile(src=original_dir / fname,\n",
    "                            dst=dir / fname)\n",
    "\n",
    "make_subset(\"train\", start_index=0, end_index=1000)\n",
    "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "make_subset(\"test\", start_index=1500, end_index=2500)\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "x = layers.Rescaling(1./255)(x) #  Don't forget input rescaling!\n",
    "# Note that the assumption that underlies separable convolution, “feature channels are largely independent,” does not hold for RGB images! \n",
    "# Red, green, and blue color channels are actually highly correlated in natural images. \n",
    "# As such, the first layer in our model is a regular Conv2D layer. We’ll start using SeparableConv2D afterwards.\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "# We apply a series of convolutional blocks with increasing feature depth. \n",
    "# Each block consists of two batch-normalized depthwise separable convolution layers and a max pooling layer, with a residual connection around the entire block.\n",
    "for size in [32, 64, 128, 256, 512]:\n",
    "    residual = x\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(\n",
    "        size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "# In the original model, we used a Flatten layer before the Dense layer. Here, we go with a GlobalAveragePooling2D layer.\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "# Like in the original model, we add a dropout layer for regularization.\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 180, 180, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 180, 180, 3)  0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " rescaling_1 (Rescaling)        (None, 180, 180, 3)  0           ['sequential[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 176, 176, 32  2400        ['rescaling_1[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 176, 176, 32  128        ['conv2d_16[0][0]']              \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 176, 176, 32  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d (SeparableCon  (None, 176, 176, 32  1312       ['activation[0][0]']             \n",
      " v2D)                           )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 176, 176, 32  128        ['separable_conv2d[0][0]']       \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 176, 176, 32  0           ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " separable_conv2d_1 (SeparableC  (None, 176, 176, 32  1312       ['activation_1[0][0]']           \n",
      " onv2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 88, 88, 32)  0           ['separable_conv2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 88, 88, 32)   1024        ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 88, 88, 32)   0           ['max_pooling2d_3[0][0]',        \n",
      "                                                                  'conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 88, 88, 32)  128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 88, 88, 32)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_2 (SeparableC  (None, 88, 88, 64)  2336        ['activation_2[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 88, 88, 64)  256         ['separable_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 88, 88, 64)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_3 (SeparableC  (None, 88, 88, 64)  4672        ['activation_3[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 44, 44, 64)  0           ['separable_conv2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 44, 44, 64)   2048        ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 44, 44, 64)   0           ['max_pooling2d_4[0][0]',        \n",
      "                                                                  'conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 44, 44, 64)  256         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 44, 44, 64)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_4 (SeparableC  (None, 44, 44, 128)  8768       ['activation_4[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 44, 44, 128)  512        ['separable_conv2d_4[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 44, 44, 128)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_5 (SeparableC  (None, 44, 44, 128)  17536      ['activation_5[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 22, 22, 128)  0          ['separable_conv2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 22, 22, 128)  8192        ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 22, 22, 128)  0           ['max_pooling2d_5[0][0]',        \n",
      "                                                                  'conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 22, 22, 128)  512        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 22, 22, 128)  0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_6 (SeparableC  (None, 22, 22, 256)  33920      ['activation_6[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 22, 22, 256)  1024       ['separable_conv2d_6[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 22, 22, 256)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_7 (SeparableC  (None, 22, 22, 256)  67840      ['activation_7[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 11, 11, 256)  0          ['separable_conv2d_7[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 11, 11, 256)  32768       ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 11, 11, 256)  0           ['max_pooling2d_6[0][0]',        \n",
      "                                                                  'conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 11, 11, 256)  1024       ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 11, 11, 256)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_8 (SeparableC  (None, 11, 11, 512)  133376     ['activation_8[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 11, 11, 512)  2048       ['separable_conv2d_8[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 11, 11, 512)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " separable_conv2d_9 (SeparableC  (None, 11, 11, 512)  266752     ['activation_9[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 6, 6, 512)   0           ['separable_conv2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 6, 6, 512)    131072      ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 6, 6, 512)    0           ['max_pooling2d_7[0][0]',        \n",
      "                                                                  'conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 512)         0           ['add_9[0][0]']                  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            513         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 721,857\n",
      "Trainable params: 718,849\n",
      "Non-trainable params: 3,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This convnet has a trainable parameter count of 721,857, slightly lower than the 991,041 trainable parameters of the original model, but still in the same ballpark. Figure 9.11 shows its training and validation curves(for 100 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 135s 2s/step - loss: 0.7079 - accuracy: 0.5525 - val_loss: 0.6945 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 133s 2s/step - loss: 0.6616 - accuracy: 0.5905 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 138s 2s/step - loss: 0.6439 - accuracy: 0.6320 - val_loss: 0.6985 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 130s 2s/step - loss: 0.6239 - accuracy: 0.6545 - val_loss: 0.7186 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 130s 2s/step - loss: 0.6068 - accuracy: 0.6755 - val_loss: 0.6904 - val_accuracy: 0.5490\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 132s 2s/step - loss: 0.5942 - accuracy: 0.6805 - val_loss: 0.7369 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 129s 2s/step - loss: 0.5849 - accuracy: 0.6960 - val_loss: 0.8233 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 126s 2s/step - loss: 0.5608 - accuracy: 0.7100 - val_loss: 0.9454 - val_accuracy: 0.5010\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 128s 2s/step - loss: 0.5444 - accuracy: 0.7260 - val_loss: 0.7215 - val_accuracy: 0.5960\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 127s 2s/step - loss: 0.5314 - accuracy: 0.7385 - val_loss: 0.5599 - val_accuracy: 0.7090\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./chapter_images/9.11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll find that our new model achieves a test accuracy of 90.8%, compared to 83.5% for the naive model in the last chapter. As you can see, following architecture best practices does have an immediate, sizable impact on model performance! <br>\n",
    "At this point, if you want to further improve performance, you should start systematically tuning the hyperparameters of your architecture—a topic we’ll cover in detail in chapter 13. We haven’t gone through this step here, so the configuration of the preceding model is purely based on the best practices we discussed, plus, when it comes to gauging model size, a small amount of intuition. <br> Note that these architecture best practices are relevant to computer vision in general, not just image classification. For example, Xception is used as the standard convolutional base in DeepLabV3, a popular state-of-the-art image segmentation solution.\n",
    "\n",
    "This concludes our introduction to essential convnet architecture best practices. With these principles in hand, you’ll be able to develop higher-performing models across a wide range of computer vision tasks. You’re now well on your way to becoming a proficient computer vision practitioner. To further deepen your expertise, there’s one last important topic we need to cover: interpreting how a model arrives at its predictions. --> part03_interpreting-what-convnets-learn"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483abfc7fdcd927bfa336910f494643cce94b3dfa36bcded073270b8df64edb3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
